[{"path":[]},{"path":"/articles/lecture_cfa.html","id":"relation-to-efa","dir":"Articles","previous_headings":"","what":"Relation to EFA","title":"CFA: Basics","text":"bunch questions idea (sometimes !) many factors expect let questions go want remove bad questions get good fit","code":""},{"path":"/articles/lecture_cfa.html","id":"cfa-models","dir":"Articles","previous_headings":"","what":"CFA models","title":"CFA: Basics","text":"set model specific questions onto specific factors Forcing cross loadings zero test see model fits , may think confirmatory factor analysis step two exploring (exploratory factor analysis)","code":""},{"path":"/articles/lecture_cfa.html","id":"cfa-models-1","dir":"Articles","previous_headings":"","what":"CFA Models","title":"CFA: Basics","text":"Reflective – latent variable causes manifest variables scores Purpose understand relationships measured variables theoretical concept EFA","code":""},{"path":"/articles/lecture_cfa.html","id":"cfa-models-reflective-example","dir":"Articles","previous_headings":"","what":"CFA Models Reflective Example","title":"CFA: Basics","text":"","code":"# a famous example, build the model HS.model <- ' visual  =~ x1 + x2 + x3               textual =~ x4 + x5 + x6               speed   =~ x7 + x8 + x9 '  # fit the model  HS.fit <- cfa(HS.model, data = HolzingerSwineford1939)  # diagram the model semPaths(HS.fit,           whatLabels = \"std\",           layout = \"tree\",          edge.label.cex = 1)"},{"path":"/articles/lecture_cfa.html","id":"cfa-models-2","dir":"Articles","previous_headings":"","what":"CFA Models","title":"CFA: Basics","text":"Formative – latent variables result manifest variables Similar principal components analysis theoretical concept Potentially use demographics?","code":""},{"path":"/articles/lecture_cfa.html","id":"cfa-models-formative-example","dir":"Articles","previous_headings":"","what":"CFA Models Formative Example","title":"CFA: Basics","text":"","code":"# a famous example, build the model HS.model <- ' visual  <~ x1 + x2 + x3'  # fit the model  HS.fit <- cfa(HS.model, data = HolzingerSwineford1939) #> Warning: lavaan->lav_data_full():   #>    all observed variables are exogenous; model may not be identified #> Warning: lavaan->lav_model_vcov():   #>    Could not compute standard errors! The information matrix could not be  #>    inverted. This may be a symptom that the model is not identified.  # diagram the model semPaths(HS.fit,           whatLabels = \"std\",           layout = \"tree\",          edge.label.cex = 1)"},{"path":"/articles/lecture_cfa.html","id":"cfa-models-3","dir":"Articles","previous_headings":"","what":"CFA Models","title":"CFA: Basics","text":"manifest variables CFA sometimes called indicator variables indicate latent variable since directly measure latent variable","code":""},{"path":"/articles/lecture_cfa.html","id":"general-set-up","dir":"Articles","previous_headings":"","what":"General Set Up","title":"CFA: Basics","text":"latents correlated (exogenous ) Similar oblique rotation three measured variables per latent two, need set coefficients equal (estimated equal) think latent caused measured answers counting degrees freedom identification","code":""},{"path":"/articles/lecture_cfa.html","id":"correlated-error","dir":"Articles","previous_headings":"","what":"Correlated Error","title":"CFA: Basics","text":"Generally, leave error terms uncorrelated, think separate items However: questions measure factor right? Often pretty similar answers related items ’s big idea say item’s errors related Make sure make sense!","code":""},{"path":"/articles/lecture_cfa.html","id":"interpretation","dir":"Articles","previous_headings":"","what":"Interpretation","title":"CFA: Basics","text":"latent variable section includes factor loadings coefficients used rule .300 rule, examine standardized loading Otherwise, think item measures latent variable?","code":""},{"path":"/articles/lecture_cfa.html","id":"interpretation-1","dir":"Articles","previous_headings":"","what":"Interpretation","title":"CFA: Basics","text":"coefficients often called: Pattern coefficients (unstandardized): every one unit latent variable, manifest variable increases b units Structure coefficients (standardized): correlation latent variable manifest variable","code":""},{"path":"/articles/lecture_cfa.html","id":"identification-rules-of-thumb","dir":"Articles","previous_headings":"","what":"Identification Rules of Thumb:","title":"CFA: Basics","text":"Latent variables four indicators Latent variables three indicators error variances covary Latent variables two indicators Error variances covary loadings set equal ","code":""},{"path":"/articles/lecture_cfa.html","id":"scaling","dir":"Articles","previous_headings":"","what":"Scaling","title":"CFA: Basics","text":"Remember scaling way “set scale” latent variable usually setting one pattern coefficients 1 - marker variable approach ? Sets scale z-score Makes double headed arrow latents correlation Make sure using unstandardized data!","code":""},{"path":"/articles/lecture_cfa.html","id":"scaling-1","dir":"Articles","previous_headings":"","what":"Scaling","title":"CFA: Basics","text":"std.part “completely standardized solution? latent variable variance manifest variable variance set 1 going report standardized solution, version common, matches EFA regression options give different loadings, change model fit","code":""},{"path":"/articles/lecture_cfa.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"CFA: Basics","text":"Reminder: use correlation matrix input, solution already standardized! use covariance matrix input, unstandardized standardized solution can viewed","code":""},{"path":"/articles/lecture_cfa.html","id":"one-factor-cfa-example","dir":"Articles","previous_headings":"","what":"One-Factor CFA Example","title":"CFA: Basics","text":"IQ often thought “g” overall cognitive ability Let’s look example WISC, IQ test children five subtest scores including information, similarities, word reasoning, matrix reasoning, picture concepts","code":""},{"path":"/articles/lecture_cfa.html","id":"convert-correlations-to-covariance","dir":"Articles","previous_headings":"","what":"Convert Correlations to Covariance","title":"CFA: Basics","text":"","code":"wisc4.cor <- lav_matrix_lower2full(c(1,                                      0.72,1,                                      0.64,0.63,1,                                      0.51,0.48,0.37,1,                                      0.37,0.38,0.38,0.38,1)) # enter the SDs wisc4.sd <- c(3.01 , 3.03 , 2.99 , 2.89 , 2.98)  # give everything names colnames(wisc4.cor) <-    rownames(wisc4.cor) <-   names(wisc4.sd) <-    c(\"Information\", \"Similarities\",      \"Word.Reasoning\", \"Matrix.Reasoning\", \"Picture.Concepts\")  # convert wisc4.cov <- cor2cov(wisc4.cor, wisc4.sd)"},{"path":"/articles/lecture_cfa.html","id":"wisc-one-factor-model","dir":"Articles","previous_headings":"","what":"WISC One-Factor Model","title":"CFA: Basics","text":"=~ used define reflexive latent variable ~ can interpreted Y predicted X =~ can interpreted X indicated Ys","code":"wisc4.model <- ' g =~ Information + Similarities + Word.Reasoning + Matrix.Reasoning + Picture.Concepts '"},{"path":"/articles/lecture_cfa.html","id":"analyze-the-model","dir":"Articles","previous_headings":"","what":"Analyze the Model","title":"CFA: Basics","text":"Notice changed cfa() function basic arguments std.lv option can used see standardized solution latent variable, usually want set FALSE","code":"wisc4.fit <- cfa(model = wisc4.model,                  sample.cov = wisc4.cov,                  sample.nobs = 550,                   std.lv = FALSE)"},{"path":"/articles/lecture_cfa.html","id":"summarize-the-model","dir":"Articles","previous_headings":"","what":"Summarize the Model","title":"CFA: Basics","text":"Positive variances SMCs + Correlations < 1 error messages SEs “huge” questions load appropriately? fit indices indicate? Can improve model fit without overfitting?","code":""},{"path":"/articles/lecture_cfa.html","id":"summarize-the-model-1","dir":"Articles","previous_headings":"","what":"Summarize the Model","title":"CFA: Basics","text":"","code":"summary(wisc4.fit,         standardized=TRUE,          rsquare = TRUE,         fit.measures=TRUE) #> lavaan 0.6-19 ended normally after 30 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        10 #>  #>   Number of observations                           550 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                26.775 #>   Degrees of freedom                                 5 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              1073.427 #>   Degrees of freedom                                10 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.980 #>   Tucker-Lewis Index (TLI)                       0.959 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -6378.678 #>   Loglikelihood unrestricted model (H1)      -6365.291 #>                                                        #>   Akaike (AIC)                               12777.357 #>   Bayesian (BIC)                             12820.456 #>   Sample-size adjusted Bayesian (SABIC)      12788.712 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.089 #>   90 Percent confidence interval - lower         0.058 #>   90 Percent confidence interval - upper         0.123 #>   P-value H_0: RMSEA <= 0.050                    0.022 #>   P-value H_0: RMSEA >= 0.080                    0.708 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.034 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   g =~                                                                   #>     Information       1.000                               2.578    0.857 #>     Similarities      0.985    0.045   21.708    0.000    2.541    0.839 #>     Word.Reasoning    0.860    0.045   18.952    0.000    2.217    0.742 #>     Matrix.Reasnng    0.647    0.047   13.896    0.000    1.669    0.578 #>     Picture.Cncpts    0.542    0.050   10.937    0.000    1.398    0.470 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Information       2.395    0.250    9.587    0.000    2.395    0.265 #>    .Similarities      2.709    0.258   10.482    0.000    2.709    0.296 #>    .Word.Reasoning    4.009    0.295   13.600    0.000    4.009    0.449 #>    .Matrix.Reasnng    5.551    0.360   15.400    0.000    5.551    0.666 #>    .Picture.Cncpts    6.909    0.434   15.922    0.000    6.909    0.779 #>     g                 6.648    0.564   11.788    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     Information       0.735 #>     Similarities      0.704 #>     Word.Reasoning    0.551 #>     Matrix.Reasnng    0.334 #>     Picture.Cncpts    0.221"},{"path":"/articles/lecture_cfa.html","id":"new-functions","dir":"Articles","previous_headings":"","what":"New Functions","title":"CFA: Basics","text":"std.nox: standardized estimates based variances (continuous) observed latent variables, variances exogenous covariates output best way get confidence intervals parameter","code":"parameterestimates(wisc4.fit,                    standardized=TRUE) #>                 lhs op              rhs   est    se      z pvalue ci.lower #> 1                 g =~      Information 1.000 0.000     NA     NA    1.000 #> 2                 g =~     Similarities 0.985 0.045 21.708      0    0.896 #> 3                 g =~   Word.Reasoning 0.860 0.045 18.952      0    0.771 #> 4                 g =~ Matrix.Reasoning 0.647 0.047 13.896      0    0.556 #> 5                 g =~ Picture.Concepts 0.542 0.050 10.937      0    0.445 #> 6       Information ~~      Information 2.395 0.250  9.587      0    1.906 #> 7      Similarities ~~     Similarities 2.709 0.258 10.482      0    2.202 #> 8    Word.Reasoning ~~   Word.Reasoning 4.009 0.295 13.600      0    3.431 #> 9  Matrix.Reasoning ~~ Matrix.Reasoning 5.551 0.360 15.400      0    4.845 #> 10 Picture.Concepts ~~ Picture.Concepts 6.909 0.434 15.922      0    6.058 #> 11                g ~~                g 6.648 0.564 11.788      0    5.543 #>    ci.upper std.lv std.all #> 1     1.000  2.578   0.857 #> 2     1.074  2.541   0.839 #> 3     0.949  2.217   0.742 #> 4     0.739  1.669   0.578 #> 5     0.640  1.398   0.470 #> 6     2.885  2.395   0.265 #> 7     3.215  2.709   0.296 #> 8     4.587  4.009   0.449 #> 9     6.258  5.551   0.666 #> 10    7.759  6.909   0.779 #> 11    7.754  1.000   1.000"},{"path":"/articles/lecture_cfa.html","id":"new-functions-1","dir":"Articles","previous_headings":"","what":"New Functions","title":"CFA: Basics","text":"","code":"fitted(wisc4.fit) ## estimated covariances #> $cov #>                  Infrmt Smlrts Wrd.Rs Mtrx.R Pctr.C #> Information       9.044                             #> Similarities      6.551  9.164                      #> Word.Reasoning    5.716  5.633  8.924               #> Matrix.Reasoning  4.303  4.241  3.700  8.337        #> Picture.Concepts  3.606  3.553  3.100  2.334  8.864 wisc4.cov ## actual covariances #>                  Information Similarities Word.Reasoning Matrix.Reasoning #> Information         9.060100     6.566616       5.759936         4.436439 #> Similarities        6.566616     9.180900       5.707611         4.203216 #> Word.Reasoning      5.759936     5.707611       8.940100         3.197207 #> Matrix.Reasoning    4.436439     4.203216       3.197207         8.352100 #> Picture.Concepts    3.318826     3.431172       3.385876         3.272636 #>                  Picture.Concepts #> Information              3.318826 #> Similarities             3.431172 #> Word.Reasoning           3.385876 #> Matrix.Reasoning         3.272636 #> Picture.Concepts         8.880400"},{"path":"/articles/lecture_cfa.html","id":"all-fit-indices","dir":"Articles","previous_headings":"","what":"All Fit Indices","title":"CFA: Basics","text":"","code":"fitmeasures(wisc4.fit) #>                  npar                  fmin                 chisq  #>                10.000                 0.024                26.775  #>                    df                pvalue        baseline.chisq  #>                 5.000                 0.000              1073.427  #>           baseline.df       baseline.pvalue                   cfi  #>                10.000                 0.000                 0.980  #>                   tli                  nnfi                   rfi  #>                 0.959                 0.959                 0.950  #>                   nfi                  pnfi                   ifi  #>                 0.975                 0.488                 0.980  #>                   rni                  logl     unrestricted.logl  #>                 0.980             -6378.678             -6365.291  #>                   aic                   bic                ntotal  #>             12777.357             12820.456               550.000  #>                  bic2                 rmsea        rmsea.ci.lower  #>             12788.712                 0.089                 0.058  #>        rmsea.ci.upper        rmsea.ci.level          rmsea.pvalue  #>                 0.123                 0.900                 0.022  #>        rmsea.close.h0 rmsea.notclose.pvalue     rmsea.notclose.h0  #>                 0.050                 0.708                 0.080  #>                   rmr            rmr_nomean                  srmr  #>                 0.298                 0.298                 0.034  #>          srmr_bentler   srmr_bentler_nomean                  crmr  #>                 0.034                 0.034                 0.042  #>           crmr_nomean            srmr_mplus     srmr_mplus_nomean  #>                 0.042                 0.034                 0.034  #>                 cn_05                 cn_01                   gfi  #>               228.408               310.899                 0.982  #>                  agfi                  pgfi                   mfi  #>                 0.947                 0.327                 0.980  #>                  ecvi  #>                 0.085"},{"path":"/articles/lecture_cfa.html","id":"modification-indices","dir":"Articles","previous_headings":"","what":"Modification Indices","title":"CFA: Basics","text":"","code":"modificationindices(wisc4.fit, sort = T) #>                 lhs op              rhs     mi    epc sepc.lv sepc.all sepc.nox #> 21 Matrix.Reasoning ~~ Picture.Concepts 14.157  1.058   1.058    0.171    0.171 #> 19   Word.Reasoning ~~ Matrix.Reasoning  8.931 -0.710  -0.710   -0.151   -0.151 #> 15      Information ~~ Picture.Concepts  5.493 -0.565  -0.565   -0.139   -0.139 #> 20   Word.Reasoning ~~ Picture.Concepts  2.029  0.365   0.365    0.069    0.069 #> 14      Information ~~ Matrix.Reasoning  1.447  0.280   0.280    0.077    0.077 #> 18     Similarities ~~ Picture.Concepts  0.838 -0.223  -0.223   -0.051   -0.051 #> 16     Similarities ~~   Word.Reasoning  0.791  0.242   0.242    0.073    0.073 #> 13      Information ~~   Word.Reasoning  0.279  0.147   0.147    0.047    0.047 #> 17     Similarities ~~ Matrix.Reasoning  0.147 -0.089  -0.089   -0.023   -0.023 #> 12      Information ~~     Similarities  0.010  0.034   0.034    0.013    0.013"},{"path":"/articles/lecture_cfa.html","id":"diagram-the-model","dir":"Articles","previous_headings":"","what":"Diagram the Model","title":"CFA: Basics","text":"","code":"semPaths(wisc4.fit,           whatLabels=\"std\",           what = \"std\",          layout =\"tree\",          edge.color = \"blue\",          edge.label.cex = 1)"},{"path":"/articles/lecture_cfa.html","id":"wisc-two-factor-model","dir":"Articles","previous_headings":"","what":"WISC Two-Factor Model","title":"CFA: Basics","text":"Note two items latent variable see identification error, can set equal (homework hint!)","code":"wisc4.model2 <- ' V =~ Information + Similarities + Word.Reasoning  F =~ Matrix.Reasoning + Picture.Concepts '  # wisc4.model2 <- ' # V =~ Information + Similarities + Word.Reasoning  # F =~ a*Matrix.Reasoning + a*Picture.Concepts # '"},{"path":"/articles/lecture_cfa.html","id":"analyze-the-model-1","dir":"Articles","previous_headings":"","what":"Analyze the Model","title":"CFA: Basics","text":"","code":"wisc4.fit2 <- cfa(wisc4.model2,                    sample.cov=wisc4.cov,                    sample.nobs=550,                   std.lv = F)"},{"path":"/articles/lecture_cfa.html","id":"summarize-the-model-2","dir":"Articles","previous_headings":"","what":"Summarize the Model","title":"CFA: Basics","text":"","code":"summary(wisc4.fit2,         standardized=TRUE,          rsquare = TRUE,         fit.measures=TRUE) #> lavaan 0.6-19 ended normally after 44 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        11 #>  #>   Number of observations                           550 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                12.687 #>   Degrees of freedom                                 4 #>   P-value (Chi-square)                           0.013 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              1073.427 #>   Degrees of freedom                                10 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.992 #>   Tucker-Lewis Index (TLI)                       0.980 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -6371.634 #>   Loglikelihood unrestricted model (H1)      -6365.291 #>                                                        #>   Akaike (AIC)                               12765.269 #>   Bayesian (BIC)                             12812.678 #>   Sample-size adjusted Bayesian (SABIC)      12777.759 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.063 #>   90 Percent confidence interval - lower         0.026 #>   90 Percent confidence interval - upper         0.103 #>   P-value H_0: RMSEA <= 0.050                    0.244 #>   P-value H_0: RMSEA >= 0.080                    0.272 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.019 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   V =~                                                                   #>     Information       1.000                               2.587    0.860 #>     Similarities      0.984    0.046   21.625    0.000    2.545    0.841 #>     Word.Reasoning    0.858    0.045   18.958    0.000    2.219    0.743 #>   F =~                                                                   #>     Matrix.Reasnng    1.000                               1.989    0.689 #>     Picture.Cncpts    0.825    0.085    9.747    0.000    1.642    0.552 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   V ~~                                                                   #>     F                 4.233    0.399   10.604    0.000    0.823    0.823 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Information       2.352    0.253    9.295    0.000    2.352    0.260 #>    .Similarities      2.685    0.261   10.282    0.000    2.685    0.293 #>    .Word.Reasoning    4.000    0.295   13.555    0.000    4.000    0.448 #>    .Matrix.Reasnng    4.380    0.458    9.557    0.000    4.380    0.525 #>    .Picture.Cncpts    6.168    0.451   13.673    0.000    6.168    0.696 #>     V                 6.692    0.567   11.807    0.000    1.000    1.000 #>     F                 3.957    0.569    6.960    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     Information       0.740 #>     Similarities      0.707 #>     Word.Reasoning    0.552 #>     Matrix.Reasnng    0.475 #>     Picture.Cncpts    0.304"},{"path":"/articles/lecture_cfa.html","id":"diagram-the-model-1","dir":"Articles","previous_headings":"","what":"Diagram the Model","title":"CFA: Basics","text":"","code":"semPaths(wisc4.fit2,           whatLabels=\"std\",           what = \"std\",          edge.color = \"pink\",          edge.label.cex = 1,          layout=\"tree\")"},{"path":"/articles/lecture_cfa.html","id":"compare-the-models","dir":"Articles","previous_headings":"","what":"Compare the Models","title":"CFA: Basics","text":"","code":"anova(wisc4.fit, wisc4.fit2) #>  #> Chi-Squared Difference Test #>  #>            Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(>Chisq)     #> wisc4.fit2  4 12765 12813 12.687                                           #> wisc4.fit   5 12777 12820 26.775     14.088 0.15426       1  0.0001745 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 fitmeasures(wisc4.fit, c(\"aic\", \"ecvi\")) #>       aic      ecvi  #> 12777.357     0.085 fitmeasures(wisc4.fit2, c(\"aic\", \"ecvi\")) #>       aic      ecvi  #> 12765.269     0.063"},{"path":"/articles/lecture_cfa.html","id":"how-to-tidy-lavaan-output","dir":"Articles","previous_headings":"","what":"How to Tidy lavaan Output","title":"CFA: Basics","text":"https://easystats.github.io/parameters/articles/efa_cfa.html","code":"#install.packages(\"parameters\") library(parameters) model_parameters(wisc4.fit, standardize = TRUE) #> # Loading #>  #> Link                  | Coefficient |   SE |       95% CI |     z |      p #> -------------------------------------------------------------------------- #> g =~ Information      |        0.86 | 0.02 | [0.82, 0.89] | 49.47 | < .001 #> g =~ Similarities     |        0.84 | 0.02 | [0.80, 0.87] | 46.32 | < .001 #> g =~ Word.Reasoning   |        0.74 | 0.02 | [0.70, 0.79] | 32.26 | < .001 #> g =~ Matrix.Reasoning |        0.58 | 0.03 | [0.52, 0.64] | 18.29 | < .001 #> g =~ Picture.Concepts |        0.47 | 0.04 | [0.40, 0.54] | 12.94 | < .001"},{"path":"/articles/lecture_cfa.html","id":"how-to-tidy-lavaan-output-1","dir":"Articles","previous_headings":"","what":"How to Tidy lavaan Output","title":"CFA: Basics","text":"","code":"library(broom) tidy(wisc4.fit) #> # A tibble: 11 × 8 #>    term                op    estimate std.error statistic p.value std.lv std.all #>    <chr>               <chr>    <dbl>     <dbl>     <dbl>   <dbl>  <dbl>   <dbl> #>  1 g =~ Information    =~       1        0          NA         NA   2.58   0.857 #>  2 g =~ Similarities   =~       0.985    0.0454     21.7        0   2.54   0.839 #>  3 g =~ Word.Reasoning =~       0.860    0.0454     19.0        0   2.22   0.742 #>  4 g =~ Matrix.Reason… =~       0.647    0.0466     13.9        0   1.67   0.578 #>  5 g =~ Picture.Conce… =~       0.542    0.0496     10.9        0   1.40   0.470 #>  6 Information ~~ Inf… ~~       2.40     0.250       9.59       0   2.40   0.265 #>  7 Similarities ~~ Si… ~~       2.71     0.258      10.5        0   2.71   0.296 #>  8 Word.Reasoning ~~ … ~~       4.01     0.295      13.6        0   4.01   0.449 #>  9 Matrix.Reasoning ~… ~~       5.55     0.360      15.4        0   5.55   0.666 #> 10 Picture.Concepts ~… ~~       6.91     0.434      15.9        0   6.91   0.779 #> 11 g ~~ g              ~~       6.65     0.564      11.8        0   1      1 glance(wisc4.fit) #> # A tibble: 1 × 17 #>    agfi    AIC    BIC   cfi chisq  npar  rmsea rmsea.conf.high   srmr   tli #>   <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>           <dbl>  <dbl> <dbl> #> 1 0.947 12777. 12820. 0.980  26.8    10 0.0890           0.123 0.0345 0.959 #> # ℹ 7 more variables: converged <lgl>, estimator <chr>, ngroups <int>, #> #   missing_method <chr>, nobs <dbl>, norig <dbl>, nexcluded <dbl>"},{"path":"/articles/lecture_cfa.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"CFA: Basics","text":"lecture ’ve learned: create simple confirmatory factor analysis (measurement model) view parameter estimates, modification indices, Practiced building models comparing ","code":""},{"path":"/articles/lecture_data_screen.html","id":"data-screening-overview","dir":"Articles","previous_headings":"","what":"Data Screening Overview","title":"Data Screening","text":"lecture, give demonstration might data screen dataset structural equation modeling. four key steps: Accuracy: dealing errors Missing: dealing missing data Outliers: determining outliers Assumptions: additivity, multivariate normality, linearity, homogeneity, homoscedasticity Note type data screening may change depending type data (.e., ordinal data different assumptions) Mostly, focus datasets traditional parametric assumptions","code":""},{"path":"/articles/lecture_data_screen.html","id":"hypothesis-testing-versus-data-screening","dir":"Articles","previous_headings":"","what":"Hypothesis Testing versus Data Screening","title":"Data Screening","text":"Generally, set alphaalpha value, Type 1 error Often, translates “statistical significance”, p < alphaalpha = significant, alphaalpha often defined .05 data screening, want things unusual correcting eliminating things Therefore, often lower criterion use p < alphaalpha denote problems data, alphaalpha lowered .001","code":""},{"path":"/articles/lecture_data_screen.html","id":"order-is-important","dir":"Articles","previous_headings":"","what":"Order is Important","title":"Data Screening","text":"datascreening can performed many ways, ’s important know fix errors, missing data, etc. checking assumptions changes make effect next steps","code":""},{"path":"/articles/lecture_data_screen.html","id":"an-example","dir":"Articles","previous_headings":"","what":"An Example","title":"Data Screening","text":"learn data screening working example data made data people asked judge learning different experimental conditions, rated confidence remembering information, measured actual memory situation","code":""},{"path":"/articles/lecture_data_screen.html","id":"import-the-data","dir":"Articles","previous_headings":"","what":"Import the Data","title":"Data Screening","text":"","code":"library(rio) master <- import(\"data/lecture_data_screen.csv\") names(master) #>  [1] \"JOL_group\" \"type_cue\"  \"conf1\"     \"conf2\"     \"conf3\"     \"conf4\"     #>  [7] \"conf5\"     \"conf6\"     \"conf7\"     \"conf8\"     \"conf9\"     \"conf10\"    #> [13] \"rec1\"      \"rec2\"      \"rec3\"      \"rec4\"      \"rec5\"      \"rec6\"      #> [19] \"rec7\"      \"rec8\"      \"rec9\"      \"rec10\""},{"path":"/articles/lecture_data_screen.html","id":"accuracy","dir":"Articles","previous_headings":"","what":"Accuracy","title":"Data Screening","text":"Use summary() table() functions examine dataset. Categorical data: labels right? variable factored? Continuous data: min/max data correct? data scored correctly?","code":""},{"path":"/articles/lecture_data_screen.html","id":"accuracy-categorical","dir":"Articles","previous_headings":"","what":"Accuracy Categorical","title":"Data Screening","text":"","code":"#summary(master) table(master$JOL_group) #>  #>   delayed immediate  #>        84        74  table(master$type_cue) #>  #>       cue only stimulus pairs  #>             76             82"},{"path":"/articles/lecture_data_screen.html","id":"accuracy-categorical-1","dir":"Articles","previous_headings":"","what":"Accuracy Categorical","title":"Data Screening","text":"","code":"no_typos <- master no_typos$JOL_group <- factor(no_typos$JOL_group,                              levels = c(\"delayed\", \"immediate\"),                              labels = c(\"Delayed\", \"Immediate\"))  no_typos$type_cue <- factor(no_typos$type_cue,                              levels = c(\"cue only\", \"stimulus pairs\"),                             labels = c(\"Cue Only\", \"Stimulus Pairs\"))"},{"path":"/articles/lecture_data_screen.html","id":"accuracy-continuous","dir":"Articles","previous_headings":"","what":"Accuracy Continuous","title":"Data Screening","text":"Confidence recall 0 100. Looks like data clean .","code":"summary(no_typos) #>      JOL_group            type_cue      conf1           conf2       #>  Delayed  :84   Cue Only      :76   Min.   :20.70   Min.   :23.85   #>  Immediate:74   Stimulus Pairs:82   1st Qu.:42.79   1st Qu.:41.94   #>                                     Median :49.48   Median :50.63   #>                                     Mean   :49.28   Mean   :50.29   #>                                     3rd Qu.:55.50   3rd Qu.:57.21   #>                                     Max.   :72.40   Max.   :75.42   #>                                                     NA's   :3       #>      conf3           conf4            conf5           conf6       #>  Min.   :24.43   Min.   :-48.75   Min.   :19.97   Min.   :22.31   #>  1st Qu.:44.41   1st Qu.: 42.04   1st Qu.:43.48   1st Qu.:43.03   #>  Median :48.94   Median : 48.40   Median :50.79   Median :51.12   #>  Mean   :49.53   Mean   : 48.24   Mean   :50.91   Mean   :50.67   #>  3rd Qu.:54.91   3rd Qu.: 55.58   3rd Qu.:57.29   3rd Qu.:57.93   #>  Max.   :74.27   Max.   : 76.62   Max.   :77.40   Max.   :79.93   #>  NA's   :3       NA's   :2        NA's   :3       NA's   :4       #>      conf7           conf8           conf9           conf10       #>  Min.   :22.15   Min.   :24.74   Min.   :25.16   Min.   : 25.87   #>  1st Qu.:43.59   1st Qu.:42.81   1st Qu.:41.48   1st Qu.: 43.13   #>  Median :48.51   Median :50.75   Median :50.66   Median : 49.10   #>  Mean   :49.55   Mean   :50.61   Mean   :49.61   Mean   : 52.42   #>  3rd Qu.:56.32   3rd Qu.:58.10   3rd Qu.:56.68   3rd Qu.: 55.79   #>  Max.   :76.23   Max.   :80.01   Max.   :81.59   Max.   :470.53   #>  NA's   :5       NA's   :4       NA's   :4       NA's   :4        #>       rec1            rec2            rec3            rec4       #>  Min.   :47.39   Min.   :47.91   Min.   :46.79   Min.   :48.35   #>  1st Qu.:57.33   1st Qu.:55.79   1st Qu.:56.32   1st Qu.:56.51   #>  Median :60.48   Median :59.95   Median :60.16   Median :59.45   #>  Mean   :60.25   Mean   :59.90   Mean   :59.85   Mean   :59.74   #>  3rd Qu.:63.50   3rd Qu.:63.58   3rd Qu.:63.56   3rd Qu.:62.87   #>  Max.   :71.60   Max.   :71.43   Max.   :72.08   Max.   :74.07   #>  NA's   :3       NA's   :3       NA's   :3       NA's   :3       #>       rec5             rec6             rec7            rec8       #>  Min.   :-59.85   Min.   : 42.84   Min.   :46.67   Min.   :50.64   #>  1st Qu.: 56.31   1st Qu.: 56.96   1st Qu.:56.88   1st Qu.:56.58   #>  Median : 59.33   Median : 60.19   Median :60.15   Median :59.16   #>  Mean   : 58.84   Mean   : 60.81   Mean   :60.17   Mean   :59.62   #>  3rd Qu.: 62.72   3rd Qu.: 63.84   3rd Qu.:64.18   3rd Qu.:62.64   #>  Max.   : 73.07   Max.   :161.86   Max.   :71.01   Max.   :72.50   #>  NA's   :5        NA's   :3        NA's   :4       NA's   :3       #>       rec9           rec10       #>  Min.   :45.66   Min.   :45.49   #>  1st Qu.:56.04   1st Qu.:56.17   #>  Median :59.40   Median :59.68   #>  Mean   :59.56   Mean   :59.47   #>  3rd Qu.:63.12   3rd Qu.:62.70   #>  Max.   :73.32   Max.   :72.59   #>  NA's   :4       NA's   :3"},{"path":"/articles/lecture_data_screen.html","id":"accuracy-continuous-1","dir":"Articles","previous_headings":"","what":"Accuracy Continuous","title":"Data Screening","text":"","code":"# how did I get 3:22? # how did I get the rule? # what should I do?  no_typos[ , 3:22][ no_typos[ , 3:22] > 100 ] #>  [1]       NA       NA       NA       NA       NA       NA       NA       NA #>  [9]       NA       NA       NA       NA       NA       NA       NA       NA #> [17]       NA       NA       NA       NA       NA       NA       NA       NA #> [25]       NA       NA       NA       NA       NA       NA 470.5320       NA #> [33]       NA       NA       NA       NA       NA       NA       NA       NA #> [41]       NA       NA       NA       NA       NA       NA       NA       NA #> [49]       NA       NA       NA 161.8596       NA       NA       NA       NA #> [57]       NA       NA       NA       NA       NA       NA       NA       NA #> [65]       NA       NA       NA       NA  no_typos[ , 3:22][ no_typos[ , 3:22] > 100 ] <- NA  no_typos[ , 3:22][ no_typos[ , 3:22] < 0 ] <- NA"},{"path":"/articles/lecture_data_screen.html","id":"missing","dir":"Articles","previous_headings":"","what":"Missing","title":"Data Screening","text":"two main types missing data: Missing random: data missing common cause (.e., everyone skipped question five) Missing completely random: data randomly missing, potentially due computer human error also distinguish missing data incomplete data","code":"no_missing <- no_typos summary(no_missing) #>      JOL_group            type_cue      conf1           conf2       #>  Delayed  :84   Cue Only      :76   Min.   :20.70   Min.   :23.85   #>  Immediate:74   Stimulus Pairs:82   1st Qu.:42.79   1st Qu.:41.94   #>                                     Median :49.48   Median :50.63   #>                                     Mean   :49.28   Mean   :50.29   #>                                     3rd Qu.:55.50   3rd Qu.:57.21   #>                                     Max.   :72.40   Max.   :75.42   #>                                                     NA's   :3       #>      conf3           conf4           conf5           conf6       #>  Min.   :24.43   Min.   :20.92   Min.   :19.97   Min.   :22.31   #>  1st Qu.:44.41   1st Qu.:42.12   1st Qu.:43.48   1st Qu.:43.03   #>  Median :48.94   Median :48.42   Median :50.79   Median :51.12   #>  Mean   :49.53   Mean   :48.86   Mean   :50.91   Mean   :50.67   #>  3rd Qu.:54.91   3rd Qu.:55.62   3rd Qu.:57.29   3rd Qu.:57.93   #>  Max.   :74.27   Max.   :76.62   Max.   :77.40   Max.   :79.93   #>  NA's   :3       NA's   :3       NA's   :3       NA's   :4       #>      conf7           conf8           conf9           conf10      #>  Min.   :22.15   Min.   :24.74   Min.   :25.16   Min.   :25.87   #>  1st Qu.:43.59   1st Qu.:42.81   1st Qu.:41.48   1st Qu.:43.07   #>  Median :48.51   Median :50.75   Median :50.66   Median :49.00   #>  Mean   :49.55   Mean   :50.61   Mean   :49.61   Mean   :49.69   #>  3rd Qu.:56.32   3rd Qu.:58.10   3rd Qu.:56.68   3rd Qu.:55.77   #>  Max.   :76.23   Max.   :80.01   Max.   :81.59   Max.   :78.00   #>  NA's   :5       NA's   :4       NA's   :4       NA's   :5       #>       rec1            rec2            rec3            rec4       #>  Min.   :47.39   Min.   :47.91   Min.   :46.79   Min.   :48.35   #>  1st Qu.:57.33   1st Qu.:55.79   1st Qu.:56.32   1st Qu.:56.51   #>  Median :60.48   Median :59.95   Median :60.16   Median :59.45   #>  Mean   :60.25   Mean   :59.90   Mean   :59.85   Mean   :59.74   #>  3rd Qu.:63.50   3rd Qu.:63.58   3rd Qu.:63.56   3rd Qu.:62.87   #>  Max.   :71.60   Max.   :71.43   Max.   :72.08   Max.   :74.07   #>  NA's   :3       NA's   :3       NA's   :3       NA's   :3       #>       rec5            rec6            rec7            rec8       #>  Min.   :48.28   Min.   :42.84   Min.   :46.67   Min.   :50.64   #>  1st Qu.:56.40   1st Qu.:56.95   1st Qu.:56.88   1st Qu.:56.58   #>  Median :59.35   Median :60.15   Median :60.15   Median :59.16   #>  Mean   :59.62   Mean   :60.16   Mean   :60.17   Mean   :59.62   #>  3rd Qu.:62.74   3rd Qu.:63.78   3rd Qu.:64.18   3rd Qu.:62.64   #>  Max.   :73.07   Max.   :71.56   Max.   :71.01   Max.   :72.50   #>  NA's   :6       NA's   :4       NA's   :4       NA's   :3       #>       rec9           rec10       #>  Min.   :45.66   Min.   :45.49   #>  1st Qu.:56.04   1st Qu.:56.17   #>  Median :59.40   Median :59.68   #>  Mean   :59.56   Mean   :59.47   #>  3rd Qu.:63.12   3rd Qu.:62.70   #>  Max.   :73.32   Max.   :72.59   #>  NA's   :4       NA's   :3"},{"path":"/articles/lecture_data_screen.html","id":"missing-rows","dir":"Articles","previous_headings":"","what":"Missing Rows","title":"Data Screening","text":"","code":"percent_missing <- function(x){sum(is.na(x))/length(x) * 100} missing <- apply(no_missing, 1, percent_missing) table(missing) #> missing #>                0 4.54545454545455 27.2727272727273 68.1818181818182  #>              139               15                1                2  #> 86.3636363636364  #>                1"},{"path":"/articles/lecture_data_screen.html","id":"missing-replacement","dir":"Articles","previous_headings":"","what":"Missing Replacement","title":"Data Screening","text":"much data can safely replace? Replace things make sense. Replace minimal possible, often less 5% Replace based completion/missingness type","code":"replace_rows <- subset(no_missing, missing <= 5) no_rows <- subset(no_missing, missing > 5)"},{"path":"/articles/lecture_data_screen.html","id":"missing-columns","dir":"Articles","previous_headings":"","what":"Missing Columns","title":"Data Screening","text":"Separate columns replace Make sure columns less 5% missing replacement","code":"missing <- apply(replace_rows, 2, percent_missing) table(missing) #> missing #>                 0 0.649350649350649   1.2987012987013  1.94805194805195  #>                12                 6                 3                 1  replace_columns <- replace_rows[ , 3:22] no_columns <- replace_rows[ , 1:2]"},{"path":"/articles/lecture_data_screen.html","id":"missing-replacement-1","dir":"Articles","previous_headings":"","what":"Missing Replacement","title":"Data Screening","text":"","code":"library(mice) #>  #> Attaching package: 'mice' #> The following object is masked from 'package:stats': #>  #>     filter #> The following objects are masked from 'package:base': #>  #>     cbind, rbind tempnomiss <- mice(replace_columns) #>  #>  iter imp variable #>   1   1  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   1   2  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   1   3  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   1   4  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   1   5  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   2   1  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   2   2  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   2   3  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   2   4  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   2   5  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   3   1  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   3   2  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   3   3  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   3   4  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   3   5  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   4   1  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   4   2  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   4   3  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   4   4  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   4   5  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   5   1  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   5   2  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   5   3  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   5   4  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9 #>   5   5  conf2  conf3  conf4  conf5  conf7  conf10  rec5  rec6  rec7  rec9"},{"path":"/articles/lecture_data_screen.html","id":"missing-put-together","dir":"Articles","previous_headings":"","what":"Missing Put Together","title":"Data Screening","text":"","code":"fixed_columns <- complete(tempnomiss) all_columns <- cbind(no_columns, fixed_columns) all_rows <- rbind(all_columns, no_rows) nrow(no_missing) #> [1] 158 nrow(all_rows) #> [1] 158"},{"path":"/articles/lecture_data_screen.html","id":"outliers","dir":"Articles","previous_headings":"","what":"Outliers","title":"Data Screening","text":"mostly concerned multivariate outliers SEM. rows data (participants) extremely weird patterns scores compared everyone else. use Mahalanobis Distance examine row determine outlier score D distance centriod mean means use cutoff score based strict screening criterion, p < .001 determine outlier cutoff criterion based number variables rather number observations","code":""},{"path":"/articles/lecture_data_screen.html","id":"outliers-mahalanobis","dir":"Articles","previous_headings":"","what":"Outliers Mahalanobis","title":"Data Screening","text":"","code":"mahal <- mahalanobis(all_columns[ , -c(1,2)], #take note here    colMeans(all_columns[ , -c(1,2)], na.rm=TRUE),   cov(all_columns[ , -c(1,2)], use =\"pairwise.complete.obs\"))  cutoff <- qchisq(p = 1 - .001, #1 minus alpha                  df = ncol(all_columns[ , -c(1,2)])) # number of columns"},{"path":"/articles/lecture_data_screen.html","id":"outliers-mahalanobis-1","dir":"Articles","previous_headings":"","what":"Outliers Mahalanobis","title":"Data Screening","text":"outliers really matter SEM analysis though?","code":"cutoff #> [1] 45.31475  summary(mahal < cutoff) #notice the direction  #>    Mode    TRUE  #> logical     154  no_outliers <- subset(all_columns, mahal < cutoff)"},{"path":"/articles/lecture_data_screen.html","id":"assumptions-additivity","dir":"Articles","previous_headings":"","what":"Assumptions Additivity","title":"Data Screening","text":"Additivity assumption variable adds something model basically want use variable twice, lowers power Often described multicollinearity Mainly, SEM analysis lot correlated variables, just want make sure aren’t perfectly correlated","code":""},{"path":"/articles/lecture_data_screen.html","id":"assumptions-additivity-1","dir":"Articles","previous_headings":"","what":"Assumptions Additivity","title":"Data Screening","text":"","code":"library(corrplot) #> corrplot 0.95 loaded corrplot(cor(no_outliers[ , -c(1,2)]))"},{"path":"/articles/lecture_data_screen.html","id":"assumptions-set-up","dir":"Articles","previous_headings":"","what":"Assumptions Set Up","title":"Data Screening","text":"","code":"random_variable <- rchisq(nrow(no_outliers), 7) fake_model <- lm(random_variable ~ .,                   data = no_outliers[ , -c(1,2)]) standardized <- rstudent(fake_model) fitvalues <- scale(fake_model$fitted.values)"},{"path":"/articles/lecture_data_screen.html","id":"assumptions-linearity","dir":"Articles","previous_headings":"","what":"Assumptions Linearity","title":"Data Screening","text":"assume multivariate relationship continuous variables linear (.e., curved) many ways test , can use QQ/PP Plot examine linearity","code":"plot(fake_model, 2)"},{"path":"/articles/lecture_data_screen.html","id":"assumptions-normality","dir":"Articles","previous_headings":"","what":"Assumptions Normality","title":"Data Screening","text":"expect residuals normally distributed sample normally distributed Generally, SEM requires large sample size, thus, buffering normality deviations","code":"hist(standardized)"},{"path":"/articles/lecture_data_screen.html","id":"assumptions-homogeneity-homoscedasticity","dir":"Articles","previous_headings":"","what":"Assumptions Homogeneity + Homoscedasticity","title":"Data Screening","text":"assumptions equality variances assume equal variances groups things like t-tests, ANOVA assumption equality spread variance across predicted values","code":"{plot(standardized, fitvalues)   abline(v = 0)   abline(h = 0) }"},{"path":"/articles/lecture_data_screen.html","id":"recap","dir":"Articles","previous_headings":"","what":"Recap","title":"Data Screening","text":"completed datascreening check dataset problems noted, discuss handle issues relevant SEM analysis Let’s check assignment!","code":""},{"path":"/articles/lecture_efa.html","id":"terminology","dir":"Articles","previous_headings":"","what":"Terminology","title":"Exploratory Factor Analysis","text":"Squares diagram measured directly Circles diagram","code":""},{"path":[]},{"path":"/articles/lecture_efa.html","id":"what-is-efa","dir":"Articles","previous_headings":"","what":"What is EFA?","title":"Exploratory Factor Analysis","text":"Explaining maximum amount common variance correlation matrix Using smallest number explanatory constructs (factors) Common variance overlapping variance items Unique variance variance related item (error variance)","code":""},{"path":"/articles/lecture_efa.html","id":"directionality","dir":"Articles","previous_headings":"","what":"Directionality","title":"Exploratory Factor Analysis","text":"Factors thought cause measured variables Therefore, saying measured variable predicted factor","code":""},{"path":"/articles/lecture_efa.html","id":"why-use-efa","dir":"Articles","previous_headings":"","what":"Why use EFA?","title":"Exploratory Factor Analysis","text":"Understand structure set variables Construct scale measure latent variable Reduce data set smaller size still measures original information","code":""},{"path":"/articles/lecture_efa.html","id":"the-example-data","dir":"Articles","previous_headings":"","what":"The Example Data","title":"Exploratory Factor Analysis","text":"going build scale measures anxiety statistics provokes students Dataset Field textbook Statistics makes cry friends think ’m stupid able cope R Standard deviations excite dream Pearson attacking correlation coefficients don’t understand statistics little experience computers computers hate never good mathematics friends better statistics Computers useful playing games badly mathematics school People try tell R makes statistics easier understand doesn’t worry cause irreparable damage incompetence computers Computers minds deliberately go wrong whenever use Computers get weep openly mention central tendency slip coma whenever see equation R always crashes try use Everybody looks use R can’t sleep thoughts eigenvectors wake duvet thinking trapped normal distribution friends better R ’m good statistics friends think ’m nerd","code":""},{"path":"/articles/lecture_efa.html","id":"the-example-data-1","dir":"Articles","previous_headings":"","what":"The Example Data","title":"Exploratory Factor Analysis","text":"","code":"library(rio) library(psych) master <- import(\"data/lecture_efa.csv\") head(master) #>   Q01 Q02 Q03 Q04 Q05 Q06 Q07 Q08 Q09 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 #> 1   4   5   2   4   4   4   3   5   5   4   5   4   4   4   4   3   5   4   3 #> 2   5   5   2   3   4   4   4   4   1   4   4   3   5   3   2   3   4   4   3 #> 3   4   3   4   4   2   5   4   4   4   4   3   3   4   2   4   3   4   3   5 #> 4   3   5   5   2   3   3   2   4   4   2   4   4   4   3   3   3   4   2   4 #> 5   4   5   3   4   4   3   3   4   2   4   4   3   3   4   4   4   4   3   3 #> 6   4   5   3   4   2   2   2   4   2   3   4   2   3   3   1   4   3   1   5 #>   Q20 Q21 Q22 Q23 #> 1   4   4   4   1 #> 2   2   2   2   4 #> 3   2   3   4   4 #> 4   2   2   2   3 #> 5   2   4   2   2 #> 6   1   3   5   2"},{"path":"/articles/lecture_efa.html","id":"steps-to-analysis","dir":"Articles","previous_headings":"","what":"Steps to Analysis","title":"Exploratory Factor Analysis","text":"Additionally, need least interval measurement analyses shown Enough items group factors (recommended 3-4 per potential factor) many factors use? Simple structure Adequate solutions","code":""},{"path":"/articles/lecture_efa.html","id":"how-many-factors-do-i-use","dir":"Articles","previous_headings":"","what":"How many factors do I use?","title":"Exploratory Factor Analysis","text":"Theory Kaiser criterion Scree plots Parallel analysis","code":""},{"path":"/articles/lecture_efa.html","id":"kaiser-criterion","dir":"Articles","previous_headings":"","what":"Kaiser criterion","title":"Exploratory Factor Analysis","text":"Old rule: extract number eigenvalues 1 New rule: extract number eigenvalues .7 mathematical representation variance accounted grouping items","code":""},{"path":[]},{"path":"/articles/lecture_efa.html","id":"parallel-analysis","dir":"Articles","previous_headings":"","what":"Parallel analysis","title":"Exploratory Factor Analysis","text":"Calculates eigenvalues data Randomizes data recalculates eigenvalues compares determine equal","code":""},{"path":"/articles/lecture_efa.html","id":"finding-factorscomponents","dir":"Articles","previous_headings":"","what":"Finding factors/components","title":"Exploratory Factor Analysis","text":"","code":"number_items <- fa.parallel(master, #data frame                             fm=\"ml\", #math                             fa=\"fa\") #only efa #> Parallel analysis suggests that the number of factors =  6  and the number of components =  NA"},{"path":"/articles/lecture_efa.html","id":"eigenvalues","dir":"Articles","previous_headings":"","what":"Eigenvalues","title":"Exploratory Factor Analysis","text":"","code":"sum(number_items$fa.values > 1) #> [1] 1 sum(number_items$fa.values > .7) #> [1] 2"},{"path":"/articles/lecture_efa.html","id":"simple-structure","dir":"Articles","previous_headings":"","what":"Simple structure","title":"Exploratory Factor Analysis","text":"math used achieve solution: maximum likelihood Rotation increase communality items aid interpretation","code":""},{"path":[]},{"path":"/articles/lecture_efa.html","id":"rotation-1","dir":"Articles","previous_headings":"","what":"Rotation","title":"Exploratory Factor Analysis","text":"Orthogonal assume uncorrelated factors: varimax, quartermax, equamax Oblique allows factors correlated: oblimin, promax even use orthogonal?","code":""},{"path":"/articles/lecture_efa.html","id":"simple-structuresolution","dir":"Articles","previous_headings":"","what":"Simple structure/solution","title":"Exploratory Factor Analysis","text":"Want related least .3 Remember r = .3 medium effect size ~10% variance Can eliminate items load poorly Difference scale development versus exploratory clustering","code":""},{"path":"/articles/lecture_efa.html","id":"run-an-efa","dir":"Articles","previous_headings":"","what":"Run an EFA","title":"Exploratory Factor Analysis","text":"","code":"EFA_fit <- fa(master, #data               nfactors = 2, #number of factors               rotate = \"oblimin\", #rotation               fm = \"ml\") #math #> Loading required namespace: GPArotation"},{"path":"/articles/lecture_efa.html","id":"look-at-the-results","dir":"Articles","previous_headings":"","what":"Look at the results","title":"Exploratory Factor Analysis","text":"","code":"EFA_fit #> Factor Analysis using method =  ml #> Call: fa(r = master, nfactors = 2, rotate = \"oblimin\", fm = \"ml\") #> Standardized loadings (pattern matrix) based upon correlation matrix #>       ML1   ML2    h2   u2 com #> Q01  0.41  0.21 0.310 0.69 1.5 #> Q02 -0.40  0.17 0.111 0.89 1.3 #> Q03 -0.62  0.02 0.369 0.63 1.0 #> Q04  0.49  0.17 0.366 0.63 1.2 #> Q05  0.45  0.10 0.269 0.73 1.1 #> Q06  0.55  0.00 0.301 0.70 1.0 #> Q07  0.66  0.02 0.454 0.55 1.0 #> Q08 -0.08  0.86 0.670 0.33 1.0 #> Q09 -0.44  0.25 0.128 0.87 1.6 #> Q10  0.40  0.02 0.167 0.83 1.0 #> Q11  0.16  0.66 0.593 0.41 1.1 #> Q12  0.70 -0.05 0.446 0.55 1.0 #> Q13  0.60  0.09 0.429 0.57 1.0 #> Q14  0.64  0.00 0.415 0.58 1.0 #> Q15  0.47  0.13 0.313 0.69 1.1 #> Q16  0.58  0.11 0.418 0.58 1.1 #> Q17  0.17  0.64 0.570 0.43 1.1 #> Q18  0.71 -0.02 0.493 0.51 1.0 #> Q19 -0.47  0.09 0.177 0.82 1.1 #> Q20  0.42 -0.01 0.169 0.83 1.0 #> Q21  0.62  0.03 0.407 0.59 1.0 #> Q22 -0.34  0.09 0.090 0.91 1.2 #> Q23 -0.15  0.03 0.018 0.98 1.1 #>  #>                        ML1  ML2 #> SS loadings           5.70 1.98 #> Proportion Var        0.25 0.09 #> Cumulative Var        0.25 0.33 #> Proportion Explained  0.74 0.26 #> Cumulative Proportion 0.74 1.00 #>  #>  With factor correlations of  #>      ML1  ML2 #> ML1 1.00 0.58 #> ML2 0.58 1.00 #>  #> Mean item complexity =  1.1 #> Test of the hypothesis that 2 factors are sufficient. #>  #> df null model =  253  with the objective function =  7.55 with Chi Square =  19334.49 #> df of  the model are 208  and the objective function was  1.13  #>  #> The root mean square of the residuals (RMSR) is  0.05  #> The df corrected root mean square of the residuals is  0.06  #>  #> The harmonic n.obs is  2571 with the empirical chi square  3481.13  with prob <  0  #> The total n.obs was  2571  with Likelihood Chi Square =  2893.46  with prob <  0  #>  #> Tucker Lewis Index of factoring reliability =  0.829 #> RMSEA index =  0.071  and the 90 % confidence intervals are  0.069 0.073 #> BIC =  1260.24 #> Fit based upon off diagonal values = 0.97 #> Measures of factor score adequacy              #>                                                    ML1  ML2 #> Correlation of (regression) scores with factors   0.95 0.91 #> Multiple R square of scores with factors          0.90 0.84 #> Minimum correlation of possible factor scores     0.81 0.67"},{"path":"/articles/lecture_efa.html","id":"item-23","dir":"Articles","previous_headings":"","what":"Item 23","title":"Exploratory Factor Analysis","text":"","code":"EFA_fit2 <- fa(master[ , -23], #data               nfactors = 2, #number of factors               rotate = \"oblimin\", #rotation               fm = \"ml\") #math  EFA_fit2 #> Factor Analysis using method =  ml #> Call: fa(r = master[, -23], nfactors = 2, rotate = \"oblimin\", fm = \"ml\") #> Standardized loadings (pattern matrix) based upon correlation matrix #>       ML1   ML2    h2   u2 com #> Q01  0.41  0.20 0.311 0.69 1.5 #> Q02 -0.40  0.16 0.108 0.89 1.3 #> Q03 -0.61  0.01 0.367 0.63 1.0 #> Q04  0.49  0.17 0.367 0.63 1.2 #> Q05  0.45  0.10 0.270 0.73 1.1 #> Q06  0.55  0.00 0.302 0.70 1.0 #> Q07  0.67  0.01 0.455 0.55 1.0 #> Q08 -0.08  0.86 0.672 0.33 1.0 #> Q09 -0.43  0.24 0.123 0.88 1.6 #> Q10  0.40  0.02 0.167 0.83 1.0 #> Q11  0.16  0.66 0.594 0.41 1.1 #> Q12  0.70 -0.05 0.448 0.55 1.0 #> Q13  0.60  0.09 0.430 0.57 1.0 #> Q14  0.65  0.00 0.417 0.58 1.0 #> Q15  0.47  0.13 0.313 0.69 1.1 #> Q16  0.58  0.11 0.418 0.58 1.1 #> Q17  0.17  0.64 0.570 0.43 1.1 #> Q18  0.72 -0.02 0.494 0.51 1.0 #> Q19 -0.46  0.09 0.175 0.82 1.1 #> Q20  0.42 -0.01 0.169 0.83 1.0 #> Q21  0.62  0.03 0.407 0.59 1.0 #> Q22 -0.34  0.09 0.086 0.91 1.1 #>  #>                        ML1  ML2 #> SS loadings           5.68 1.98 #> Proportion Var        0.26 0.09 #> Cumulative Var        0.26 0.35 #> Proportion Explained  0.74 0.26 #> Cumulative Proportion 0.74 1.00 #>  #>  With factor correlations of  #>      ML1  ML2 #> ML1 1.00 0.58 #> ML2 0.58 1.00 #>  #> Mean item complexity =  1.1 #> Test of the hypothesis that 2 factors are sufficient. #>  #> df null model =  231  with the objective function =  7.46 with Chi Square =  19107.61 #> df of  the model are 188  and the objective function was  1.06  #>  #> The root mean square of the residuals (RMSR) is  0.05  #> The df corrected root mean square of the residuals is  0.06  #>  #> The harmonic n.obs is  2571 with the empirical chi square  3094.97  with prob <  0  #> The total n.obs was  2571  with Likelihood Chi Square =  2705.25  with prob <  0  #>  #> Tucker Lewis Index of factoring reliability =  0.836 #> RMSEA index =  0.072  and the 90 % confidence intervals are  0.07 0.075 #> BIC =  1229.07 #> Fit based upon off diagonal values = 0.97 #> Measures of factor score adequacy              #>                                                    ML1  ML2 #> Correlation of (regression) scores with factors   0.95 0.91 #> Multiple R square of scores with factors          0.90 0.84 #> Minimum correlation of possible factor scores     0.81 0.67"},{"path":"/articles/lecture_efa.html","id":"plots-of-the-results","dir":"Articles","previous_headings":"","what":"Plots of the results","title":"Exploratory Factor Analysis","text":"built plot functions psych fantastic!","code":"fa.plot(EFA_fit2,       labels = colnames(master[ , -23]))"},{"path":"/articles/lecture_efa.html","id":"plots-of-the-results-1","dir":"Articles","previous_headings":"","what":"Plots of the results","title":"Exploratory Factor Analysis","text":"","code":"fa.diagram(EFA_fit2)"},{"path":"/articles/lecture_efa.html","id":"adequate-solution","dir":"Articles","previous_headings":"","what":"Adequate solution","title":"Exploratory Factor Analysis","text":"Goodness fit statistics: measure overlap reproduced correlation matrix original, want high numbers close 1 Badness fit statistics (residual): measure mismatch, want low numbers close zero Theory/interpretability Reliability","code":""},{"path":"/articles/lecture_efa.html","id":"fit-statistics","dir":"Articles","previous_headings":"","what":"Fit statistics","title":"Exploratory Factor Analysis","text":"","code":"EFA_fit2$rms #Root mean square of the residuals #> [1] 0.05104534 EFA_fit2$RMSEA #root mean squared error of approximation #>      RMSEA      lower      upper confidence  #> 0.07216508 0.06978342 0.07460328 0.90000000 EFA_fit2$TLI #tucker lewis index #> [1] 0.8360597 1 - ((EFA_fit2$STATISTIC-EFA_fit2$dof)/        (EFA_fit2$null.chisq-EFA_fit2$null.dof)) #CFI  #> [1] 0.866647"},{"path":"/articles/lecture_efa.html","id":"reliability","dir":"Articles","previous_headings":"","what":"Reliability","title":"Exploratory Factor Analysis","text":"","code":"factor1 = c(1:7, 9:10, 12:16, 18:22) factor2 = c(8, 11, 17) ##we use the psych::alpha to make sure that R knows we want the alpha function from the psych package. ##ggplot2 has an alpha function and if we have them both open at the same time ##you will sometimes get a color error without this :: information.  psych::alpha(master[, factor1], check.keys = T) #> Warning in psych::alpha(master[, factor1], check.keys = T): Some items were negatively correlated with the first principal component and were automatically reversed. #>  This is indicated by a negative sign for the variable name. #>  #> Reliability analysis    #> Call: psych::alpha(x = master[, factor1], check.keys = T) #>  #>   raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r #>       0.88      0.88    0.89      0.28 7.4 0.0034    3 0.56     0.27 #>  #>     95% confidence boundaries  #>          lower alpha upper #> Feldt     0.87  0.88  0.89 #> Duhachek  0.87  0.88  0.89 #>  #>  Reliability if an item is dropped: #>      raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r #> Q01       0.87      0.88    0.88      0.28 7.0   0.0036 0.013  0.27 #> Q02-      0.88      0.88    0.89      0.29 7.5   0.0035 0.013  0.30 #> Q03-      0.87      0.87    0.88      0.28 6.8   0.0037 0.014  0.26 #> Q04       0.87      0.87    0.88      0.28 6.9   0.0037 0.013  0.27 #> Q05       0.87      0.88    0.89      0.28 7.1   0.0036 0.014  0.27 #> Q06       0.87      0.88    0.88      0.28 7.0   0.0036 0.013  0.27 #> Q07       0.87      0.87    0.88      0.27 6.8   0.0038 0.013  0.26 #> Q09-      0.88      0.88    0.89      0.29 7.5   0.0033 0.013  0.30 #> Q10       0.88      0.88    0.89      0.29 7.3   0.0035 0.014  0.30 #> Q12       0.87      0.87    0.88      0.27 6.8   0.0037 0.013  0.26 #> Q13       0.87      0.87    0.88      0.27 6.8   0.0037 0.013  0.27 #> Q14       0.87      0.87    0.88      0.27 6.8   0.0037 0.013  0.27 #> Q15       0.87      0.87    0.88      0.28 7.0   0.0036 0.014  0.27 #> Q16       0.87      0.87    0.88      0.27 6.8   0.0037 0.013  0.26 #> Q18       0.87      0.87    0.88      0.27 6.7   0.0038 0.012  0.27 #> Q19-      0.88      0.88    0.89      0.29 7.2   0.0035 0.014  0.30 #> Q20       0.88      0.88    0.89      0.29 7.3   0.0035 0.014  0.30 #> Q21       0.87      0.87    0.88      0.27 6.8   0.0037 0.013  0.26 #> Q22-      0.88      0.88    0.89      0.29 7.5   0.0034 0.013  0.30 #>  #>  Item statistics  #>         n raw.r std.r r.cor r.drop mean   sd #> Q01  2571  0.55  0.57  0.54   0.49  3.6 0.83 #> Q02- 2571  0.38  0.38  0.33   0.31  1.6 0.85 #> Q03- 2571  0.65  0.65  0.62   0.59  2.6 1.08 #> Q04  2571  0.60  0.61  0.59   0.54  3.2 0.95 #> Q05  2571  0.55  0.56  0.52   0.48  3.3 0.96 #> Q06  2571  0.57  0.56  0.54   0.49  3.8 1.12 #> Q07  2571  0.68  0.68  0.66   0.62  3.1 1.10 #> Q09- 2571  0.40  0.38  0.32   0.30  2.8 1.26 #> Q10  2571  0.45  0.46  0.41   0.38  3.7 0.88 #> Q12  2571  0.67  0.67  0.66   0.61  2.8 0.92 #> Q13  2571  0.65  0.65  0.64   0.59  3.6 0.95 #> Q14  2571  0.65  0.65  0.63   0.59  3.1 1.00 #> Q15  2571  0.59  0.59  0.55   0.52  3.2 1.01 #> Q16  2571  0.66  0.67  0.66   0.61  3.1 0.92 #> Q18  2571  0.69  0.69  0.68   0.64  3.4 1.05 #> Q19- 2571  0.49  0.48  0.43   0.41  2.3 1.10 #> Q20  2571  0.47  0.47  0.42   0.39  2.4 1.04 #> Q21  2571  0.65  0.65  0.64   0.59  2.8 0.98 #> Q22- 2571  0.38  0.37  0.31   0.29  2.9 1.04 #>  #> Non missing response frequency for each item #>        1    2    3    4    5 miss #> Q01 0.02 0.07 0.29 0.52 0.11    0 #> Q02 0.01 0.04 0.08 0.31 0.56    0 #> Q03 0.03 0.17 0.34 0.26 0.19    0 #> Q04 0.05 0.17 0.36 0.37 0.05    0 #> Q05 0.04 0.18 0.29 0.43 0.06    0 #> Q06 0.06 0.10 0.13 0.44 0.27    0 #> Q07 0.09 0.24 0.26 0.34 0.07    0 #> Q09 0.08 0.28 0.23 0.20 0.20    0 #> Q10 0.02 0.10 0.18 0.57 0.14    0 #> Q12 0.09 0.23 0.46 0.20 0.02    0 #> Q13 0.03 0.12 0.25 0.48 0.12    0 #> Q14 0.07 0.18 0.38 0.31 0.06    0 #> Q15 0.06 0.18 0.30 0.39 0.07    0 #> Q16 0.06 0.16 0.42 0.33 0.04    0 #> Q18 0.06 0.12 0.31 0.37 0.14    0 #> Q19 0.02 0.15 0.22 0.33 0.29    0 #> Q20 0.22 0.37 0.25 0.15 0.02    0 #> Q21 0.09 0.29 0.34 0.26 0.02    0 #> Q22 0.05 0.26 0.34 0.26 0.10    0 psych::alpha(master[, factor2], check.keys = T) #>  #> Reliability analysis    #> Call: psych::alpha(x = master[, factor2], check.keys = T) #>  #>   raw_alpha std.alpha G6(smc) average_r S/N    ase mean   sd median_r #>       0.82      0.82    0.75       0.6 4.5 0.0062  3.7 0.75     0.59 #>  #>     95% confidence boundaries  #>          lower alpha upper #> Feldt     0.81  0.82  0.83 #> Duhachek  0.81  0.82  0.83 #>  #>  Reliability if an item is dropped: #>     raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r #> Q08      0.74      0.74    0.59      0.59 2.8    0.010    NA  0.59 #> Q11      0.74      0.74    0.59      0.59 2.9    0.010    NA  0.59 #> Q17      0.77      0.77    0.63      0.63 3.4    0.009    NA  0.63 #>  #>  Item statistics  #>        n raw.r std.r r.cor r.drop mean   sd #> Q08 2571  0.86  0.86  0.76   0.68  3.8 0.87 #> Q11 2571  0.86  0.86  0.75   0.68  3.7 0.88 #> Q17 2571  0.85  0.85  0.72   0.65  3.5 0.88 #>  #> Non missing response frequency for each item #>        1    2    3    4    5 miss #> Q08 0.03 0.06 0.19 0.58 0.15    0 #> Q11 0.02 0.06 0.22 0.53 0.16    0 #> Q17 0.03 0.10 0.27 0.52 0.08    0"},{"path":"/articles/lecture_efa.html","id":"interpretation","dir":"Articles","previous_headings":"","what":"Interpretation","title":"Exploratory Factor Analysis","text":"Factor 1: Statistics makes cry friends think ’m stupid able cope R Standard deviations excite dream Pearson attacking correlation coefficients don’t understand statistics little experience computers computers hate friends better statistics Computers useful playing games People try tell R makes statistics easier understand doesn’t worry cause irreparable damage incompetence computers Computers minds deliberately go wrong whenever use Computers get weep openly mention central tendency R always crashes try use Everybody looks use R can’t sleep thoughts eigenvectors wake duvet thinking trapped normal distribution friends better R Factor 2: never good mathematics badly mathematics school slip coma whenever see equation Bad: ’m good statistics friends think ’m nerd","code":""},{"path":"/articles/lecture_efa.html","id":"wrapping-up","dir":"Articles","previous_headings":"","what":"Wrapping Up","title":"Exploratory Factor Analysis","text":"’ve learned exploratory factor analysis, revisit cover confirmatory factor analysis learned examine number possible latent variables learned determine simple structure learned determine simple structure adequate model","code":""},{"path":"/articles/lecture_introR.html","id":"overall-note","dir":"Articles","previous_headings":"","what":"Overall Note","title":"Introduction to R","text":"can learn R. get frustrated. Google friend. Try Googling specific error message first. try googling specific function error. Try bunch different search terms.","code":""},{"path":"/articles/lecture_introR.html","id":"helpful-websites","dir":"Articles","previous_headings":"","what":"Helpful Websites","title":"Introduction to R","text":"Quick-R: www.statmethods.net R documentation: www.rdocumentation.org Swirl: www.swirlstats.com Stack Overflow: www.stackoverflow.com Learn Statistics R: https://learningstatisticswithr.com/","code":""},{"path":"/articles/lecture_introR.html","id":"download-requirements","dir":"Articles","previous_headings":"","what":"Download Requirements","title":"Introduction to R","text":"Get R: http://cran.r-project.org/ Mac: XQuartz https://www.xquartz.org/ (find ) RStudio: http://www.rstudio.org/","code":""},{"path":"/articles/lecture_introR.html","id":"outline","dir":"Articles","previous_headings":"","what":"Outline","title":"Introduction to R","text":"Commands Object Types Subsetting Missing Data Working Directories Packages Functions","code":""},{"path":"/articles/lecture_introR.html","id":"commands","dir":"Articles","previous_headings":"","what":"Commands","title":"Introduction to R","text":"Commands code tell R . can simple complex. Maybe ’s typo, maybe ’s misunderstanding code ","code":""},{"path":"/articles/lecture_introR.html","id":"commands-1","dir":"Articles","previous_headings":"","what":"Commands","title":"Introduction to R","text":"can type command directly console can type document (Script Markdown) tell run console","code":"X <- 4"},{"path":"/articles/lecture_introR.html","id":"commands-2","dir":"Articles","previous_headings":"","what":"Commands","title":"Introduction to R","text":"> indicates console ready code + indicates haven’t finished code block Capitalization symbols matter = <- equivalent Hit arrow – can scroll last commands run Hit tab key – ’ll get list variable names options select Use ? followed command learn ","code":""},{"path":"/articles/lecture_introR.html","id":"commands-3","dir":"Articles","previous_headings":"","what":"Commands","title":"Introduction to R","text":"console move around? run code? Script? Markdown? run code ?","code":""},{"path":"/articles/lecture_introR.html","id":"rstudio","dir":"Articles","previous_headings":"","what":"RStudio","title":"Introduction to R","text":"windows RStudio? Current files open like scripts, markdown, etc. magic happens everything runs Tells saved working environment variables types variables made Allows click view Shows file viewer, pictures/plots, packages, help!","code":""},{"path":"/articles/lecture_introR.html","id":"object-types","dir":"Articles","previous_headings":"","what":"Object Types","title":"Introduction to R","text":"Vectors Lists Matrices Data Frames Character Factor (special type character) Numeric/Integer/Complex Logical (True, False) NaN (versus NA) Last, objects can attributes (names)","code":""},{"path":"/articles/lecture_introR.html","id":"objects-example","dir":"Articles","previous_headings":"","what":"Objects Example","title":"Introduction to R","text":"","code":"library(palmerpenguins) data(penguins) attributes(penguins) #> $class #> [1] \"tbl_df\"     \"tbl\"        \"data.frame\" #>  #> $row.names #>   [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18 #>  [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36 #>  [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54 #>  [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72 #>  [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90 #>  [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 #> [109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 #> [127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 #> [145] 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 #> [163] 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 #> [181] 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 #> [199] 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 #> [217] 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 #> [235] 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 #> [253] 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 #> [271] 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 #> [289] 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 #> [307] 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 #> [325] 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 #> [343] 343 344 #>  #> $names #> [1] \"species\"           \"island\"            \"bill_length_mm\"    #> [4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"       #> [7] \"sex\"               \"year\""},{"path":"/articles/lecture_introR.html","id":"objects-example-1","dir":"Articles","previous_headings":"","what":"Objects Example","title":"Introduction to R","text":"","code":"str(penguins) #> tibble [344 × 8] (S3: tbl_df/tbl/data.frame) #>  $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ... #>  $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ... #>  $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ... #>  $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ... #>  $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ... #>  $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ... #>  $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ... #>  $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...  names(penguins) #ls(penguins) provides this as well  #> [1] \"species\"           \"island\"            \"bill_length_mm\"    #> [4] \"bill_depth_mm\"     \"flipper_length_mm\" \"body_mass_g\"       #> [7] \"sex\"               \"year\""},{"path":"/articles/lecture_introR.html","id":"vectors","dir":"Articles","previous_headings":"","what":"Vectors","title":"Introduction to R","text":"can think vector one row column data objects must class try mix match, coerce type make NA . [1] indicates number first item printed row","code":"X #> [1] 4 penguins$species #>   [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>   [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #>  [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #> [106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #> [113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #> [120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #> [127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #> [134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #> [141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    #> [148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo    #> [155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    #> [274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap #> [281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap #> [288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap #> [295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap #> [302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap #> [309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap #> [316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap #> [323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap #> [330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap #> [337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap #> [344] Chinstrap #> Levels: Adelie Chinstrap Gentoo"},{"path":"/articles/lecture_introR.html","id":"vector-examples","dir":"Articles","previous_headings":"","what":"Vector Examples","title":"Introduction to R","text":"","code":"A <- 1:20 A #>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20  B <- seq(from = 1, to = 20, by = 1) B #>  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20  C <- c(\"cheese\", \"is\", \"great\") C #> [1] \"cheese\" \"is\"     \"great\"  D <- rep(1, times = 30) D #>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1"},{"path":"/articles/lecture_introR.html","id":"value-types","dir":"Articles","previous_headings":"","what":"Value Types","title":"Introduction to R","text":"","code":"class(A) #> [1] \"integer\" class(C) #> [1] \"character\" class(penguins) #> [1] \"tbl_df\"     \"tbl\"        \"data.frame\" class(penguins$species) #> [1] \"factor\""},{"path":"/articles/lecture_introR.html","id":"functions-vary","dir":"Articles","previous_headings":"","what":"Functions Vary","title":"Introduction to R","text":"Functions commands running –> things like class(), rep() code typed () called arguments output varies based type variable put arguments","code":"dim(penguins) #rows, columns #> [1] 344   8 length(penguins) #> [1] 8 length(penguins$species) #> [1] 344"},{"path":"/articles/lecture_introR.html","id":"lists","dir":"Articles","previous_headings":"","what":"Lists","title":"Introduction to R","text":"vectors one row data, might want multiple rows types vector, key understand type Lists grouping variables can multiple types (list items) can different lengths Often function output saved list reason usually names help print just small part list","code":"output <- lm(flipper_length_mm ~ bill_length_mm, data = penguins) str(output) #> List of 13 #>  $ coefficients : Named num [1:2] 126.68 1.69 #>   ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"bill_length_mm\" #>  $ residuals    : Named num [1:342] -11.766 -7.442 0.206 4.29 -3.104 ... #>   ..- attr(*, \"names\")= chr [1:342] \"1\" \"2\" \"3\" \"5\" ... #>  $ effects      : Named num [1:342] -3715.57 170.39 1.03 5.35 -2.22 ... #>   ..- attr(*, \"names\")= chr [1:342] \"(Intercept)\" \"bill_length_mm\" \"\" \"\" ... #>  $ rank         : int 2 #>  $ fitted.values: Named num [1:342] 193 193 195 189 193 ... #>   ..- attr(*, \"names\")= chr [1:342] \"1\" \"2\" \"3\" \"5\" ... #>  $ assign       : int [1:2] 0 1 #>  $ qr           :List of 5 #>   ..$ qr   : num [1:342, 1:2] -18.4932 0.0541 0.0541 0.0541 0.0541 ... #>   .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. ..$ : chr [1:342] \"1\" \"2\" \"3\" \"5\" ... #>   .. .. ..$ : chr [1:2] \"(Intercept)\" \"bill_length_mm\" #>   .. ..- attr(*, \"assign\")= int [1:2] 0 1 #>   ..$ qraux: num [1:2] 1.05 1.04 #>   ..$ pivot: int [1:2] 1 2 #>   ..$ tol  : num 1e-07 #>   ..$ rank : int 2 #>   ..- attr(*, \"class\")= chr \"qr\" #>  $ df.residual  : int 340 #>  $ na.action    : 'omit' Named int [1:2] 4 272 #>   ..- attr(*, \"names\")= chr [1:2] \"4\" \"272\" #>  $ xlevels      : Named list() #>  $ call         : language lm(formula = flipper_length_mm ~ bill_length_mm, data = penguins) #>  $ terms        :Classes 'terms', 'formula'  language flipper_length_mm ~ bill_length_mm #>   .. ..- attr(*, \"variables\")= language list(flipper_length_mm, bill_length_mm) #>   .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1 #>   .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. ..$ : chr [1:2] \"flipper_length_mm\" \"bill_length_mm\" #>   .. .. .. ..$ : chr \"bill_length_mm\" #>   .. ..- attr(*, \"term.labels\")= chr \"bill_length_mm\" #>   .. ..- attr(*, \"order\")= int 1 #>   .. ..- attr(*, \"intercept\")= int 1 #>   .. ..- attr(*, \"response\")= int 1 #>   .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv>  #>   .. ..- attr(*, \"predvars\")= language list(flipper_length_mm, bill_length_mm) #>   .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\" #>   .. .. ..- attr(*, \"names\")= chr [1:2] \"flipper_length_mm\" \"bill_length_mm\" #>  $ model        :'data.frame':   342 obs. of  2 variables: #>   ..$ flipper_length_mm: int [1:342] 181 186 195 193 190 181 195 193 190 186 ... #>   ..$ bill_length_mm   : num [1:342] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 34.1 42 37.8 ... #>   ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language flipper_length_mm ~ bill_length_mm #>   .. .. ..- attr(*, \"variables\")= language list(flipper_length_mm, bill_length_mm) #>   .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1 #>   .. .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. .. .. .. ..$ : chr [1:2] \"flipper_length_mm\" \"bill_length_mm\" #>   .. .. .. .. ..$ : chr \"bill_length_mm\" #>   .. .. ..- attr(*, \"term.labels\")= chr \"bill_length_mm\" #>   .. .. ..- attr(*, \"order\")= int 1 #>   .. .. ..- attr(*, \"intercept\")= int 1 #>   .. .. ..- attr(*, \"response\")= int 1 #>   .. .. ..- attr(*, \".Environment\")=<environment: R_GlobalEnv>  #>   .. .. ..- attr(*, \"predvars\")= language list(flipper_length_mm, bill_length_mm) #>   .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\" #>   .. .. .. ..- attr(*, \"names\")= chr [1:2] \"flipper_length_mm\" \"bill_length_mm\" #>   ..- attr(*, \"na.action\")= 'omit' Named int [1:2] 4 272 #>   .. ..- attr(*, \"names\")= chr [1:2] \"4\" \"272\" #>  - attr(*, \"class\")= chr \"lm\" output$coefficients #>    (Intercept) bill_length_mm  #>     126.684427       1.690062"},{"path":"/articles/lecture_introR.html","id":"dimensional-data","dir":"Articles","previous_headings":"","what":"Dimensional Data","title":"Introduction to R","text":"Matrices vectors dimensions (like 2X3) data must type Like matrix, columns can different types classes","code":""},{"path":"/articles/lecture_introR.html","id":"matrix","dir":"Articles","previous_headings":"","what":"Matrix","title":"Introduction to R","text":"Let’s talk [ , ] [row, column] subset grab specific values","code":"myMatrix <- matrix(data = 1:10,                    nrow = 5,                    ncol = 2) myMatrix #>      [,1] [,2] #> [1,]    1    6 #> [2,]    2    7 #> [3,]    3    8 #> [4,]    4    9 #> [5,]    5   10"},{"path":"/articles/lecture_introR.html","id":"data-frames","dir":"Articles","previous_headings":"","what":"Data Frames","title":"Introduction to R","text":"data frames, can use [ , ] However, also attributes allow us use $ (lists !)","code":"penguins[1, 2:3] #> # A tibble: 1 × 2 #>   island    bill_length_mm #>   <fct>              <dbl> #> 1 Torgersen           39.1 penguins$sex[4:25] #why no comma? #>  [1] <NA>   female male   female male   <NA>   <NA>   <NA>   <NA>   female #> [11] male   male   female female male   female male   female male   female #> [21] male   male   #> Levels: female male"},{"path":"/articles/lecture_introR.html","id":"dimensional-data-1","dir":"Articles","previous_headings":"","what":"Dimensional Data","title":"Introduction to R","text":"want combine data? ’ve already talked c(). rbind() allows put together rows cbind() allows put together columns","code":"X <- 1:5 Y <- 6:10 # I can use either because they are the same size  cbind(X,Y) #>      X  Y #> [1,] 1  6 #> [2,] 2  7 #> [3,] 3  8 #> [4,] 4  9 #> [5,] 5 10 rbind(X,Y) #>   [,1] [,2] [,3] [,4] [,5] #> X    1    2    3    4    5 #> Y    6    7    8    9   10"},{"path":"/articles/lecture_introR.html","id":"remind-r-where-things-are","dir":"Articles","previous_headings":"","what":"Remind R Where Things Are","title":"Introduction to R","text":"Just know penguins open ’s variable called species … just use species","code":"ls() #>  [1] \"A\"            \"B\"            \"C\"            \"D\"            \"myMatrix\"     #>  [6] \"output\"       \"penguins\"     \"penguins_raw\" \"X\"            \"Y\" ls(penguins) #> [1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"       #> [4] \"flipper_length_mm\" \"island\"            \"sex\"               #> [7] \"species\"           \"year\""},{"path":"/articles/lecture_introR.html","id":"converting-object-types","dir":"Articles","previous_headings":"","what":"Converting Object Types","title":"Introduction to R","text":"can use . functions convert types Show . see available careful though!","code":"newDF <- as.data.frame(cbind(X,Y)) str(newDF) #> 'data.frame':    5 obs. of  2 variables: #>  $ X: int  1 2 3 4 5 #>  $ Y: int  6 7 8 9 10 as.numeric(c(\"one\", \"two\", \"3\")) #> Warning: NAs introduced by coercion #> [1] NA NA  3"},{"path":"/articles/lecture_introR.html","id":"subsetting","dir":"Articles","previous_headings":"","what":"Subsetting","title":"Introduction to R","text":"Subsetting parceling rows/columns need given criteria. already talked select one row/column [1,] [,1] $ operator. cases want select based scores, missing data, etc.?","code":""},{"path":"/articles/lecture_introR.html","id":"subsetting-examples","dir":"Articles","previous_headings":"","what":"Subsetting Examples","title":"Introduction to R","text":"analyzes row/column appropriate logical question asking bill length greater 54 got back rows length greater 54 Careful put (,)","code":"penguins[1:2,] #just the first two rows  #> # A tibble: 2 × 8 #>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> #> 1 Adelie  Torgersen           39.1          18.7               181        3750 #> 2 Adelie  Torgersen           39.5          17.4               186        3800 #> # ℹ 2 more variables: sex <fct>, year <int> penguins[penguins$bill_length_mm > 54 , ] #how does this work? #> # A tibble: 9 × 8 #>   species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #>   <fct>     <fct>           <dbl>         <dbl>             <int>       <int> #> 1 NA        NA               NA            NA                  NA          NA #> 2 Gentoo    Biscoe           59.6          17                 230        6050 #> 3 Gentoo    Biscoe           54.3          15.7               231        5650 #> 4 Gentoo    Biscoe           55.9          17                 228        5600 #> 5 Gentoo    Biscoe           55.1          16                 230        5850 #> 6 NA        NA               NA            NA                  NA          NA #> 7 Chinstrap Dream            58            17.8               181        3700 #> 8 Chinstrap Dream            54.2          20.8               201        4300 #> 9 Chinstrap Dream            55.8          19.8               207        4000 #> # ℹ 2 more variables: sex <fct>, year <int> penguins$bill_length_mm > 54 #>   [1] FALSE FALSE FALSE    NA FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>  [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>  [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>  [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>  [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>  [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #>  [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [181] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE #> [193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE #> [217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [253] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [265] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE    NA FALSE FALSE FALSE FALSE #> [277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [289] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE #> [301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE #> [313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #> [337] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE"},{"path":"/articles/lecture_introR.html","id":"subsetting-examples-1","dir":"Articles","previous_headings":"","what":"Subsetting Examples","title":"Introduction to R","text":"","code":"#you can create complex rules penguins[penguins$bill_length_mm > 54 & penguins$bill_depth_mm > 17, ] #> # A tibble: 5 × 8 #>   species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #>   <fct>     <fct>           <dbl>         <dbl>             <int>       <int> #> 1 NA        NA               NA            NA                  NA          NA #> 2 NA        NA               NA            NA                  NA          NA #> 3 Chinstrap Dream            58            17.8               181        3700 #> 4 Chinstrap Dream            54.2          20.8               201        4300 #> 5 Chinstrap Dream            55.8          19.8               207        4000 #> # ℹ 2 more variables: sex <fct>, year <int> #you can do all BUT penguins[ , -1] #> # A tibble: 344 × 7 #>    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex    year #>    <fct>           <dbl>         <dbl>             <int>       <int> <fct> <int> #>  1 Torge…           39.1          18.7               181        3750 male   2007 #>  2 Torge…           39.5          17.4               186        3800 fema…  2007 #>  3 Torge…           40.3          18                 195        3250 fema…  2007 #>  4 Torge…           NA            NA                  NA          NA NA     2007 #>  5 Torge…           36.7          19.3               193        3450 fema…  2007 #>  6 Torge…           39.3          20.6               190        3650 male   2007 #>  7 Torge…           38.9          17.8               181        3625 fema…  2007 #>  8 Torge…           39.2          19.6               195        4675 male   2007 #>  9 Torge…           34.1          18.1               193        3475 NA     2007 #> 10 Torge…           42            20.2               190        4250 NA     2007 #> # ℹ 334 more rows #grab a few columns by name vars <- c(\"bill_length_mm\", \"sex\") penguins[ , vars] #> # A tibble: 344 × 2 #>    bill_length_mm sex    #>             <dbl> <fct>  #>  1           39.1 male   #>  2           39.5 female #>  3           40.3 female #>  4           NA   NA     #>  5           36.7 female #>  6           39.3 male   #>  7           38.9 female #>  8           39.2 male   #>  9           34.1 NA     #> 10           42   NA     #> # ℹ 334 more rows"},{"path":"/articles/lecture_introR.html","id":"subsetting-1","dir":"Articles","previous_headings":"","what":"Subsetting","title":"Introduction to R","text":"","code":"#another function #notice any differences?  subset(penguins, bill_length_mm > 54) #> # A tibble: 7 × 8 #>   species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #>   <fct>     <fct>           <dbl>         <dbl>             <int>       <int> #> 1 Gentoo    Biscoe           59.6          17                 230        6050 #> 2 Gentoo    Biscoe           54.3          15.7               231        5650 #> 3 Gentoo    Biscoe           55.9          17                 228        5600 #> 4 Gentoo    Biscoe           55.1          16                 230        5850 #> 5 Chinstrap Dream            58            17.8               181        3700 #> 6 Chinstrap Dream            54.2          20.8               201        4300 #> 7 Chinstrap Dream            55.8          19.8               207        4000 #> # ℹ 2 more variables: sex <fct>, year <int> #other functions include filter() in tidyverse"},{"path":"/articles/lecture_introR.html","id":"missing-values","dir":"Articles","previous_headings":"","what":"Missing Values","title":"Introduction to R","text":"Missing values marked NA NaN stands number, doesn’t automatically convert missing functions option excluding NA values can slightly different","code":"head(complete.cases(penguins)) #creates logical #> [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE head(na.omit(penguins)) #creates actual rows #> # A tibble: 6 × 8 #>   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g #>   <fct>   <fct>              <dbl>         <dbl>             <int>       <int> #> 1 Adelie  Torgersen           39.1          18.7               181        3750 #> 2 Adelie  Torgersen           39.5          17.4               186        3800 #> 3 Adelie  Torgersen           40.3          18                 195        3250 #> 4 Adelie  Torgersen           36.7          19.3               193        3450 #> 5 Adelie  Torgersen           39.3          20.6               190        3650 #> 6 Adelie  Torgersen           38.9          17.8               181        3625 #> # ℹ 2 more variables: sex <fct>, year <int> head(is.na(penguins$body_mass_g)) #for individual vectors #> [1] FALSE FALSE FALSE  TRUE FALSE FALSE"},{"path":"/articles/lecture_introR.html","id":"working-directories","dir":"Articles","previous_headings":"","what":"Working Directories","title":"Introduction to R","text":"computer files folders, tell R look working directory currently telling look","code":"getwd() #> [1] \"/Users/erinbuchanan/GitHub/Research/1.5_packages/learnSEM/vignettes\""},{"path":"/articles/lecture_introR.html","id":"working-directory","dir":"Articles","previous_headings":"","what":"Working Directory","title":"Introduction to R","text":"can set working directory something like suggest pretty error prone breaks move files!","code":"setwd(\"/Users/buchanan/OneDrive - Harrisburg University/Teaching/ANLY 580/updated/1 Introduction R\")"},{"path":"/articles/lecture_introR.html","id":"working-directory-1","dir":"Articles","previous_headings":"","what":"Working Directory","title":"Introduction to R","text":"Working directories critical allow automate Instead using point click options, can just run code open specific files Markdown files best! Projects best!","code":""},{"path":"/articles/lecture_introR.html","id":"importing-files","dir":"Articles","previous_headings":"","what":"Importing Files","title":"Introduction to R","text":"can use base R functions (readLines, read.csv) can use tidyverse (read_csv) can use Import Dataset clickable option use one package like magic?","code":"library(rio) myDF <- import(\"data/assignment_introR.csv\") head(myDF) #>   expno rating orginalcode id speed error whichhand LR_switch finger_switch rha #> 1   1_2      8         faw  1    75    49      Left         0             2  -3 #> 2   1_2      5        resz  1    75    49      Left         0             3  -4 #> 3   1_2      4         saf  1    NA    49      Left         0             2  -3 #> 4   1_2      5        zers  1    75    49      Left         0             3  -4 #> 5   1_2      7         zet  1    75    49      Left         0             2  -3 #> 6   1_2      5        dafe  1    75    49      Left         0             3  -4 #>   word_length letter_freq real_fake speed_c #> 1           3    4.251667         1   15.17 #> 2           4    6.272500         1   15.17 #> 3           3    5.574000         1   15.17 #> 4           4    6.272500         1   15.17 #> 5           3    7.277333         1   15.17 #> 6           4    6.837500         1   15.17"},{"path":"/articles/lecture_introR.html","id":"packages","dir":"Articles","previous_headings":"","what":"Packages","title":"Introduction to R","text":"can install extra functions installing packages libraries can also install using Packages tab Additional packages can installed GitHub places","code":"install.packages(\"car\")"},{"path":"/articles/lecture_introR.html","id":"packages-1","dir":"Articles","previous_headings":"","what":"Packages","title":"Introduction to R","text":"View installed Packages window Every time get major R update, likely reinstall packages Helpful put library code right top scripts","code":"library(car) #> Loading required package: carData"},{"path":"/articles/lecture_introR.html","id":"functions","dir":"Articles","previous_headings":"","what":"Functions","title":"Introduction to R","text":"Functions pre-written code help run analyses Let’s flip back RStudio see ","code":"?lm help(lm)"},{"path":"/articles/lecture_introR.html","id":"functions-1","dir":"Articles","previous_headings":"","what":"Functions","title":"Introduction to R","text":"","code":"args(lm) #> function (formula, data, subset, weights, na.action, method = \"qr\",  #>     model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE,  #>     contrasts = NULL, offset, ...)  #> NULL example(lm) #>  #> lm> require(graphics) #>  #> lm> ## Annette Dobson (1990) \"An Introduction to Generalized Linear Models\". #> lm> ## Page 9: Plant Weight Data. #> lm> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14) #>  #> lm> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69) #>  #> lm> group <- gl(2, 10, 20, labels = c(\"Ctl\",\"Trt\")) #>  #> lm> weight <- c(ctl, trt) #>  #> lm> lm.D9 <- lm(weight ~ group) #>  #> lm> lm.D90 <- lm(weight ~ group - 1) # omitting intercept #>  #> lm> ## No test:  #> lm> ##D anova(lm.D9) #> lm> ##D summary(lm.D90) #> lm> ## End(No test) #> lm> opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0)) #>  #> lm> plot(lm.D9, las = 1)      # Residuals, Fitted, ... #>  #> lm> par(opar) #>  #> lm> ## Don't show:  #> lm> ## model frame : #> lm> stopifnot(identical(lm(weight ~ group, method = \"model.frame\"), #> lm+                     model.frame(lm.D9))) #>  #> lm> ## End(Don't show) #> lm> ### less simple examples in \"See Also\" above #> lm>  #> lm>  #> lm>"},{"path":"/articles/lecture_introR.html","id":"define-your-own-function","dir":"Articles","previous_headings":"","what":"Define Your Own Function","title":"Introduction to R","text":"Name function <- Define arguments inside () Define function inside {}","code":"pizza <- function(x){ x^2 } pizza(3) #> [1] 9"},{"path":"/articles/lecture_introR.html","id":"example-functions","dir":"Articles","previous_headings":"","what":"Example Functions","title":"Introduction to R","text":"","code":"table(penguins$species) #>  #>    Adelie Chinstrap    Gentoo  #>       152        68       124 summary(penguins$bill_length_mm) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's  #>   32.10   39.23   44.45   43.92   48.50   59.60       2"},{"path":"/articles/lecture_introR.html","id":"examples-functions-with-missing-data","dir":"Articles","previous_headings":"","what":"Examples Functions with Missing Data","title":"Introduction to R","text":"","code":"mean(penguins$bill_length_mm) #returns NA #> [1] NA mean(penguins$bill_length_mm, na.rm = TRUE) #> [1] 43.92193  cor(penguins[ , c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\")]) #>                   bill_length_mm bill_depth_mm flipper_length_mm #> bill_length_mm                 1            NA                NA #> bill_depth_mm                 NA             1                NA #> flipper_length_mm             NA            NA                 1 cor(penguins[ , c(\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\")],     use = \"pairwise.complete.obs\") #>                   bill_length_mm bill_depth_mm flipper_length_mm #> bill_length_mm         1.0000000    -0.2350529         0.6561813 #> bill_depth_mm         -0.2350529     1.0000000        -0.5838512 #> flipper_length_mm      0.6561813    -0.5838512         1.0000000"},{"path":"/articles/lecture_introR.html","id":"other-descriptive-functions","dir":"Articles","previous_headings":"","what":"Other Descriptive Functions","title":"Introduction to R","text":"cov() var() sd() scale()","code":""},{"path":"/articles/lecture_introR.html","id":"wrapping-up","dir":"Articles","previous_headings":"","what":"Wrapping Up","title":"Introduction to R","text":"basic programming terminology Specific R defaults issues Example functions use cases Practice!","code":""},{"path":"/articles/lecture_irt.html","id":"item-response-theory","dir":"Articles","previous_headings":"","what":"Item Response Theory","title":"Item Response Theory","text":"Many agree four response options can treated continuous without loss power interpretation. treat values categorical? assume underlying latent variable continuous?","code":""},{"path":"/articles/lecture_irt.html","id":"categorical-options","dir":"Articles","previous_headings":"","what":"Categorical Options","title":"Item Response Theory","text":"traditional factor analysis approach using ordered responses can talk item loading, eliminate bad questions, etc. lavaan framework, update cfa() include ordered argument Item Response Theory","code":""},{"path":"/articles/lecture_irt.html","id":"item-response-theory-1","dir":"Articles","previous_headings":"","what":"Item Response Theory","title":"Item Response Theory","text":"differences responses differences ability underlying trait CTT focuses reliability item correlation type analysis separate test person characteristics Focuses item measures latent trait, discrimination, guessing Additionally, two outcomes, can examine ordering, response choice options, ","code":""},{"path":"/articles/lecture_irt.html","id":"issues","dir":"Articles","previous_headings":"","what":"Issues","title":"Item Response Theory","text":"can run separate models dimension multitrait options IRT control latent variable, items uncorrelated","code":""},{"path":"/articles/lecture_irt.html","id":"item-response-theory-2","dir":"Articles","previous_headings":"","what":"Item Response Theory","title":"Item Response Theory","text":"3 item questionnaire Yes/scaling 8 response patterns Four total scores (0, 1, 2, 3)","code":""},{"path":"/articles/lecture_irt.html","id":"item-response-theory-3","dir":"Articles","previous_headings":"","what":"Item Response Theory","title":"Item Response Theory","text":"log probability curve theta probability correct response","code":""},{"path":"/articles/lecture_irt.html","id":"item-response-theory-4","dir":"Articles","previous_headings":"","what":"Item Response Theory","title":"Item Response Theory","text":"Theta – ability underlying latent variable score Also considered item performs best Can thought item difficulty","code":""},{"path":"/articles/lecture_irt.html","id":"item-response-theory-5","dir":"Articles","previous_headings":"","what":"Item Response Theory","title":"Item Response Theory","text":"Tells well item measures latent variable Larger values indicate better items","code":""},{"path":"/articles/lecture_irt.html","id":"item-response-theory-6","dir":"Articles","previous_headings":"","what":"Item Response Theory","title":"Item Response Theory","text":"lower level likelihood getting item correct","code":""},{"path":"/articles/lecture_irt.html","id":"item-response-theory-7","dir":"Articles","previous_headings":"","what":"Item Response Theory","title":"Item Response Theory","text":"Also known Rasch Model uses b Uses b Uses b, , c","code":""},{"path":"/articles/lecture_irt.html","id":"polytomous-irt","dir":"Articles","previous_headings":"","what":"Polytomous IRT","title":"Item Response Theory","text":"large portion IRT focuses dichotomous data (yes/, correct/incorrect) Scoring easier “right” “wrong” answers Focus ordering, meaning low scores represent lower abilities, high scores higher abilities Likert type scales","code":""},{"path":"/articles/lecture_irt.html","id":"polytomous-irt-1","dir":"Articles","previous_headings":"","what":"Polytomous IRT","title":"Item Response Theory","text":"Graded Response Model Generalized Partial Credit Model Partial Credit Model","code":""},{"path":"/articles/lecture_irt.html","id":"polytomous-irt-2","dir":"Articles","previous_headings":"","what":"Polytomous IRT","title":"Item Response Theory","text":"graded response model simplest can hard fit. Takes number categories – 1 creates mini 2PLs boundary points (1-rest, 2-rest, 3-rest, etc.). get probabilities scoring level higher","code":""},{"path":"/articles/lecture_irt.html","id":"polytomous-irt-3","dir":"Articles","previous_headings":"","what":"Polytomous IRT","title":"Item Response Theory","text":"generalized partial credit partial credit models account fact may category used equally Therefore, get mini 2PLs adjacent categories (1-2, 2-3, 3-4) categories ordered (often want), two estimations can similar. Another concern partial credit models making sure categories point likely answer (thresholds)","code":""},{"path":"/articles/lecture_irt.html","id":"polytomous-irt-4","dir":"Articles","previous_headings":"","what":"Polytomous IRT","title":"Item Response Theory","text":"Install mirt() library use multidimensional IRT package. covering multiple dimensional multigroup IRT, package can models polytomous estimation.","code":""},{"path":"/articles/lecture_irt.html","id":"irt-examples","dir":"Articles","previous_headings":"","what":"IRT Examples","title":"Item Response Theory","text":"Let’s start DIRT: Dichotomous IRT Dataset LSAT, scored right wrong","code":"library(ltm) #> Loading required package: MASS #> Loading required package: msm #> Loading required package: polycor library(mirt) #> Loading required package: stats4 #> Loading required package: lattice #>  #> Attaching package: 'mirt' #> The following object is masked from 'package:ltm': #>  #>     Science data(LSAT) head(LSAT) #>   Item 1 Item 2 Item 3 Item 4 Item 5 #> 1      0      0      0      0      0 #> 2      0      0      0      0      0 #> 3      0      0      0      0      0 #> 4      0      0      0      0      1 #> 5      0      0      0      0      1 #> 6      0      0      0      0      1"},{"path":"/articles/lecture_irt.html","id":"two-parameter-logistic","dir":"Articles","previous_headings":"","what":"Two Parameter Logistic","title":"Item Response Theory","text":"","code":"# Data frame name ~ z1 for one latent variable #irt.param to give it to you standardized LSAT.model <- ltm(LSAT ~ z1,                   IRT.param = TRUE)"},{"path":"/articles/lecture_irt.html","id":"pl-output","dir":"Articles","previous_headings":"","what":"2PL Output","title":"Item Response Theory","text":"Difficulty = b = theta = ability Discrimination = = good question figuring person .","code":"coef(LSAT.model) #>            Dffclt    Dscrmn #> Item 1 -3.3597341 0.8253715 #> Item 2 -1.3696497 0.7229499 #> Item 3 -0.2798983 0.8904748 #> Item 4 -1.8659189 0.6885502 #> Item 5 -3.1235725 0.6574516"},{"path":"/articles/lecture_irt.html","id":"pl-plots","dir":"Articles","previous_headings":"","what":"2PL Plots","title":"Item Response Theory","text":"","code":"plot(LSAT.model, type = \"ICC\") ## all items at once"},{"path":"/articles/lecture_irt.html","id":"pl-plots-1","dir":"Articles","previous_headings":"","what":"2PL Plots","title":"Item Response Theory","text":"","code":"plot(LSAT.model, type = \"IIC\", items = 0) ## Test Information Function"},{"path":"/articles/lecture_irt.html","id":"pl-other-options","dir":"Articles","previous_headings":"","what":"2PL Other Options","title":"Item Response Theory","text":"","code":"factor.scores(LSAT.model) #>  #> Call: #> ltm(formula = LSAT ~ z1, IRT.param = TRUE) #>  #> Scoring Method: Empirical Bayes #>  #> Factor-Scores for observed response patterns: #>    Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1 #> 1       0      0      0      0      0   3   2.277 -1.895 0.795 #> 2       0      0      0      0      1   6   5.861 -1.479 0.796 #> 3       0      0      0      1      0   2   2.596 -1.460 0.796 #> 4       0      0      0      1      1  11   8.942 -1.041 0.800 #> 5       0      0      1      0      0   1   0.696 -1.331 0.797 #> 6       0      0      1      0      1   1   2.614 -0.911 0.802 #> 7       0      0      1      1      0   3   1.179 -0.891 0.803 #> 8       0      0      1      1      1   4   5.955 -0.463 0.812 #> 9       0      1      0      0      0   1   1.840 -1.438 0.796 #> 10      0      1      0      0      1   8   6.431 -1.019 0.801 #> 11      0      1      0      1      1  16  13.577 -0.573 0.809 #> 12      0      1      1      0      1   3   4.370 -0.441 0.813 #> 13      0      1      1      1      0   2   2.000 -0.420 0.813 #> 14      0      1      1      1      1  15  13.920  0.023 0.828 #> 15      1      0      0      0      0  10   9.480 -1.373 0.797 #> 16      1      0      0      0      1  29  34.616 -0.953 0.802 #> 17      1      0      0      1      0  14  15.590 -0.933 0.802 #> 18      1      0      0      1      1  81  76.562 -0.506 0.811 #> 19      1      0      1      0      0   3   4.659 -0.803 0.804 #> 20      1      0      1      0      1  28  24.989 -0.373 0.815 #> 21      1      0      1      1      0  15  11.463 -0.352 0.815 #> 22      1      0      1      1      1  80  83.541  0.093 0.831 #> 23      1      1      0      0      0  16  11.254 -0.911 0.802 #> 24      1      1      0      0      1  56  56.105 -0.483 0.812 #> 25      1      1      0      1      0  21  25.646 -0.463 0.812 #> 26      1      1      0      1      1 173 173.310 -0.022 0.827 #> 27      1      1      1      0      0  11   8.445 -0.329 0.816 #> 28      1      1      1      0      1  61  62.520  0.117 0.832 #> 29      1      1      1      1      0  28  29.127  0.139 0.833 #> 30      1      1      1      1      1 298 296.693  0.606 0.855"},{"path":"/articles/lecture_irt.html","id":"three-parameter-logistic","dir":"Articles","previous_headings":"","what":"Three Parameter Logistic","title":"Item Response Theory","text":"","code":"LSAT.model2 <- tpm(LSAT, #dataset                    type = \"latent.trait\",                    IRT.param = TRUE) #> Warning in tpm(LSAT, type = \"latent.trait\", IRT.param = TRUE): Hessian matrix at convergence is not positive definite; unstable solution."},{"path":"/articles/lecture_irt.html","id":"pl-output-1","dir":"Articles","previous_headings":"","what":"3PL Output","title":"Item Response Theory","text":"Difficulty = b = theta = ability Discrimination = = good question figuring person . Guessing = c = easy item guess","code":"coef(LSAT.model2) #>            Gussng     Dffclt     Dscrmn #> Item 1 0.06389395 -3.3423509  0.8048523 #> Item 2 0.01567005 -1.5530954  0.6070241 #> Item 3 0.30088256  0.3301527 26.4150208 #> Item 4 0.06521055 -2.0342571  0.5700252 #> Item 5 0.02908352 -3.5826451  0.5523586"},{"path":"/articles/lecture_irt.html","id":"pl-plots-2","dir":"Articles","previous_headings":"","what":"3PL Plots","title":"Item Response Theory","text":"","code":"plot(LSAT.model2, type = \"ICC\") ## all items at once"},{"path":"/articles/lecture_irt.html","id":"pl-plots-3","dir":"Articles","previous_headings":"","what":"3PL Plots","title":"Item Response Theory","text":"","code":"plot(LSAT.model2, type = \"IIC\", items = 0) ## Test Information Function"},{"path":"/articles/lecture_irt.html","id":"pl-other-options-1","dir":"Articles","previous_headings":"","what":"3PL Other Options","title":"Item Response Theory","text":"","code":"factor.scores(LSAT.model2) #>  #> Call: #> tpm(data = LSAT, type = \"latent.trait\", IRT.param = TRUE) #>  #> Scoring Method: Empirical Bayes #>  #> Factor-Scores for observed response patterns: #>    Item 1 Item 2 Item 3 Item 4 Item 5 Obs     Exp     z1 se.z1 #> 1       0      0      0      0      0   3   1.538 -1.659 0.865 #> 2       0      0      0      0      1   6   5.113 -1.245 0.876 #> 3       0      0      0      1      0   2   2.375 -1.245 0.879 #> 4       0      0      0      1      1  11   9.552 -0.815 0.891 #> 5       0      0      1      0      0   1   0.686 -1.659 0.865 #> 6       0      0      1      0      1   1   2.472 -1.245 0.876 #> 7       0      0      1      1      0   3   1.149 -1.245 0.879 #> 8       0      0      1      1      1   4   5.597 -0.815 0.891 #> 9       0      1      0      0      0   1   1.678 -1.205 0.878 #> 10      0      1      0      0      1   8   6.870 -0.777 0.890 #> 11      0      1      0      1      1  16  15.339 -0.330 0.906 #> 12      0      1      1      0      1   3   4.118 -0.777 0.890 #> 13      0      1      1      1      0   2   1.917 -0.774 0.893 #> 14      0      1      1      1      1  15  13.227 -0.330 0.906 #> 15      1      0      0      0      0  10   7.980 -1.053 0.883 #> 16      1      0      0      0      1  29  34.733 -0.619 0.895 #> 17      1      0      0      1      0  14  16.116 -0.616 0.898 #> 18      1      0      0      1      1  81  81.511 -0.166 0.910 #> 19      1      0      1      0      0   3   4.134 -1.053 0.883 #> 20      1      0      1      0      1  28  23.238 -0.619 0.895 #> 21      1      0      1      1      0  15  10.828 -0.616 0.898 #> 22      1      0      1      1      1  80  83.177 -0.166 0.912 #> 23      1      1      0      0      0  16  11.668 -0.577 0.897 #> 24      1      1      0      0      1  56  59.731 -0.129 0.908 #> 25      1      1      0      1      0  21  27.722 -0.122 0.910 #> 26      1      1      0      1      1 173 158.900  0.150 0.376 #> 27      1      1      1      0      0  11   8.068 -0.577 0.897 #> 28      1      1      1      0      1  61  63.489 -0.128 0.913 #> 29      1      1      1      1      0  28  29.695 -0.122 0.916 #> 30      1      1      1      1      1 298 303.369  0.503 0.407"},{"path":"/articles/lecture_irt.html","id":"compare-models","dir":"Articles","previous_headings":"","what":"Compare Models","title":"Item Response Theory","text":"","code":"anova(LSAT.model, LSAT.model2) #> Warning in anova.ltm(LSAT.model, LSAT.model2): either the two models are not nested or the model represented by 'object2' fell on a local maxima. #>  #>  Likelihood Ratio Table #>                 AIC     BIC  log.Lik   LRT df p.value #> LSAT.model  4953.31 5002.38 -2466.65                  #> LSAT.model2 4967.02 5040.64 -2468.51 -3.71  5       1"},{"path":"/articles/lecture_irt.html","id":"polytomous-irt-5","dir":"Articles","previous_headings":"","what":"Polytomous IRT","title":"Item Response Theory","text":"Dataset includes Meaning Life Questionnaire","code":"library(rio) poly.data <- import(\"data/lecture_irt.csv\") poly.data <- na.omit(poly.data)  #reverse code poly.data$Q99_9 = 8 - poly.data$Q99_9  #separate factors poly.data1 = poly.data[ , c(1, 4, 5, 6, 9)] poly.data2 = poly.data[ , c(2, 3, 7, 8, 10)]"},{"path":"/articles/lecture_irt.html","id":"graded-partial-credit-model","dir":"Articles","previous_headings":"","what":"Graded Partial Credit Model","title":"Item Response Theory","text":"","code":"gpcm.model1 <- mirt(data = poly.data1, #data                     model = 1, #number of factors                     itemtype = \"gpcm\") #poly model type #> Iteration: 1, Log-Lik: -11632.167, Max-Change: 4.82538Iteration: 2, Log-Lik: -10643.196, Max-Change: 2.80519Iteration: 3, Log-Lik: -10466.648, Max-Change: 1.46456Iteration: 4, Log-Lik: -10407.465, Max-Change: 1.07570Iteration: 5, Log-Lik: -10391.165, Max-Change: 0.70497Iteration: 6, Log-Lik: -10380.881, Max-Change: 0.46272Iteration: 7, Log-Lik: -10377.111, Max-Change: 0.31365Iteration: 8, Log-Lik: -10374.263, Max-Change: 0.21356Iteration: 9, Log-Lik: -10372.203, Max-Change: 0.38073Iteration: 10, Log-Lik: -10369.957, Max-Change: 0.15524Iteration: 11, Log-Lik: -10368.478, Max-Change: 0.22565Iteration: 12, Log-Lik: -10367.024, Max-Change: 0.16375Iteration: 13, Log-Lik: -10364.773, Max-Change: 0.09306Iteration: 14, Log-Lik: -10363.888, Max-Change: 0.14681Iteration: 15, Log-Lik: -10363.042, Max-Change: 0.13197Iteration: 16, Log-Lik: -10360.346, Max-Change: 0.12133Iteration: 17, Log-Lik: -10359.333, Max-Change: 0.03727Iteration: 18, Log-Lik: -10359.002, Max-Change: 0.04397Iteration: 19, Log-Lik: -10358.801, Max-Change: 0.04719Iteration: 20, Log-Lik: -10358.591, Max-Change: 0.05133Iteration: 21, Log-Lik: -10358.410, Max-Change: 0.02077Iteration: 22, Log-Lik: -10358.337, Max-Change: 0.04891Iteration: 23, Log-Lik: -10358.184, Max-Change: 0.03701Iteration: 24, Log-Lik: -10358.050, Max-Change: 0.01716Iteration: 25, Log-Lik: -10358.005, Max-Change: 0.03666Iteration: 26, Log-Lik: -10357.894, Max-Change: 0.03487Iteration: 27, Log-Lik: -10357.797, Max-Change: 0.01621Iteration: 28, Log-Lik: -10357.767, Max-Change: 0.01415Iteration: 29, Log-Lik: -10357.694, Max-Change: 0.04601Iteration: 30, Log-Lik: -10357.617, Max-Change: 0.03283Iteration: 31, Log-Lik: -10357.524, Max-Change: 0.01379Iteration: 32, Log-Lik: -10357.474, Max-Change: 0.03921Iteration: 33, Log-Lik: -10357.424, Max-Change: 0.00916Iteration: 34, Log-Lik: -10357.416, Max-Change: 0.01006Iteration: 35, Log-Lik: -10357.380, Max-Change: 0.02822Iteration: 36, Log-Lik: -10357.343, Max-Change: 0.00895Iteration: 37, Log-Lik: -10357.337, Max-Change: 0.00870Iteration: 38, Log-Lik: -10357.310, Max-Change: 0.00980Iteration: 39, Log-Lik: -10357.287, Max-Change: 0.03201Iteration: 40, Log-Lik: -10357.257, Max-Change: 0.00613Iteration: 41, Log-Lik: -10357.241, Max-Change: 0.00621Iteration: 42, Log-Lik: -10357.226, Max-Change: 0.00628Iteration: 43, Log-Lik: -10357.203, Max-Change: 0.00560Iteration: 44, Log-Lik: -10357.190, Max-Change: 0.00178Iteration: 45, Log-Lik: -10357.186, Max-Change: 0.00399Iteration: 46, Log-Lik: -10357.179, Max-Change: 0.00227Iteration: 47, Log-Lik: -10357.176, Max-Change: 0.03798Iteration: 48, Log-Lik: -10357.155, Max-Change: 0.00415Iteration: 49, Log-Lik: -10357.155, Max-Change: 0.00413Iteration: 50, Log-Lik: -10357.148, Max-Change: 0.00366Iteration: 51, Log-Lik: -10357.144, Max-Change: 0.00372Iteration: 52, Log-Lik: -10357.131, Max-Change: 0.04537Iteration: 53, Log-Lik: -10357.110, Max-Change: 0.00132Iteration: 54, Log-Lik: -10357.109, Max-Change: 0.00345Iteration: 55, Log-Lik: -10357.106, Max-Change: 0.00159Iteration: 56, Log-Lik: -10357.105, Max-Change: 0.00040Iteration: 57, Log-Lik: -10357.104, Max-Change: 0.00058Iteration: 58, Log-Lik: -10357.104, Max-Change: 0.00021Iteration: 59, Log-Lik: -10357.104, Max-Change: 0.00047Iteration: 60, Log-Lik: -10357.104, Max-Change: 0.00016Iteration: 61, Log-Lik: -10357.104, Max-Change: 0.00013Iteration: 62, Log-Lik: -10357.104, Max-Change: 0.00025Iteration: 63, Log-Lik: -10357.104, Max-Change: 0.00012Iteration: 64, Log-Lik: -10357.104, Max-Change: 0.00010"},{"path":"/articles/lecture_irt.html","id":"gpcm-output","dir":"Articles","previous_headings":"","what":"GPCM Output","title":"Item Response Theory","text":"Can also get factor loadings , standardized coefficients help us determine relate latent trait","code":"summary(gpcm.model1) ##standardized coefficients  #>          F1    h2 #> Q99_1 0.964 0.929 #> Q99_4 0.984 0.968 #> Q99_5 0.976 0.953 #> Q99_6 0.977 0.955 #> Q99_9 0.720 0.519 #>  #> SS loadings:  4.324  #> Proportion Var:  0.865  #>  #> Factor correlations:  #>  #>    F1 #> F1  1"},{"path":"/articles/lecture_irt.html","id":"gpcm-output-1","dir":"Articles","previous_headings":"","what":"GPCM Output","title":"Item Response Theory","text":"","code":"coef(gpcm.model1, IRTpars = T) ##coefficients #> $Q99_1 #>         a     b1     b2     b3     b4    b5    b6 #> par 1.927 -1.905 -1.344 -1.107 -0.607 0.225 1.236 #>  #> $Q99_4 #>         a     b1    b2     b3     b4    b5    b6 #> par 2.941 -1.952 -1.67 -1.082 -0.592 0.121 0.972 #>  #> $Q99_5 #>         a     b1     b2     b3     b4     b5    b6 #> par 2.395 -2.052 -1.601 -1.255 -0.825 -0.093 0.979 #>  #> $Q99_6 #>         a     b1    b2     b3     b4    b5    b6 #> par 2.448 -2.014 -1.43 -1.168 -0.531 0.118 1.085 #>  #> $Q99_9 #>         a     b1     b2     b3     b4     b5     b6 #> par 0.553 -1.671 -2.488 -1.202 -0.113 -1.115 -0.296 #>  #> $GroupPars #>     MEAN_1 COV_11 #> par      0      1  head(fscores(gpcm.model1)) ##factor scores #>              F1 #> [1,] -0.6805579 #> [2,] -2.7481783 #> [3,] -1.2486173 #> [4,] -1.4226850 #> [5,] -2.7481783 #> [6,] -2.7481783"},{"path":"/articles/lecture_irt.html","id":"gpcm-plots","dir":"Articles","previous_headings":"","what":"GPCM Plots","title":"Item Response Theory","text":"","code":"plot(gpcm.model1, type = \"trace\") ##curves for all items at once itemplot(gpcm.model1, 5, type = \"trace\")"},{"path":"/articles/lecture_irt.html","id":"gpcm-plots-1","dir":"Articles","previous_headings":"","what":"GPCM Plots","title":"Item Response Theory","text":"","code":"itemplot(gpcm.model1, 4, type = \"info\") ##IIC for each item plot(gpcm.model1, type = \"info\") ##test information curve"},{"path":"/articles/lecture_irt.html","id":"gpcm-plots-2","dir":"Articles","previous_headings":"","what":"GPCM Plots","title":"Item Response Theory","text":"","code":"plot(gpcm.model1) ##expected score curve"},{"path":"/articles/lecture_irt.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Item Response Theory","text":"lecture ’ve learned: Item response theory compared classical test theory run dichotomous traditional IRT 2PL 3PL run polytomous IRT using graded partial credit model compare models interpret output","code":""},{"path":"/articles/lecture_lgm.html","id":"latent-growth-modeling","dir":"Articles","previous_headings":"","what":"Latent Growth Modeling","title":"Latent Growth Models","text":"Measuring change repeated time measurements Gives information traditional repeated measures ANOVA regression line . Estimate means covariances separately Estimating observed values unobserved values separately","code":""},{"path":"/articles/lecture_lgm.html","id":"assumptions","dir":"Articles","previous_headings":"","what":"Assumptions","title":"Latent Growth Models","text":"Continuous measurement DVs assumption true many structural models though! Time spacing across people across measurements, people need spaced least three time points per person (otherwise, dependent t-test) Large samples!","code":""},{"path":"/articles/lecture_lgm.html","id":"before-you-start","dir":"Articles","previous_headings":"","what":"Before you start","title":"Latent Growth Models","text":"know expected type change start Generally ’s linear, however, can curvilinear power functions","code":""},{"path":"/articles/lecture_lgm.html","id":"example-model","dir":"Articles","previous_headings":"","what":"Example model","title":"Latent Growth Models","text":"set regression loading values specific numbers able estimate intercept slope.","code":""},{"path":"/articles/lecture_lgm.html","id":"understand-the-idea","dir":"Articles","previous_headings":"","what":"Understand the idea","title":"Latent Growth Models","text":"set values 1 indicating want estimate Basically, gives starting value first time point, average point people start (y-intercept)","code":""},{"path":"/articles/lecture_lgm.html","id":"understand-the-idea-1","dir":"Articles","previous_headings":"","what":"Understand the idea","title":"Latent Growth Models","text":"can set values anything want Usually first time indicated 0 ’s slope time 1, just intercept paths set based time differences ","code":""},{"path":"/articles/lecture_lgm.html","id":"understand-the-idea-2","dir":"Articles","previous_headings":"","what":"Understand the idea","title":"Latent Growth Models","text":"Helps identification theoretical match concept slope intercept estimation Allows set variances, can look ","code":""},{"path":"/articles/lecture_lgm.html","id":"questions","dir":"Articles","previous_headings":"","what":"Questions","title":"Latent Growth Models","text":"average change person? start? change time? form change?","code":""},{"path":"/articles/lecture_lgm.html","id":"questions-1","dir":"Articles","previous_headings":"","what":"Questions","title":"Latent Growth Models","text":"average slope intercept good fit participants? include variance term account differences people? Models called random effects add random variances (similar multilevel models) random effects model – another variable explains random effects?","code":""},{"path":"/articles/lecture_lgm.html","id":"understanding-the-output","dir":"Articles","previous_headings":"","what":"Understanding the Output","title":"Latent Growth Models","text":"Intercept mean: average starting point time 1 Slope mean: average increment across time points","code":""},{"path":"/articles/lecture_lgm.html","id":"understanding-the-output-1","dir":"Articles","previous_headings":"","what":"Understanding the Output","title":"Latent Growth Models","text":"Large scores indicate lot spread – meaning people start lot different places Small scores indicate small spread – everyone starts place","code":""},{"path":"/articles/lecture_lgm.html","id":"understanding-the-output-2","dir":"Articles","previous_headings":"","what":"Understanding the Output","title":"Latent Growth Models","text":"Small variances mean everyone going /amount Large variances mean people scores going /differently (almost like interaction)","code":""},{"path":"/articles/lecture_lgm.html","id":"understanding-the-output-3","dir":"Articles","previous_headings":"","what":"Understanding the Output","title":"Latent Growth Models","text":"Factor covariance – examines relationship intercept slope People start higher intercepts higher positive slopes People start higher go faster (larger negative slopes) People start higher intercepts go slower (smaller slopes) People start higher intercepts go slower","code":""},{"path":"/articles/lecture_lgm.html","id":"example-crime-data-across-time","dir":"Articles","previous_headings":"","what":"Example: Crime Data across time","title":"Latent Growth Models","text":"","code":"##load the data crime.cov <- lav_matrix_lower2full(c(.63,                                      .50, .60,                                      .48, .48, .58,                                      .47, .48, .51, .67))  crime.mean <- c(5.17, 5.32, 5.40, 5.52)  names(crime.mean) <-    rownames(crime.cov) <-    colnames(crime.cov) <- c(\"Time1\", \"Time2\", \"Time3\", \"Time4\")"},{"path":"/articles/lecture_lgm.html","id":"new-functions","dir":"Articles","previous_headings":"","what":"New Functions","title":"Latent Growth Models","text":"growth(): function helps set LCM, fixes parameters correctly . use data covariances means, can import data just like cfa()","code":""},{"path":"/articles/lecture_lgm.html","id":"how-to-test","dir":"Articles","previous_headings":"","what":"How to Test","title":"Latent Growth Models","text":"procedure similar multigroup model testing, reverse. going start constrained model slowly let parameters free see best. slope intercept useful variances low indicating everyone pattern slope intercept","code":""},{"path":"/articles/lecture_lgm.html","id":"intercept-only-model","dir":"Articles","previous_headings":"","what":"Intercept Only Model","title":"Latent Growth Models","text":"Use intercept (mean data) estimate want model bad, otherwise saying one average score best time points Intercept variance constrained 0, get mean. Residuals forced time point, variance across time points","code":"crime.model1 <- ' # intercept i =~ 1*Time1 + 1*Time2 + 1*Time3 + 1*Time4 i~~0*i # residual variances Time1~~r*Time1 Time2~~r*Time2 Time3~~r*Time3 Time4~~r*Time4 '"},{"path":"/articles/lecture_lgm.html","id":"intercept-only-model-1","dir":"Articles","previous_headings":"","what":"Intercept Only Model","title":"Latent Growth Models","text":"","code":"crime.fit1 <- growth(crime.model1,                     sample.cov=crime.cov,                      sample.mean=crime.mean,                      sample.nobs=952) summary(crime.fit1,         standardized = TRUE,         fit.measures = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 19 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         5 #>   Number of equality constraints                     3 #>  #>   Number of observations                           952 #>  #> Model Test User Model: #>                                                        #>   Test statistic                              3461.981 #>   Degrees of freedom                                12 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              3358.206 #>   Degrees of freedom                                 6 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.000 #>   Tucker-Lewis Index (TLI)                       0.485 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -4540.205 #>   Loglikelihood unrestricted model (H1)      -2809.215 #>                                                        #>   Akaike (AIC)                                9084.410 #>   Bayesian (BIC)                              9094.127 #>   Sample-size adjusted Bayesian (SABIC)       9087.775 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.550 #>   90 Percent confidence interval - lower         0.534 #>   90 Percent confidence interval - upper         0.565 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.523 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   i =~                                                                   #>     Time1             1.000                               0.000    0.000 #>     Time2             1.000                               0.000    0.000 #>     Time3             1.000                               0.000    0.000 #>     Time4             1.000                               0.000    0.000 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>     i                 5.352    0.013  414.325    0.000      Inf      Inf #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>     i                 0.000                                 NaN      NaN #>    .Time1      (r)    0.636    0.015   43.635    0.000    0.636    1.000 #>    .Time2      (r)    0.636    0.015   43.635    0.000    0.636    1.000 #>    .Time3      (r)    0.636    0.015   43.635    0.000    0.636    1.000 #>    .Time4      (r)    0.636    0.015   43.635    0.000    0.636    1.000 #>  #> R-Square: #>                    Estimate #>     Time1             0.000 #>     Time2             0.000 #>     Time3             0.000 #>     Time4             0.000"},{"path":"/articles/lecture_lgm.html","id":"intercept-only-model-2","dir":"Articles","previous_headings":"","what":"Intercept Only Model","title":"Latent Growth Models","text":"","code":"semPaths(crime.fit1,          whatLabels = \"par\",          edge.label.cex = 1,          layout = \"tree\")"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-fit-indices","dir":"Articles","previous_headings":"","what":"Comparison Table Fit Indices","title":"Latent Growth Models","text":"","code":"library(knitr) fit.table <- matrix(NA, nrow = 5, ncol = 6) colnames(fit.table) <- c(\"Model\", \"X2\", \"df\", \"RMSEA\", \"SRMR\", \"CFI\") fit.table[1, ] <- c(\"Intercept Only\", round(fitmeasures(crime.fit1, c(\"chisq\", \"df\", \"rmsea\", \"srmr\", \"cfi\")),3)) kable(fit.table)"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-parameters","dir":"Articles","previous_headings":"","what":"Comparison Table Parameters","title":"Latent Growth Models","text":"","code":"#save the parameter estimates crime.fit1.par <- parameterestimates(crime.fit1)  #make table par.table <- matrix(NA, nrow = 5, ncol = 7) colnames(par.table) <- c(\"Model\", \"Intercept Mean\", \"Intercept Variance\", \"Residual Variance\", \"Slope Mean\", \"Slope Variance\", \"Covariance\")  #put data in table par.table[1, ] <- c(\"Intercept Only\",                      round(crime.fit1.par$est[crime.fit1.par$lhs == \"i\" & crime.fit1.par$op == \"~1\"], 3),                     \"X\",                      round(crime.fit1.par$est[crime.fit1.par$lhs == \"Time1\" & crime.fit1.par$op == \"~~\"], 3),                     \"X\",                      \"X\",                      \"X\") kable(par.table)"},{"path":"/articles/lecture_lgm.html","id":"random-intercept-only-model","dir":"Articles","previous_headings":"","what":"Random Intercept Only Model","title":"Latent Growth Models","text":"allow intercept variance > 0 People can start different places Residual variances still Still slope","code":"crime.model2 <- ' # intercept i =~ 1*Time1 + 1*Time2 + 1*Time3 + 1*Time4 # residual variances Time1~~r*Time1 Time2~~r*Time2 Time3~~r*Time3 Time4~~r*Time4 '"},{"path":"/articles/lecture_lgm.html","id":"random-intercept-only-model-1","dir":"Articles","previous_headings":"","what":"Random Intercept Only Model","title":"Latent Growth Models","text":"","code":"crime.fit2 <- growth(crime.model2,                     sample.cov=crime.cov,                      sample.mean=crime.mean,                      sample.nobs=952) summary(crime.fit2,         standardized = TRUE,         fit.measures = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 19 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         6 #>   Number of equality constraints                     3 #>  #>   Number of observations                           952 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               555.313 #>   Degrees of freedom                                11 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              3358.206 #>   Degrees of freedom                                 6 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.838 #>   Tucker-Lewis Index (TLI)                       0.911 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -3086.871 #>   Loglikelihood unrestricted model (H1)      -2809.215 #>                                                        #>   Akaike (AIC)                                6179.742 #>   Bayesian (BIC)                              6194.318 #>   Sample-size adjusted Bayesian (SABIC)       6184.790 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.228 #>   90 Percent confidence interval - lower         0.212 #>   90 Percent confidence interval - upper         0.244 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.092 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   i =~                                                                   #>     Time1             1.000                               0.693    0.870 #>     Time2             1.000                               0.693    0.870 #>     Time3             1.000                               0.693    0.870 #>     Time4             1.000                               0.693    0.870 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>     i                 5.352    0.023  229.140    0.000    7.720    7.720 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Time1      (r)    0.155    0.004   37.789    0.000    0.155    0.244 #>    .Time2      (r)    0.155    0.004   37.789    0.000    0.155    0.244 #>    .Time3      (r)    0.155    0.004   37.789    0.000    0.155    0.244 #>    .Time4      (r)    0.155    0.004   37.789    0.000    0.155    0.244 #>     i                 0.481    0.024   20.174    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     Time1             0.756 #>     Time2             0.756 #>     Time3             0.756 #>     Time4             0.756"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-fit-indices-1","dir":"Articles","previous_headings":"","what":"Comparison Table Fit Indices","title":"Latent Growth Models","text":"model much better model 1 Implies people start different places","code":"fit.table[2, ] <- c(\"Random Intercept\", round(fitmeasures(crime.fit2, c(\"chisq\", \"df\", \"rmsea\", \"srmr\", \"cfi\")),3)) kable(fit.table)"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-parameters-1","dir":"Articles","previous_headings":"","what":"Comparison Table Parameters","title":"Latent Growth Models","text":"","code":"#save the parameter estimates crime.fit2.par <- parameterestimates(crime.fit2)  #put data in table par.table[2, ] <- c(\"Random Intercept\",                      round(crime.fit2.par$est[crime.fit2.par$lhs == \"i\" & crime.fit2.par$op == \"~1\"], 3),                     round(crime.fit2.par$est[crime.fit2.par$lhs == \"i\" & crime.fit2.par$op == \"~~\"], 3),                      round(crime.fit2.par$est[crime.fit2.par$lhs == \"Time1\" & crime.fit2.par$op == \"~~\"], 3),                     \"X\",                      \"X\",                      \"X\") kable(par.table)"},{"path":"/articles/lecture_lgm.html","id":"random-slope-intercepts","dir":"Articles","previous_headings":"","what":"Random Slope + Intercepts","title":"Latent Growth Models","text":"s~0*1 (makes average slope 0) s~~0*(uncorrelated slope/intercept) Leave intercept variance model Keep residuals constrained ","code":"crime.model3 <- ' # intercept i =~ 1*Time1 + 1*Time2 + 1*Time3 + 1*Time4 # slope s =~ 0*Time1 + 1*Time2 + 2*Time3 + 3*Time4 s~0*1 s~~0*i # residual variances Time1~~r*Time1 Time2~~r*Time2 Time3~~r*Time3 Time4~~r*Time4 '"},{"path":"/articles/lecture_lgm.html","id":"random-slope-intercepts-1","dir":"Articles","previous_headings":"","what":"Random Slope + Intercepts","title":"Latent Growth Models","text":"","code":"crime.fit3 <- growth(crime.model3,                     sample.cov=crime.cov,                      sample.mean=crime.mean,                      sample.nobs=952) summary(crime.fit3,         standardized = TRUE,         fit.measures = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 38 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         7 #>   Number of equality constraints                     3 #>  #>   Number of observations                           952 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               339.586 #>   Degrees of freedom                                10 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              3358.206 #>   Degrees of freedom                                 6 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.902 #>   Tucker-Lewis Index (TLI)                       0.941 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -2979.008 #>   Loglikelihood unrestricted model (H1)      -2809.215 #>                                                        #>   Akaike (AIC)                                5966.015 #>   Bayesian (BIC)                              5985.450 #>   Sample-size adjusted Bayesian (SABIC)       5972.746 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.186 #>   90 Percent confidence interval - lower         0.169 #>   90 Percent confidence interval - upper         0.203 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.149 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   i =~                                                                   #>     Time1             1.000                               0.696    0.903 #>     Time2             1.000                               0.696    0.884 #>     Time3             1.000                               0.696    0.835 #>     Time4             1.000                               0.696    0.768 #>   s =~                                                                   #>     Time1             0.000                               0.000    0.000 #>     Time2             1.000                               0.158    0.201 #>     Time3             2.000                               0.317    0.380 #>     Time4             3.000                               0.475    0.525 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   i ~~                                                                   #>     s                 0.000                               0.000    0.000 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>     s                 0.000                               0.000    0.000 #>     i                 5.262    0.024  221.401    0.000    7.565    7.565 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Time1      (r)    0.110    0.003   31.461    0.000    0.110    0.185 #>    .Time2      (r)    0.110    0.003   31.461    0.000    0.110    0.178 #>    .Time3      (r)    0.110    0.003   31.461    0.000    0.110    0.159 #>    .Time4      (r)    0.110    0.003   31.461    0.000    0.110    0.134 #>     i                 0.484    0.025   19.597    0.000    1.000    1.000 #>     s                 0.025    0.002   11.651    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     Time1             0.815 #>     Time2             0.822 #>     Time3             0.841 #>     Time4             0.866"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-fit-indices-2","dir":"Articles","previous_headings":"","what":"Comparison Table Fit Indices","title":"Latent Growth Models","text":"model much better model 2 Implies least random slope better slope","code":"fit.table[3, ] <- c(\"Random Slope\", round(fitmeasures(crime.fit3, c(\"chisq\", \"df\", \"rmsea\", \"srmr\", \"cfi\")),3)) kable(fit.table)"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-parameters-2","dir":"Articles","previous_headings":"","what":"Comparison Table Parameters","title":"Latent Growth Models","text":"","code":"#save the parameter estimates crime.fit3.par <- parameterestimates(crime.fit3)  #put data in table par.table[3, ] <- c(\"Random Slope\",                      round(crime.fit3.par$est[crime.fit3.par$lhs == \"i\" & crime.fit3.par$op == \"~1\"], 3),                     round(crime.fit3.par$est[crime.fit3.par$lhs == \"i\" & crime.fit3.par$op == \"~~\" & crime.fit3.par$rhs == \"i\"], 3),                      round(crime.fit3.par$est[crime.fit3.par$lhs == \"Time1\" & crime.fit3.par$op == \"~~\"], 3),                     \"X\",                      round(crime.fit3.par$est[crime.fit3.par$lhs == \"s\" & crime.fit3.par$op == \"~~\"], 3),                      \"X\") kable(par.table)"},{"path":"/articles/lecture_lgm.html","id":"full-slopes-intercept","dir":"Articles","previous_headings":"","what":"Full Slopes + Intercept","title":"Latent Growth Models","text":"Allow slope intercept vary Constrain residual variances","code":"crime.model4 <- ' # intercept i =~ 1*Time1 + 1*Time2 + 1*Time3 + 1*Time4 # slope s =~ 0*Time1 + 1*Time2 + 2*Time3 + 3*Time4 # residual variances Time1~~r*Time1 Time2~~r*Time2 Time3~~r*Time3 Time4~~r*Time4 '"},{"path":"/articles/lecture_lgm.html","id":"full-slopes-intercept-1","dir":"Articles","previous_headings":"","what":"Full Slopes + Intercept","title":"Latent Growth Models","text":"People start high go slower people start lower.","code":"crime.fit4 <- growth(crime.model4,                     sample.cov=crime.cov,                      sample.mean=crime.mean,                      sample.nobs=952) summary(crime.fit4,         standardized = TRUE,         fit.measures = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 33 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         9 #>   Number of equality constraints                     3 #>  #>   Number of observations                           952 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                24.001 #>   Degrees of freedom                                 8 #>   P-value (Chi-square)                           0.002 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              3358.206 #>   Degrees of freedom                                 6 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.995 #>   Tucker-Lewis Index (TLI)                       0.996 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -2821.215 #>   Loglikelihood unrestricted model (H1)      -2809.215 #>                                                        #>   Akaike (AIC)                                5654.431 #>   Bayesian (BIC)                              5683.582 #>   Sample-size adjusted Bayesian (SABIC)       5664.526 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.046 #>   90 Percent confidence interval - lower         0.025 #>   90 Percent confidence interval - upper         0.067 #>   P-value H_0: RMSEA <= 0.050                    0.589 #>   P-value H_0: RMSEA >= 0.080                    0.004 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.020 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   i =~                                                                   #>     Time1             1.000                               0.720    0.911 #>     Time2             1.000                               0.720    0.930 #>     Time3             1.000                               0.720    0.925 #>     Time4             1.000                               0.720    0.896 #>   s =~                                                                   #>     Time1             0.000                               0.000    0.000 #>     Time2             1.000                               0.128    0.165 #>     Time3             2.000                               0.255    0.328 #>     Time4             3.000                               0.383    0.476 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   i ~~                                                                   #>     s                -0.021    0.005   -4.002    0.000   -0.228   -0.228 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>     i                 5.183    0.025  207.586    0.000    7.194    7.194 #>     s                 0.113    0.006   17.990    0.000    0.885    0.885 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Time1      (r)    0.106    0.003   30.854    0.000    0.106    0.170 #>    .Time2      (r)    0.106    0.003   30.854    0.000    0.106    0.177 #>    .Time3      (r)    0.106    0.003   30.854    0.000    0.106    0.175 #>    .Time4      (r)    0.106    0.003   30.854    0.000    0.106    0.164 #>     i                 0.519    0.027   19.007    0.000    1.000    1.000 #>     s                 0.016    0.002    8.790    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     Time1             0.830 #>     Time2             0.823 #>     Time3             0.825 #>     Time4             0.836"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-fit-indices-3","dir":"Articles","previous_headings":"","what":"Comparison Table Fit Indices","title":"Latent Growth Models","text":"model much better model 3 Implies single slope better just random slopes","code":"fit.table[4, ] <- c(\"Full Slope\", round(fitmeasures(crime.fit4, c(\"chisq\", \"df\", \"rmsea\", \"srmr\", \"cfi\")),3)) kable(fit.table)"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-parameters-3","dir":"Articles","previous_headings":"","what":"Comparison Table Parameters","title":"Latent Growth Models","text":"","code":"#save the parameter estimates crime.fit4.par <- parameterestimates(crime.fit4)  #put data in table par.table[4, ] <- c(\"Full Slope\",                      round(crime.fit4.par$est[crime.fit4.par$lhs == \"i\" & crime.fit4.par$op == \"~1\"], 3),                     round(crime.fit4.par$est[crime.fit4.par$lhs == \"i\" & crime.fit4.par$op == \"~~\" & crime.fit4.par$rhs == \"i\"], 3),                      round(crime.fit4.par$est[crime.fit4.par$lhs == \"Time1\" & crime.fit4.par$op == \"~~\"], 3),                     round(crime.fit4.par$est[crime.fit4.par$lhs == \"s\" & crime.fit4.par$op == \"~1\"], 3),                      round(crime.fit4.par$est[crime.fit4.par$lhs == \"s\" & crime.fit4.par$op == \"~~\"], 3),                      round(crime.fit4.par$est[crime.fit4.par$lhs == \"i\" & crime.fit4.par$op == \"~~\" & crime.fit4.par$rhs == \"s\"], 3)) kable(par.table)"},{"path":"/articles/lecture_lgm.html","id":"totally-unconstrained-model","dir":"Articles","previous_headings":"","what":"Totally Unconstrained Model","title":"Latent Growth Models","text":"Now residuals free vary want residuals small roughly equal, model shouldn’t different previous model","code":"crime.model5 <- ' # intercept i =~ 1*Time1 + 1*Time2 + 1*Time3 + 1*Time4 # slope s =~ 0*Time1 + 1*Time2 + 2*Time3 + 3*Time4 '"},{"path":"/articles/lecture_lgm.html","id":"totally-unconstrained-model-1","dir":"Articles","previous_headings":"","what":"Totally Unconstrained Model","title":"Latent Growth Models","text":"","code":"crime.fit5 <- growth(crime.model5,                     sample.cov=crime.cov,                      sample.mean=crime.mean,                      sample.nobs=952) summary(crime.fit5,         standardized = TRUE,         fit.measures = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 48 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         9 #>  #>   Number of observations                           952 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 9.137 #>   Degrees of freedom                                 5 #>   P-value (Chi-square)                           0.104 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              3358.206 #>   Degrees of freedom                                 6 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.999 #>   Tucker-Lewis Index (TLI)                       0.999 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -2813.783 #>   Loglikelihood unrestricted model (H1)      -2809.215 #>                                                        #>   Akaike (AIC)                                5645.567 #>   Bayesian (BIC)                              5689.294 #>   Sample-size adjusted Bayesian (SABIC)       5660.710 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.029 #>   90 Percent confidence interval - lower         0.000 #>   90 Percent confidence interval - upper         0.059 #>   P-value H_0: RMSEA <= 0.050                    0.854 #>   P-value H_0: RMSEA >= 0.080                    0.001 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.014 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   i =~                                                                   #>     Time1             1.000                               0.721    0.914 #>     Time2             1.000                               0.721    0.922 #>     Time3             1.000                               0.721    0.943 #>     Time4             1.000                               0.721    0.887 #>   s =~                                                                   #>     Time1             0.000                               0.000    0.000 #>     Time2             1.000                               0.121    0.155 #>     Time3             2.000                               0.242    0.316 #>     Time4             3.000                               0.363    0.446 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   i ~~                                                                   #>     s                -0.020    0.006   -3.517    0.000   -0.231   -0.231 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>     i                 5.182    0.025  207.459    0.000    7.190    7.190 #>     s                 0.113    0.006   18.106    0.000    0.937    0.937 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Time1             0.102    0.010   10.029    0.000    0.102    0.165 #>    .Time2             0.117    0.007   16.319    0.000    0.117    0.191 #>    .Time3             0.087    0.006   14.268    0.000    0.087    0.149 #>    .Time4             0.131    0.011   12.138    0.000    0.131    0.198 #>     i                 0.519    0.028   18.712    0.000    1.000    1.000 #>     s                 0.015    0.002    6.558    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     Time1             0.835 #>     Time2             0.809 #>     Time3             0.851 #>     Time4             0.802"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-fit-indices-4","dir":"Articles","previous_headings":"","what":"Comparison Table Fit Indices","title":"Latent Growth Models","text":"model equal model 4 result implies residual variances approximately equal","code":"fit.table[5, ] <- c(\"Unconstrained\", round(fitmeasures(crime.fit5, c(\"chisq\", \"df\", \"rmsea\", \"srmr\", \"cfi\")),3)) kable(fit.table)"},{"path":"/articles/lecture_lgm.html","id":"comparison-table-parameters-4","dir":"Articles","previous_headings":"","what":"Comparison Table Parameters","title":"Latent Growth Models","text":"","code":"#save the parameter estimates crime.fit5.par <- parameterestimates(crime.fit5)  residual_numbers <- paste(round(crime.fit5.par$est[crime.fit5.par$lhs == \"Time1\" & crime.fit5.par$op == \"~~\"], 3),                            round(crime.fit5.par$est[crime.fit5.par$lhs == \"Time2\" & crime.fit5.par$op == \"~~\"], 3),                           round(crime.fit5.par$est[crime.fit5.par$lhs == \"Time3\" & crime.fit5.par$op == \"~~\"], 3),                           round(crime.fit5.par$est[crime.fit5.par$lhs == \"Time4\" & crime.fit5.par$op == \"~~\"], 3))  #put data in table par.table[5, ] <- c(\"Unconstrained\",                      round(crime.fit5.par$est[crime.fit5.par$lhs == \"i\" & crime.fit5.par$op == \"~1\"], 3),                     round(crime.fit5.par$est[crime.fit5.par$lhs == \"i\" & crime.fit5.par$op == \"~~\" & crime.fit5.par$rhs == \"i\"], 3),                      residual_numbers,                     round(crime.fit5.par$est[crime.fit5.par$lhs == \"s\" & crime.fit5.par$op == \"~1\"], 3),                      round(crime.fit5.par$est[crime.fit5.par$lhs == \"s\" & crime.fit5.par$op == \"~~\"], 3),                      round(crime.fit5.par$est[crime.fit5.par$lhs == \"i\" & crime.fit5.par$op == \"~~\" & crime.fit5.par$rhs == \"s\"], 3)) kable(par.table)"},{"path":"/articles/lecture_lgm.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Latent Growth Models","text":"lecture ’ve learned: Latent growth modeling idea random intercepts slopes account participant variability test least restrictive models compare parameters models","code":""},{"path":"/articles/lecture_mgcfa.html","id":"multigroup-models","dir":"Articles","previous_headings":"","what":"Multigroup Models","title":"Multi-Group Confirmatory Factor Analysis","text":"Let’s start terminology specific multigroup models: invariance = equivalence different conditions, measurements yield measurement attributes? Generally, ’ve seen latent variable models estimating data based covariances. models, also add mean structure. can test many groups models, want generally compare two time.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"mean-structure","dir":"Articles","previous_headings":"","what":"Mean Structure","title":"Multi-Group Confirmatory Factor Analysis","text":"Mean structure represents intercept manifest variables ’s best guess someone going score item without X information idea latent variable model - ’s starting value manifest variable.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"latent-means","dir":"Articles","previous_headings":"","what":"Latent Means","title":"Multi-Group Confirmatory Factor Analysis","text":"Latent means estimated score latent variable participants. ’s weighted score item’s original value path coefficient, items averaged together. can use lavPredict() calculate us.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"multigroup-questions","dir":"Articles","previous_headings":"","what":"Multigroup Questions","title":"Multi-Group Confirmatory Factor Analysis","text":"items act across groups? factor structure across groups? paths equal across groups? latent means equal across groups?","code":""},{"path":"/articles/lecture_mgcfa.html","id":"how-to-mg","dir":"Articles","previous_headings":"","what":"How to MG","title":"Multi-Group Confirmatory Factor Analysis","text":"model tell problems CFA model least restrictive use grouping variables","code":""},{"path":"/articles/lecture_mgcfa.html","id":"how-to-mg-1","dir":"Articles","previous_headings":"","what":"How to MG","title":"Multi-Group Confirmatory Factor Analysis","text":"Second, test group separately CFA structure determine individual group fits ok. Fit indices often decrease slightly , sample size smaller (usually bit error variance). poor fitting CFA overall use step see ’s one group just fit.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"how-to-mg-2","dir":"Articles","previous_headings":"","what":"How to MG","title":"Multi-Group Confirmatory Factor Analysis","text":"next steps nest two models together. Nesting like stacking models together (like pancakes). nested model steps, people use Brown’s terminology procedure.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"how-to-mg-3","dir":"Articles","previous_headings":"","what":"How to MG","title":"Multi-Group Confirmatory Factor Analysis","text":"whole model (picture) Loadings (regression weights) Intercepts (y-intercept item) Error variances (variance) Factor variances (variances latents) Factor covariances (correlation) Factor means (latent means)","code":""},{"path":"/articles/lecture_mgcfa.html","id":"equal-form-configural-invariance","dir":"Articles","previous_headings":"","what":"Equal Form / Configural Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"model, put two groups together model. force paths , forcing model picture . testing groups show factor structure (configuration).","code":""},{"path":"/articles/lecture_mgcfa.html","id":"metric-invariance","dir":"Articles","previous_headings":"","what":"Metric Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"model, forcing factor loadings (regression weights) exactly . step tell groups weights question – questions different signs strengths.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"scalar-invariance","dir":"Articles","previous_headings":"","what":"Scalar Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"model, forcing intercepts items . step tell items starting point – remember y-intercept mean item. MG model going indicate non-invariance – step often one breaks.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"strict-factorial-invariance","dir":"Articles","previous_headings":"","what":"Strict Factorial Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"model, forcing error variances item . step tell variance (spread) item group. get differences, indicates one group larger range answers another, indicates heterogeneous.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"other-steps","dir":"Articles","previous_headings":"","what":"Other Steps","title":"Multi-Group Confirmatory Factor Analysis","text":"previous steps manifest/measured variables . Generally, focus indicate performance scale. Equal factor variances: Testing latents set variance – means overall score spread Equal factor covariances: Testing correlations factors group Equal latent means: Testing overall latent means equal group","code":""},{"path":"/articles/lecture_mgcfa.html","id":"how-to-judge","dir":"Articles","previous_headings":"","what":"How to Judge","title":"Multi-Group Confirmatory Factor Analysis","text":"can tell steps invariant? expect fit get worse go restrictive. can use change χ2\\chi^2, suggest change CFI > .01 difference fit.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"model-breakdown","dir":"Articles","previous_headings":"","what":"Model Breakdown","title":"Multi-Group Confirmatory Factor Analysis","text":"steps invariant? Partial invariance – strict invariance met, can test partial invariance Partial invariance occurs items invariant couple. meet invariance criteria, trying bring bad model “” invariant level want items possible","code":""},{"path":"/articles/lecture_mgcfa.html","id":"partial-invariance","dir":"Articles","previous_headings":"","what":"Partial Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":", test partial invariance? can use modification indices see paths problematic. change ONE item time. Slowly add items bring CFI acceptable change level (.e. .01 less change previous model).","code":""},{"path":"/articles/lecture_mgcfa.html","id":"test-latent-means","dir":"Articles","previous_headings":"","what":"Test Latent Means","title":"Multi-Group Confirmatory Factor Analysis","text":"Instead population heterogeneity steps. minimum, useful know calculate weighted score participant. normally use EFA/CFA show question nice loading questions load together procedure totally ignores fact loadings often different. lose information?","code":""},{"path":"/articles/lecture_mgcfa.html","id":"lets-get-started","dir":"Articles","previous_headings":"","what":"Let’s get started!","title":"Multi-Group Confirmatory Factor Analysis","text":"Equality constraints cheese =~ \\*feta + \\*swiss feta swiss estimated value. entire purpose multigroup model build equality constraints packages used … become trouble worth, let’s look steps just lavaan.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"an-example","dir":"Articles","previous_headings":"","what":"An Example","title":"Multi-Group Confirmatory Factor Analysis","text":"Using RS-14 scale: one-factor measure resiliency","code":"library(lavaan) library(rio) res.data <- import(\"data/assignment_mgcfa.csv\")  head(res.data) #>   Sex Ethnicity RS1 RS2 RS3 RS4 RS5 RS6 RS7 RS8 RS9 RS10 RS11 RS12 RS13 RS14 #> 1   2         0   4   4   4   4   4   4   4   4   4    4    4    4    4    4 #> 2   1         0   5   2   6   4   4   5   4   5   5    4    4    6    4    4 #> 3   2         0   7   7   7   7   2   7   7   4   3    7    7    5    7    5 #> 4   2         0   7   6   6   7   7   7   7   7   7    7    7    6    7    7 #> 5   1         0   2   2   2   2   2   2   2   2   2    2    2    2    2    2 #> 6   2         0   4   5   5   7   5   7   5   7   5    4    5    6    6    5"},{"path":"/articles/lecture_mgcfa.html","id":"clean-up-the-data","dir":"Articles","previous_headings":"","what":"Clean up the data","title":"Multi-Group Confirmatory Factor Analysis","text":"including labels, dropping categories","code":"table(res.data$Sex) #>  #>   1   2   3  #> 244 266   1 res.data$Sex <- factor(res.data$Sex,                         levels = c(1,2),                        labels = c(\"Men\", \"Women\")) res.data <- subset(res.data, !is.na(Sex)) nrow(res.data) #> [1] 510"},{"path":"/articles/lecture_mgcfa.html","id":"start-with-overall-cfa","dir":"Articles","previous_headings":"","what":"Start with Overall CFA","title":"Multi-Group Confirmatory Factor Analysis","text":"NEW: meanstructure = TRUE Gives intercepts means model. want turn one beginning, first examining models means, can tell bad parameters start.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"start-with-overall-cfa-1","dir":"Articles","previous_headings":"","what":"Start with Overall CFA","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"overall.model <- ' RS =~ RS1 + RS2 + RS3 + RS4 + RS5 + RS6 + RS7 + RS8 + RS9 + RS10 + RS11 + RS12 + RS13 + RS14 '  overall.fit <- cfa(model = overall.model,                    data = res.data,                     meanstructure = TRUE) ##this is important"},{"path":"/articles/lecture_mgcfa.html","id":"examine-overall-fit","dir":"Articles","previous_headings":"","what":"Examine Overall Fit","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"summary(overall.fit,          standardized = TRUE,          rsquare = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 26 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        42 #>  #>   Number of observations                           510 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               368.984 #>   Degrees of freedom                                77 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4064.761 #>   Degrees of freedom                                91 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.927 #>   Tucker-Lewis Index (TLI)                       0.913 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -13050.226 #>   Loglikelihood unrestricted model (H1)     -12865.734 #>                                                        #>   Akaike (AIC)                               26184.452 #>   Bayesian (BIC)                             26362.297 #>   Sample-size adjusted Bayesian (SABIC)      26228.983 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.086 #>   90 Percent confidence interval - lower         0.078 #>   90 Percent confidence interval - upper         0.095 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    0.882 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.041 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.135    0.637 #>     RS2               1.230    0.091   13.529    0.000    1.397    0.685 #>     RS3               1.065    0.083   12.806    0.000    1.209    0.641 #>     RS4               1.231    0.094   13.150    0.000    1.398    0.662 #>     RS5               1.203    0.094   12.846    0.000    1.366    0.644 #>     RS6               1.218    0.085   14.258    0.000    1.383    0.732 #>     RS7               1.264    0.087   14.554    0.000    1.435    0.751 #>     RS8               1.207    0.087   13.836    0.000    1.371    0.705 #>     RS9               1.271    0.088   14.461    0.000    1.444    0.745 #>     RS10              1.193    0.086   13.946    0.000    1.355    0.712 #>     RS11              1.318    0.091   14.526    0.000    1.496    0.749 #>     RS12              1.156    0.086   13.494    0.000    1.312    0.683 #>     RS13              1.357    0.093   14.634    0.000    1.541    0.756 #>     RS14              1.335    0.086   15.587    0.000    1.515    0.821 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               4.849    0.079   61.416    0.000    4.849    2.720 #>    .RS2               4.218    0.090   46.752    0.000    4.218    2.070 #>    .RS3               4.163    0.084   49.853    0.000    4.163    2.208 #>    .RS4               4.451    0.094   47.600    0.000    4.451    2.108 #>    .RS5               3.808    0.094   40.536    0.000    3.808    1.795 #>    .RS6               5.037    0.084   60.180    0.000    5.037    2.665 #>    .RS7               4.937    0.085   58.353    0.000    4.937    2.584 #>    .RS8               4.351    0.086   50.508    0.000    4.351    2.237 #>    .RS9               4.233    0.086   49.333    0.000    4.233    2.185 #>    .RS10              4.622    0.084   54.829    0.000    4.622    2.428 #>    .RS11              4.486    0.088   50.739    0.000    4.486    2.247 #>    .RS12              5.006    0.085   58.865    0.000    5.006    2.607 #>    .RS13              4.894    0.090   54.257    0.000    4.894    2.403 #>    .RS14              4.749    0.082   58.132    0.000    4.749    2.574 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               1.890    0.124   15.242    0.000    1.890    0.594 #>    .RS2               2.200    0.146   15.025    0.000    2.200    0.530 #>    .RS3               2.094    0.138   15.225    0.000    2.094    0.589 #>    .RS4               2.504    0.165   15.137    0.000    2.504    0.562 #>    .RS5               2.636    0.173   15.215    0.000    2.636    0.586 #>    .RS6               1.660    0.113   14.740    0.000    1.660    0.465 #>    .RS7               1.591    0.109   14.588    0.000    1.591    0.436 #>    .RS8               1.905    0.128   14.917    0.000    1.905    0.503 #>    .RS9               1.671    0.114   14.639    0.000    1.671    0.445 #>    .RS10              1.788    0.120   14.875    0.000    1.788    0.493 #>    .RS11              1.749    0.120   14.604    0.000    1.749    0.439 #>    .RS12              1.966    0.131   15.036    0.000    1.966    0.533 #>    .RS13              1.776    0.122   14.543    0.000    1.776    0.428 #>    .RS14              1.107    0.081   13.753    0.000    1.107    0.325 #>     RS                1.289    0.164    7.847    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.406 #>     RS2               0.470 #>     RS3               0.411 #>     RS4               0.438 #>     RS5               0.414 #>     RS6               0.535 #>     RS7               0.564 #>     RS8               0.497 #>     RS9               0.555 #>     RS10              0.507 #>     RS11              0.561 #>     RS12              0.467 #>     RS13              0.572 #>     RS14              0.675"},{"path":"/articles/lecture_mgcfa.html","id":"make-a-table-of-fit-indices","dir":"Articles","previous_headings":"","what":"Make a table of fit indices","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"library(knitr) table_fit <- matrix(NA, nrow = 9, ncol = 6) colnames(table_fit) = c(\"Model\", \"X2\", \"df\", \"CFI\", \"RMSEA\", \"SRMR\") table_fit[1, ] <- c(\"Overall Model\", round(fitmeasures(overall.fit,                                                   c(\"chisq\", \"df\", \"cfi\",                                                    \"rmsea\", \"srmr\")),3)) kable(table_fit)"},{"path":"/articles/lecture_mgcfa.html","id":"create-a-picture","dir":"Articles","previous_headings":"","what":"Create a Picture","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"library(semPlot)  semPaths(overall.fit,           whatLabels = \"std\",           edge.label.cex = 1,          layout = \"tree\")"},{"path":"/articles/lecture_mgcfa.html","id":"men-overall-summary","dir":"Articles","previous_headings":"","what":"Men Overall Summary","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"men.fit <- cfa(model = overall.model,                data = res.data[res.data$Sex == \"Men\" , ],                 meanstructure = TRUE) summary(men.fit,         standardized = TRUE,          rsquare = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 25 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        42 #>  #>   Number of observations                           244 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               271.457 #>   Degrees of freedom                                77 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              2515.572 #>   Degrees of freedom                                91 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.920 #>   Tucker-Lewis Index (TLI)                       0.905 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -6043.672 #>   Loglikelihood unrestricted model (H1)      -5907.944 #>                                                        #>   Akaike (AIC)                               12171.345 #>   Bayesian (BIC)                             12318.226 #>   Sample-size adjusted Bayesian (SABIC)      12185.091 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.102 #>   90 Percent confidence interval - lower         0.089 #>   90 Percent confidence interval - upper         0.115 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    0.997 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.041 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.293    0.701 #>     RS2               1.190    0.105   11.360    0.000    1.538    0.755 #>     RS3               1.015    0.097   10.467    0.000    1.313    0.694 #>     RS4               1.131    0.108   10.498    0.000    1.462    0.696 #>     RS5               1.098    0.108   10.168    0.000    1.420    0.674 #>     RS6               1.122    0.101   11.093    0.000    1.451    0.737 #>     RS7               1.125    0.100   11.204    0.000    1.455    0.745 #>     RS8               1.188    0.101   11.799    0.000    1.536    0.785 #>     RS9               1.200    0.098   12.228    0.000    1.552    0.815 #>     RS10              1.103    0.097   11.354    0.000    1.426    0.755 #>     RS11              1.185    0.101   11.757    0.000    1.533    0.782 #>     RS12              1.213    0.102   11.877    0.000    1.568    0.791 #>     RS13              1.331    0.107   12.497    0.000    1.722    0.833 #>     RS14              1.267    0.101   12.546    0.000    1.639    0.837 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               4.697    0.118   39.741    0.000    4.697    2.544 #>    .RS2               4.197    0.130   32.180    0.000    4.197    2.060 #>    .RS3               4.242    0.121   35.035    0.000    4.242    2.243 #>    .RS4               4.615    0.134   34.330    0.000    4.615    2.198 #>    .RS5               3.971    0.135   29.448    0.000    3.971    1.885 #>    .RS6               4.889    0.126   38.788    0.000    4.889    2.483 #>    .RS7               4.820    0.125   38.520    0.000    4.820    2.466 #>    .RS8               4.422    0.125   35.315    0.000    4.422    2.261 #>    .RS9               4.299    0.122   35.259    0.000    4.299    2.257 #>    .RS10              4.537    0.121   37.516    0.000    4.537    2.402 #>    .RS11              4.467    0.125   35.614    0.000    4.467    2.280 #>    .RS12              4.820    0.127   37.954    0.000    4.820    2.430 #>    .RS13              4.799    0.132   36.291    0.000    4.799    2.323 #>    .RS14              4.697    0.125   37.467    0.000    4.697    2.399 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               1.736    0.165   10.517    0.000    1.736    0.509 #>    .RS2               1.783    0.173   10.317    0.000    1.783    0.430 #>    .RS3               1.852    0.176   10.535    0.000    1.852    0.518 #>    .RS4               2.270    0.216   10.529    0.000    2.270    0.515 #>    .RS5               2.421    0.229   10.589    0.000    2.421    0.546 #>    .RS6               1.771    0.170   10.393    0.000    1.771    0.457 #>    .RS7               1.702    0.164   10.363    0.000    1.702    0.446 #>    .RS8               1.466    0.144   10.162    0.000    1.466    0.383 #>    .RS9               1.219    0.122    9.960    0.000    1.219    0.336 #>    .RS10              1.535    0.149   10.319    0.000    1.535    0.430 #>    .RS11              1.489    0.146   10.179    0.000    1.489    0.388 #>    .RS12              1.475    0.146   10.130    0.000    1.475    0.375 #>    .RS13              1.303    0.133    9.795    0.000    1.303    0.305 #>    .RS14              1.149    0.118    9.762    0.000    1.149    0.300 #>     RS                1.672    0.270    6.188    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.491 #>     RS2               0.570 #>     RS3               0.482 #>     RS4               0.485 #>     RS5               0.454 #>     RS6               0.543 #>     RS7               0.554 #>     RS8               0.617 #>     RS9               0.664 #>     RS10              0.570 #>     RS11              0.612 #>     RS12              0.625 #>     RS13              0.695 #>     RS14              0.700"},{"path":"/articles/lecture_mgcfa.html","id":"women-overall-summary","dir":"Articles","previous_headings":"","what":"Women Overall Summary","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"women.fit <- cfa(model = overall.model,                  data = res.data[res.data$Sex == \"Women\" , ],                   meanstructure = TRUE) summary(women.fit,         standardized = TRUE,          rsquare = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 29 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        42 #>  #>   Number of observations                           266 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               258.610 #>   Degrees of freedom                                77 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              1813.360 #>   Degrees of freedom                                91 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.895 #>   Tucker-Lewis Index (TLI)                       0.875 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -6938.738 #>   Loglikelihood unrestricted model (H1)      -6809.434 #>                                                        #>   Akaike (AIC)                               13961.477 #>   Bayesian (BIC)                             14111.984 #>   Sample-size adjusted Bayesian (SABIC)      13978.820 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.094 #>   90 Percent confidence interval - lower         0.082 #>   90 Percent confidence interval - upper         0.107 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    0.968 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.052 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               0.958    0.560 #>     RS2               1.291    0.163    7.908    0.000    1.237    0.607 #>     RS3               1.140    0.149    7.675    0.000    1.093    0.582 #>     RS4               1.401    0.172    8.157    0.000    1.342    0.636 #>     RS5               1.381    0.172    8.046    0.000    1.323    0.623 #>     RS6               1.381    0.155    8.933    0.000    1.323    0.733 #>     RS7               1.485    0.162    9.150    0.000    1.423    0.764 #>     RS8               1.259    0.156    8.055    0.000    1.206    0.624 #>     RS9               1.400    0.164    8.543    0.000    1.341    0.682 #>     RS10              1.333    0.158    8.426    0.000    1.278    0.668 #>     RS11              1.503    0.172    8.754    0.000    1.440    0.709 #>     RS12              1.106    0.145    7.609    0.000    1.060    0.575 #>     RS13              1.421    0.167    8.513    0.000    1.362    0.679 #>     RS14              1.470    0.155    9.476    0.000    1.408    0.813 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               4.989    0.105   47.546    0.000    4.989    2.915 #>    .RS2               4.237    0.125   33.918    0.000    4.237    2.080 #>    .RS3               4.090    0.115   35.528    0.000    4.090    2.178 #>    .RS4               4.301    0.129   33.220    0.000    4.301    2.037 #>    .RS5               3.658    0.130   28.090    0.000    3.658    1.722 #>    .RS6               5.173    0.111   46.757    0.000    5.173    2.867 #>    .RS7               5.045    0.114   44.162    0.000    5.045    2.708 #>    .RS8               4.286    0.119   36.153    0.000    4.286    2.217 #>    .RS9               4.173    0.121   34.619    0.000    4.173    2.123 #>    .RS10              4.699    0.117   40.054    0.000    4.699    2.456 #>    .RS11              4.504    0.124   36.179    0.000    4.504    2.218 #>    .RS12              5.177    0.113   45.781    0.000    5.177    2.807 #>    .RS13              4.981    0.123   40.489    0.000    4.981    2.483 #>    .RS14              4.797    0.106   45.141    0.000    4.797    2.768 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               2.010    0.181   11.094    0.000    2.010    0.686 #>    .RS2               2.620    0.239   10.972    0.000    2.620    0.631 #>    .RS3               2.332    0.211   11.041    0.000    2.332    0.661 #>    .RS4               2.657    0.244   10.881    0.000    2.657    0.596 #>    .RS5               2.760    0.253   10.924    0.000    2.760    0.612 #>    .RS6               1.505    0.145   10.414    0.000    1.505    0.462 #>    .RS7               1.447    0.142   10.186    0.000    1.447    0.417 #>    .RS8               2.283    0.209   10.921    0.000    2.283    0.611 #>    .RS9               2.066    0.193   10.696    0.000    2.066    0.534 #>    .RS10              2.028    0.189   10.759    0.000    2.028    0.554 #>    .RS11              2.048    0.194   10.559    0.000    2.048    0.497 #>    .RS12              2.277    0.206   11.059    0.000    2.277    0.669 #>    .RS13              2.172    0.203   10.712    0.000    2.172    0.540 #>    .RS14              1.020    0.106    9.659    0.000    1.020    0.340 #>     RS                0.918    0.191    4.804    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.314 #>     RS2               0.369 #>     RS3               0.339 #>     RS4               0.404 #>     RS5               0.388 #>     RS6               0.538 #>     RS7               0.583 #>     RS8               0.389 #>     RS9               0.466 #>     RS10              0.446 #>     RS11              0.503 #>     RS12              0.331 #>     RS13              0.460 #>     RS14              0.660"},{"path":"/articles/lecture_mgcfa.html","id":"add-the-fit-to-table","dir":"Articles","previous_headings":"","what":"Add the fit to table","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"table_fit[2, ] <- c(\"Men Model\", round(fitmeasures(men.fit,                                                   c(\"chisq\", \"df\", \"cfi\",                                                    \"rmsea\", \"srmr\")),3))  table_fit[3, ] <- c(\"Women Model\", round(fitmeasures(women.fit,                                                   c(\"chisq\", \"df\", \"cfi\",                                                    \"rmsea\", \"srmr\")),3))  kable(table_fit)"},{"path":"/articles/lecture_mgcfa.html","id":"configural-invariance","dir":"Articles","previous_headings":"","what":"Configural Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"groups “pancaked” together","code":"configural.fit <- cfa(model = overall.model,                       data = res.data,                       meanstructure = TRUE,                       group = \"Sex\") summary(configural.fit,         standardized = TRUE,          rsquare = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 49 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        84 #>  #>   Number of observations per group:                    #>     Women                                          266 #>     Men                                            244 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               530.067 #>   Degrees of freedom                               154 #>   P-value (Chi-square)                           0.000 #>   Test statistic for each group: #>     Women                                      258.610 #>     Men                                        271.457 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4328.931 #>   Degrees of freedom                               182 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.909 #>   Tucker-Lewis Index (TLI)                       0.893 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -12982.411 #>   Loglikelihood unrestricted model (H1)     -12717.377 #>                                                        #>   Akaike (AIC)                               26132.822 #>   Bayesian (BIC)                             26488.512 #>   Sample-size adjusted Bayesian (SABIC)      26221.885 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.098 #>   90 Percent confidence interval - lower         0.089 #>   90 Percent confidence interval - upper         0.107 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    0.999 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.047 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #>  #> Group 1 [Women]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               0.958    0.560 #>     RS2               1.291    0.163    7.908    0.000    1.237    0.607 #>     RS3               1.140    0.149    7.675    0.000    1.093    0.582 #>     RS4               1.401    0.172    8.157    0.000    1.342    0.636 #>     RS5               1.381    0.172    8.046    0.000    1.323    0.623 #>     RS6               1.381    0.155    8.933    0.000    1.323    0.733 #>     RS7               1.485    0.162    9.150    0.000    1.423    0.764 #>     RS8               1.259    0.156    8.055    0.000    1.206    0.624 #>     RS9               1.400    0.164    8.543    0.000    1.341    0.682 #>     RS10              1.333    0.158    8.426    0.000    1.278    0.668 #>     RS11              1.503    0.172    8.754    0.000    1.440    0.709 #>     RS12              1.106    0.145    7.609    0.000    1.060    0.575 #>     RS13              1.421    0.167    8.513    0.000    1.362    0.679 #>     RS14              1.470    0.155    9.476    0.000    1.408    0.813 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               4.989    0.105   47.546    0.000    4.989    2.915 #>    .RS2               4.237    0.125   33.918    0.000    4.237    2.080 #>    .RS3               4.090    0.115   35.528    0.000    4.090    2.178 #>    .RS4               4.301    0.129   33.220    0.000    4.301    2.037 #>    .RS5               3.658    0.130   28.090    0.000    3.658    1.722 #>    .RS6               5.173    0.111   46.757    0.000    5.173    2.867 #>    .RS7               5.045    0.114   44.162    0.000    5.045    2.708 #>    .RS8               4.286    0.119   36.153    0.000    4.286    2.217 #>    .RS9               4.173    0.121   34.619    0.000    4.173    2.123 #>    .RS10              4.699    0.117   40.054    0.000    4.699    2.456 #>    .RS11              4.504    0.124   36.179    0.000    4.504    2.218 #>    .RS12              5.177    0.113   45.781    0.000    5.177    2.807 #>    .RS13              4.981    0.123   40.489    0.000    4.981    2.483 #>    .RS14              4.797    0.106   45.141    0.000    4.797    2.768 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               2.010    0.181   11.094    0.000    2.010    0.686 #>    .RS2               2.620    0.239   10.972    0.000    2.620    0.631 #>    .RS3               2.332    0.211   11.041    0.000    2.332    0.661 #>    .RS4               2.657    0.244   10.881    0.000    2.657    0.596 #>    .RS5               2.760    0.253   10.924    0.000    2.760    0.612 #>    .RS6               1.505    0.145   10.414    0.000    1.505    0.462 #>    .RS7               1.447    0.142   10.186    0.000    1.447    0.417 #>    .RS8               2.283    0.209   10.921    0.000    2.283    0.611 #>    .RS9               2.066    0.193   10.696    0.000    2.066    0.534 #>    .RS10              2.028    0.189   10.759    0.000    2.028    0.554 #>    .RS11              2.048    0.194   10.559    0.000    2.048    0.497 #>    .RS12              2.277    0.206   11.059    0.000    2.277    0.669 #>    .RS13              2.172    0.203   10.712    0.000    2.172    0.540 #>    .RS14              1.020    0.106    9.659    0.000    1.020    0.340 #>     RS                0.918    0.191    4.804    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.314 #>     RS2               0.369 #>     RS3               0.339 #>     RS4               0.404 #>     RS5               0.388 #>     RS6               0.538 #>     RS7               0.583 #>     RS8               0.389 #>     RS9               0.466 #>     RS10              0.446 #>     RS11              0.503 #>     RS12              0.331 #>     RS13              0.460 #>     RS14              0.660 #>  #>  #> Group 2 [Men]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.293    0.701 #>     RS2               1.190    0.105   11.360    0.000    1.538    0.755 #>     RS3               1.015    0.097   10.467    0.000    1.313    0.694 #>     RS4               1.131    0.108   10.498    0.000    1.462    0.696 #>     RS5               1.098    0.108   10.168    0.000    1.420    0.674 #>     RS6               1.122    0.101   11.093    0.000    1.451    0.737 #>     RS7               1.125    0.100   11.204    0.000    1.455    0.745 #>     RS8               1.188    0.101   11.799    0.000    1.536    0.785 #>     RS9               1.200    0.098   12.228    0.000    1.552    0.815 #>     RS10              1.103    0.097   11.354    0.000    1.426    0.755 #>     RS11              1.185    0.101   11.757    0.000    1.533    0.782 #>     RS12              1.213    0.102   11.877    0.000    1.568    0.791 #>     RS13              1.331    0.107   12.497    0.000    1.722    0.833 #>     RS14              1.267    0.101   12.546    0.000    1.639    0.837 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               4.697    0.118   39.741    0.000    4.697    2.544 #>    .RS2               4.197    0.130   32.180    0.000    4.197    2.060 #>    .RS3               4.242    0.121   35.035    0.000    4.242    2.243 #>    .RS4               4.615    0.134   34.330    0.000    4.615    2.198 #>    .RS5               3.971    0.135   29.448    0.000    3.971    1.885 #>    .RS6               4.889    0.126   38.788    0.000    4.889    2.483 #>    .RS7               4.820    0.125   38.520    0.000    4.820    2.466 #>    .RS8               4.422    0.125   35.315    0.000    4.422    2.261 #>    .RS9               4.299    0.122   35.259    0.000    4.299    2.257 #>    .RS10              4.537    0.121   37.516    0.000    4.537    2.402 #>    .RS11              4.467    0.125   35.614    0.000    4.467    2.280 #>    .RS12              4.820    0.127   37.954    0.000    4.820    2.430 #>    .RS13              4.799    0.132   36.291    0.000    4.799    2.323 #>    .RS14              4.697    0.125   37.467    0.000    4.697    2.399 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               1.736    0.165   10.517    0.000    1.736    0.509 #>    .RS2               1.783    0.173   10.317    0.000    1.783    0.430 #>    .RS3               1.852    0.176   10.535    0.000    1.852    0.518 #>    .RS4               2.270    0.216   10.529    0.000    2.270    0.515 #>    .RS5               2.421    0.229   10.589    0.000    2.421    0.546 #>    .RS6               1.771    0.170   10.393    0.000    1.771    0.457 #>    .RS7               1.702    0.164   10.363    0.000    1.702    0.446 #>    .RS8               1.466    0.144   10.162    0.000    1.466    0.383 #>    .RS9               1.219    0.122    9.960    0.000    1.219    0.336 #>    .RS10              1.535    0.149   10.319    0.000    1.535    0.430 #>    .RS11              1.489    0.146   10.179    0.000    1.489    0.388 #>    .RS12              1.475    0.146   10.130    0.000    1.475    0.375 #>    .RS13              1.303    0.133    9.795    0.000    1.303    0.305 #>    .RS14              1.149    0.118    9.762    0.000    1.149    0.300 #>     RS                1.672    0.270    6.188    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.491 #>     RS2               0.570 #>     RS3               0.482 #>     RS4               0.485 #>     RS5               0.454 #>     RS6               0.543 #>     RS7               0.554 #>     RS8               0.617 #>     RS9               0.664 #>     RS10              0.570 #>     RS11              0.612 #>     RS12              0.625 #>     RS13              0.695 #>     RS14              0.700"},{"path":"/articles/lecture_mgcfa.html","id":"add-the-fit-to-table-1","dir":"Articles","previous_headings":"","what":"Add the fit to table","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"table_fit[4, ] <- c(\"Configural Model\", round(fitmeasures(configural.fit,                                                   c(\"chisq\", \"df\", \"cfi\",                                                    \"rmsea\", \"srmr\")),3))  kable(table_fit)"},{"path":"/articles/lecture_mgcfa.html","id":"metric-invariance-1","dir":"Articles","previous_headings":"","what":"Metric Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"factor loadings ?","code":"metric.fit <- cfa(model = overall.model,                   data = res.data,                   meanstructure = TRUE,                   group = \"Sex\",                   group.equal = c(\"loadings\")) summary(metric.fit,         standardized = TRUE,          rsquare = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 36 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        84 #>   Number of equality constraints                    13 #>  #>   Number of observations per group:                    #>     Women                                          266 #>     Men                                            244 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               545.518 #>   Degrees of freedom                               167 #>   P-value (Chi-square)                           0.000 #>   Test statistic for each group: #>     Women                                      268.133 #>     Men                                        277.385 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4328.931 #>   Degrees of freedom                               182 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.909 #>   Tucker-Lewis Index (TLI)                       0.901 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -12990.137 #>   Loglikelihood unrestricted model (H1)     -12717.377 #>                                                        #>   Akaike (AIC)                               26122.273 #>   Bayesian (BIC)                             26422.916 #>   Sample-size adjusted Bayesian (SABIC)      26197.552 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.094 #>   90 Percent confidence interval - lower         0.086 #>   90 Percent confidence interval - upper         0.103 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    0.996 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.057 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #>  #> Group 1 [Women]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.047    0.595 #>     RS2     (.p2.)    1.232    0.089   13.828    0.000    1.290    0.624 #>     RS3     (.p3.)    1.063    0.082   12.984    0.000    1.113    0.589 #>     RS4     (.p4.)    1.229    0.092   13.338    0.000    1.287    0.619 #>     RS5     (.p5.)    1.202    0.092   13.025    0.000    1.258    0.603 #>     RS6     (.p6.)    1.221    0.085   14.407    0.000    1.279    0.720 #>     RS7     (.p7.)    1.269    0.086   14.698    0.000    1.328    0.738 #>     RS8     (.p8.)    1.219    0.086   14.256    0.000    1.277    0.646 #>     RS9     (.p9.)    1.270    0.085   14.871    0.000    1.330    0.679 #>     RS10    (.10.)    1.184    0.084   14.133    0.000    1.240    0.656 #>     RS11    (.11.)    1.296    0.088   14.662    0.000    1.357    0.686 #>     RS12    (.12.)    1.187    0.084   14.058    0.000    1.243    0.636 #>     RS13    (.13.)    1.370    0.091   15.075    0.000    1.434    0.698 #>     RS14    (.14.)    1.340    0.085   15.774    0.000    1.403    0.811 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               4.989    0.108   46.212    0.000    4.989    2.833 #>    .RS2               4.237    0.127   33.447    0.000    4.237    2.051 #>    .RS3               4.090    0.116   35.332    0.000    4.090    2.166 #>    .RS4               4.301    0.128   33.724    0.000    4.301    2.068 #>    .RS5               3.658    0.128   28.593    0.000    3.658    1.753 #>    .RS6               5.173    0.109   47.491    0.000    5.173    2.912 #>    .RS7               5.045    0.110   45.709    0.000    5.045    2.803 #>    .RS8               4.286    0.121   35.351    0.000    4.286    2.167 #>    .RS9               4.173    0.120   34.740    0.000    4.173    2.130 #>    .RS10              4.699    0.116   40.534    0.000    4.699    2.485 #>    .RS11              4.504    0.121   37.122    0.000    4.504    2.276 #>    .RS12              5.177    0.120   43.174    0.000    5.177    2.647 #>    .RS13              4.981    0.126   39.540    0.000    4.981    2.424 #>    .RS14              4.797    0.106   45.258    0.000    4.797    2.775 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               2.004    0.182   11.028    0.000    2.004    0.646 #>    .RS2               2.605    0.238   10.946    0.000    2.605    0.610 #>    .RS3               2.327    0.211   11.043    0.000    2.327    0.653 #>    .RS4               2.670    0.244   10.961    0.000    2.670    0.617 #>    .RS5               2.771    0.252   11.006    0.000    2.771    0.636 #>    .RS6               1.520    0.144   10.533    0.000    1.520    0.482 #>    .RS7               1.476    0.142   10.422    0.000    1.476    0.455 #>    .RS8               2.279    0.210   10.876    0.000    2.279    0.583 #>    .RS9               2.070    0.193   10.749    0.000    2.070    0.539 #>    .RS10              2.037    0.188   10.838    0.000    2.037    0.570 #>    .RS11              2.073    0.194   10.715    0.000    2.073    0.530 #>    .RS12              2.279    0.209   10.910    0.000    2.279    0.596 #>    .RS13              2.165    0.203   10.660    0.000    2.165    0.513 #>    .RS14              1.021    0.105    9.740    0.000    1.021    0.342 #>     RS                1.096    0.156    7.018    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.354 #>     RS2               0.390 #>     RS3               0.347 #>     RS4               0.383 #>     RS5               0.364 #>     RS6               0.518 #>     RS7               0.545 #>     RS8               0.417 #>     RS9               0.461 #>     RS10              0.430 #>     RS11              0.470 #>     RS12              0.404 #>     RS13              0.487 #>     RS14              0.658 #>  #>  #> Group 2 [Men]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.225    0.680 #>     RS2     (.p2.)    1.232    0.089   13.828    0.000    1.510    0.749 #>     RS3     (.p3.)    1.063    0.082   12.984    0.000    1.302    0.692 #>     RS4     (.p4.)    1.229    0.092   13.338    0.000    1.506    0.707 #>     RS5     (.p5.)    1.202    0.092   13.025    0.000    1.472    0.687 #>     RS6     (.p6.)    1.221    0.085   14.407    0.000    1.497    0.748 #>     RS7     (.p7.)    1.269    0.086   14.698    0.000    1.555    0.767 #>     RS8     (.p8.)    1.219    0.086   14.256    0.000    1.494    0.776 #>     RS9     (.p9.)    1.270    0.085   14.871    0.000    1.556    0.815 #>     RS10    (.10.)    1.184    0.084   14.133    0.000    1.451    0.761 #>     RS11    (.11.)    1.296    0.088   14.662    0.000    1.588    0.794 #>     RS12    (.12.)    1.187    0.084   14.058    0.000    1.455    0.764 #>     RS13    (.13.)    1.370    0.091   15.075    0.000    1.678    0.825 #>     RS14    (.14.)    1.340    0.085   15.774    0.000    1.642    0.836 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               4.697    0.115   40.725    0.000    4.697    2.607 #>    .RS2               4.197    0.129   32.526    0.000    4.197    2.082 #>    .RS3               4.242    0.121   35.202    0.000    4.242    2.254 #>    .RS4               4.615    0.136   33.863    0.000    4.615    2.168 #>    .RS5               3.971    0.137   28.963    0.000    3.971    1.854 #>    .RS6               4.889    0.128   38.172    0.000    4.889    2.444 #>    .RS7               4.820    0.130   37.142    0.000    4.820    2.378 #>    .RS8               4.422    0.123   35.886    0.000    4.422    2.297 #>    .RS9               4.299    0.122   35.175    0.000    4.299    2.252 #>    .RS10              4.537    0.122   37.150    0.000    4.537    2.378 #>    .RS11              4.467    0.128   34.892    0.000    4.467    2.234 #>    .RS12              4.820    0.122   39.522    0.000    4.820    2.530 #>    .RS13              4.799    0.130   36.857    0.000    4.799    2.360 #>    .RS14              4.697    0.126   37.373    0.000    4.697    2.393 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               1.744    0.165   10.580    0.000    1.744    0.537 #>    .RS2               1.783    0.172   10.353    0.000    1.783    0.439 #>    .RS3               1.847    0.175   10.549    0.000    1.847    0.521 #>    .RS4               2.263    0.215   10.504    0.000    2.263    0.499 #>    .RS5               2.420    0.229   10.561    0.000    2.420    0.527 #>    .RS6               1.763    0.170   10.360    0.000    1.763    0.440 #>    .RS7               1.691    0.165   10.275    0.000    1.691    0.412 #>    .RS8               1.472    0.144   10.222    0.000    1.472    0.397 #>    .RS9               1.223    0.123    9.969    0.000    1.223    0.336 #>    .RS10              1.532    0.149   10.301    0.000    1.532    0.421 #>    .RS11              1.476    0.146   10.119    0.000    1.476    0.369 #>    .RS12              1.512    0.147   10.286    0.000    1.512    0.417 #>    .RS13              1.320    0.134    9.886    0.000    1.320    0.319 #>    .RS14              1.158    0.118    9.788    0.000    1.158    0.301 #>     RS                1.502    0.215    6.973    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.463 #>     RS2               0.561 #>     RS3               0.479 #>     RS4               0.501 #>     RS5               0.473 #>     RS6               0.560 #>     RS7               0.588 #>     RS8               0.603 #>     RS9               0.664 #>     RS10              0.579 #>     RS11              0.631 #>     RS12              0.583 #>     RS13              0.681 #>     RS14              0.699"},{"path":"/articles/lecture_mgcfa.html","id":"add-the-fit-to-table-2","dir":"Articles","previous_headings":"","what":"Add the fit to table","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"table_fit[5, ] <- c(\"Metric Model\", round(fitmeasures(metric.fit,                                                   c(\"chisq\", \"df\", \"cfi\",                                                    \"rmsea\", \"srmr\")),3))  kable(table_fit)"},{"path":"/articles/lecture_mgcfa.html","id":"scalar-invariance-1","dir":"Articles","previous_headings":"","what":"Scalar Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"item intercepts ?","code":"scalar.fit <- cfa(model = overall.model,                   data = res.data,                   meanstructure = TRUE,                   group = \"Sex\",                   group.equal = c(\"loadings\", \"intercepts\")) summary(scalar.fit,         standardized = TRUE,          rsquare = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 47 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        85 #>   Number of equality constraints                    27 #>  #>   Number of observations per group:                    #>     Women                                          266 #>     Men                                            244 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               585.327 #>   Degrees of freedom                               180 #>   P-value (Chi-square)                           0.000 #>   Test statistic for each group: #>     Women                                      289.405 #>     Men                                        295.921 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4328.931 #>   Degrees of freedom                               182 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.902 #>   Tucker-Lewis Index (TLI)                       0.901 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -13010.041 #>   Loglikelihood unrestricted model (H1)     -12717.377 #>                                                        #>   Akaike (AIC)                               26136.082 #>   Bayesian (BIC)                             26381.678 #>   Sample-size adjusted Bayesian (SABIC)      26197.577 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.094 #>   90 Percent confidence interval - lower         0.086 #>   90 Percent confidence interval - upper         0.103 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    0.997 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.060 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #>  #> Group 1 [Women]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.049    0.594 #>     RS2     (.p2.)    1.229    0.089   13.815    0.000    1.289    0.624 #>     RS3     (.p3.)    1.057    0.082   12.939    0.000    1.110    0.587 #>     RS4     (.p4.)    1.221    0.092   13.252    0.000    1.281    0.614 #>     RS5     (.p5.)    1.193    0.092   12.939    0.000    1.252    0.598 #>     RS6     (.p6.)    1.221    0.085   14.382    0.000    1.281    0.719 #>     RS7     (.p7.)    1.268    0.086   14.682    0.000    1.330    0.738 #>     RS8     (.p8.)    1.214    0.085   14.216    0.000    1.274    0.644 #>     RS9     (.p9.)    1.265    0.085   14.833    0.000    1.327    0.677 #>     RS10    (.10.)    1.183    0.084   14.124    0.000    1.241    0.656 #>     RS11    (.11.)    1.293    0.088   14.648    0.000    1.357    0.686 #>     RS12    (.12.)    1.187    0.085   14.028    0.000    1.246    0.634 #>     RS13    (.13.)    1.368    0.091   15.062    0.000    1.435    0.698 #>     RS14    (.14.)    1.338    0.085   15.761    0.000    1.403    0.812 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.30.)    4.866    0.090   54.364    0.000    4.866    2.755 #>    .RS2     (.31.)    4.252    0.104   40.972    0.000    4.252    2.058 #>    .RS3     (.32.)    4.202    0.094   44.469    0.000    4.202    2.223 #>    .RS4     (.33.)    4.498    0.107   42.235    0.000    4.498    2.155 #>    .RS5     (.34.)    3.851    0.106   36.218    0.000    3.851    1.840 #>    .RS6     (.35.)    5.077    0.098   51.873    0.000    5.077    2.851 #>    .RS7     (.36.)    4.975    0.100   49.859    0.000    4.975    2.760 #>    .RS8     (.37.)    4.404    0.100   44.026    0.000    4.404    2.225 #>    .RS9     (.38.)    4.291    0.100   42.723    0.000    4.291    2.188 #>    .RS10    (.39.)    4.645    0.098   47.580    0.000    4.645    2.455 #>    .RS11    (.40.)    4.522    0.103   43.754    0.000    4.522    2.286 #>    .RS12    (.41.)    5.006    0.099   50.590    0.000    5.006    2.547 #>    .RS13    (.42.)    4.916    0.107   45.940    0.000    4.916    2.391 #>    .RS14    (.43.)    4.784    0.099   48.420    0.000    4.784    2.767 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               2.019    0.183   11.027    0.000    2.019    0.647 #>    .RS2               2.606    0.238   10.944    0.000    2.606    0.611 #>    .RS3               2.342    0.212   11.046    0.000    2.342    0.655 #>    .RS4               2.714    0.247   10.972    0.000    2.714    0.623 #>    .RS5               2.814    0.255   11.016    0.000    2.814    0.642 #>    .RS6               1.530    0.145   10.531    0.000    1.530    0.482 #>    .RS7               1.480    0.142   10.416    0.000    1.480    0.455 #>    .RS8               2.295    0.211   10.880    0.000    2.295    0.586 #>    .RS9               2.085    0.194   10.754    0.000    2.085    0.542 #>    .RS10              2.040    0.188   10.834    0.000    2.040    0.570 #>    .RS11              2.074    0.194   10.712    0.000    2.074    0.530 #>    .RS12              2.310    0.212   10.913    0.000    2.310    0.598 #>    .RS13              2.169    0.204   10.656    0.000    2.169    0.513 #>    .RS14              1.019    0.105    9.727    0.000    1.019    0.341 #>     RS                1.101    0.157    7.011    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.353 #>     RS2               0.389 #>     RS3               0.345 #>     RS4               0.377 #>     RS5               0.358 #>     RS6               0.518 #>     RS7               0.545 #>     RS8               0.414 #>     RS9               0.458 #>     RS10              0.430 #>     RS11              0.470 #>     RS12              0.402 #>     RS13              0.487 #>     RS14              0.659 #>  #>  #> Group 2 [Men]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.228    0.680 #>     RS2     (.p2.)    1.229    0.089   13.815    0.000    1.509    0.749 #>     RS3     (.p3.)    1.057    0.082   12.939    0.000    1.299    0.690 #>     RS4     (.p4.)    1.221    0.092   13.252    0.000    1.500    0.703 #>     RS5     (.p5.)    1.193    0.092   12.939    0.000    1.466    0.683 #>     RS6     (.p6.)    1.221    0.085   14.382    0.000    1.500    0.747 #>     RS7     (.p7.)    1.268    0.086   14.682    0.000    1.557    0.767 #>     RS8     (.p8.)    1.214    0.085   14.216    0.000    1.491    0.775 #>     RS9     (.p9.)    1.265    0.085   14.833    0.000    1.554    0.814 #>     RS10    (.10.)    1.183    0.084   14.124    0.000    1.453    0.761 #>     RS11    (.11.)    1.293    0.088   14.648    0.000    1.588    0.794 #>     RS12    (.12.)    1.187    0.085   14.028    0.000    1.458    0.763 #>     RS13    (.13.)    1.368    0.091   15.062    0.000    1.680    0.825 #>     RS14    (.14.)    1.338    0.085   15.761    0.000    1.643    0.837 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.30.)    4.866    0.090   54.364    0.000    4.866    2.693 #>    .RS2     (.31.)    4.252    0.104   40.972    0.000    4.252    2.110 #>    .RS3     (.32.)    4.202    0.094   44.469    0.000    4.202    2.232 #>    .RS4     (.33.)    4.498    0.107   42.235    0.000    4.498    2.109 #>    .RS5     (.34.)    3.851    0.106   36.218    0.000    3.851    1.794 #>    .RS6     (.35.)    5.077    0.098   51.873    0.000    5.077    2.530 #>    .RS7     (.36.)    4.975    0.100   49.859    0.000    4.975    2.450 #>    .RS8     (.37.)    4.404    0.100   44.026    0.000    4.404    2.288 #>    .RS9     (.38.)    4.291    0.100   42.723    0.000    4.291    2.248 #>    .RS10    (.39.)    4.645    0.098   47.580    0.000    4.645    2.433 #>    .RS11    (.40.)    4.522    0.103   43.754    0.000    4.522    2.262 #>    .RS12    (.41.)    5.006    0.099   50.590    0.000    5.006    2.619 #>    .RS13    (.42.)    4.916    0.107   45.940    0.000    4.916    2.415 #>    .RS14    (.43.)    4.784    0.099   48.420    0.000    4.784    2.437 #>     RS               -0.054    0.105   -0.512    0.609   -0.044   -0.044 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1               1.757    0.166   10.579    0.000    1.757    0.538 #>    .RS2               1.784    0.172   10.350    0.000    1.784    0.439 #>    .RS3               1.858    0.176   10.552    0.000    1.858    0.524 #>    .RS4               2.300    0.219   10.514    0.000    2.300    0.506 #>    .RS5               2.457    0.232   10.571    0.000    2.457    0.533 #>    .RS6               1.779    0.172   10.361    0.000    1.779    0.442 #>    .RS7               1.700    0.165   10.273    0.000    1.700    0.412 #>    .RS8               1.481    0.145   10.226    0.000    1.481    0.400 #>    .RS9               1.230    0.123    9.973    0.000    1.230    0.338 #>    .RS10              1.533    0.149   10.296    0.000    1.533    0.421 #>    .RS11              1.476    0.146   10.114    0.000    1.476    0.369 #>    .RS12              1.527    0.148   10.286    0.000    1.527    0.418 #>    .RS13              1.321    0.134    9.878    0.000    1.321    0.319 #>    .RS14              1.156    0.118    9.778    0.000    1.156    0.300 #>     RS                1.509    0.217    6.966    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.462 #>     RS2               0.561 #>     RS3               0.476 #>     RS4               0.494 #>     RS5               0.467 #>     RS6               0.558 #>     RS7               0.588 #>     RS8               0.600 #>     RS9               0.662 #>     RS10              0.579 #>     RS11              0.631 #>     RS12              0.582 #>     RS13              0.681 #>     RS14              0.700"},{"path":"/articles/lecture_mgcfa.html","id":"add-the-fit-to-table-3","dir":"Articles","previous_headings":"","what":"Add the fit to table","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"table_fit[6, ] <- c(\"Scalar Model\", round(fitmeasures(scalar.fit,                                                   c(\"chisq\", \"df\", \"cfi\",                                                    \"rmsea\", \"srmr\")),3))  kable(table_fit)"},{"path":"/articles/lecture_mgcfa.html","id":"strict-error-invariance","dir":"Articles","previous_headings":"","what":"Strict (Error) Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"item residuals ?","code":"strict.fit <- cfa(model = overall.model,                   data = res.data,                   meanstructure = TRUE,                   group = \"Sex\",                   group.equal = c(\"loadings\", \"intercepts\", \"residuals\")) summary(strict.fit,         standardized = TRUE,          rsquare = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 46 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        85 #>   Number of equality constraints                    41 #>  #>   Number of observations per group:                    #>     Women                                          266 #>     Men                                            244 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               660.398 #>   Degrees of freedom                               194 #>   P-value (Chi-square)                           0.000 #>   Test statistic for each group: #>     Women                                      314.816 #>     Men                                        345.582 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4328.931 #>   Degrees of freedom                               182 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.888 #>   Tucker-Lewis Index (TLI)                       0.894 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -13047.576 #>   Loglikelihood unrestricted model (H1)     -12717.377 #>                                                        #>   Akaike (AIC)                               26183.153 #>   Bayesian (BIC)                             26369.467 #>   Sample-size adjusted Bayesian (SABIC)      26229.805 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.097 #>   90 Percent confidence interval - lower         0.089 #>   90 Percent confidence interval - upper         0.105 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.062 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #>  #> Group 1 [Women]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.052    0.608 #>     RS2     (.p2.)    1.229    0.091   13.545    0.000    1.292    0.657 #>     RS3     (.p3.)    1.064    0.083   12.817    0.000    1.119    0.612 #>     RS4     (.p4.)    1.229    0.093   13.156    0.000    1.293    0.632 #>     RS5     (.p5.)    1.201    0.093   12.851    0.000    1.262    0.614 #>     RS6     (.p6.)    1.217    0.085   14.274    0.000    1.280    0.705 #>     RS7     (.p7.)    1.262    0.087   14.567    0.000    1.327    0.725 #>     RS8     (.p8.)    1.206    0.087   13.852    0.000    1.269    0.677 #>     RS9     (.p9.)    1.270    0.088   14.473    0.000    1.335    0.718 #>     RS10    (.10.)    1.192    0.085   13.957    0.000    1.253    0.684 #>     RS11    (.11.)    1.315    0.090   14.535    0.000    1.383    0.722 #>     RS12    (.12.)    1.156    0.085   13.524    0.000    1.216    0.656 #>     RS13    (.13.)    1.356    0.093   14.656    0.000    1.426    0.731 #>     RS14    (.14.)    1.334    0.085   15.610    0.000    1.402    0.800 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.30.)    4.874    0.090   54.448    0.000    4.874    2.817 #>    .RS2     (.31.)    4.249    0.104   40.833    0.000    4.249    2.160 #>    .RS3     (.32.)    4.190    0.095   44.193    0.000    4.190    2.291 #>    .RS4     (.33.)    4.482    0.107   41.915    0.000    4.482    2.193 #>    .RS5     (.34.)    3.838    0.107   35.963    0.000    3.838    1.866 #>    .RS6     (.35.)    5.068    0.098   51.613    0.000    5.068    2.791 #>    .RS7     (.36.)    4.969    0.100   49.707    0.000    4.969    2.713 #>    .RS8     (.37.)    4.381    0.100   43.790    0.000    4.381    2.337 #>    .RS9     (.38.)    4.265    0.101   42.167    0.000    4.265    2.295 #>    .RS10    (.39.)    4.652    0.098   47.397    0.000    4.652    2.538 #>    .RS11    (.40.)    4.519    0.104   43.296    0.000    4.519    2.361 #>    .RS12    (.41.)    5.035    0.098   51.359    0.000    5.035    2.715 #>    .RS13    (.42.)    4.928    0.107   46.139    0.000    4.928    2.526 #>    .RS14    (.43.)    4.783    0.099   48.219    0.000    4.783    2.728 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.15.)    1.887    0.124   15.241    0.000    1.887    0.631 #>    .RS2     (.16.)    2.200    0.146   15.025    0.000    2.200    0.568 #>    .RS3     (.17.)    2.094    0.138   15.226    0.000    2.094    0.626 #>    .RS4     (.18.)    2.507    0.166   15.141    0.000    2.507    0.600 #>    .RS5     (.19.)    2.638    0.173   15.218    0.000    2.638    0.623 #>    .RS6     (.20.)    1.660    0.113   14.742    0.000    1.660    0.503 #>    .RS7     (.21.)    1.593    0.109   14.593    0.000    1.593    0.475 #>    .RS8     (.22.)    1.905    0.128   14.918    0.000    1.905    0.542 #>    .RS9     (.23.)    1.673    0.114   14.643    0.000    1.673    0.484 #>    .RS10    (.24.)    1.789    0.120   14.878    0.000    1.789    0.533 #>    .RS11    (.25.)    1.752    0.120   14.610    0.000    1.752    0.478 #>    .RS12    (.26.)    1.961    0.130   15.032    0.000    1.961    0.570 #>    .RS13    (.27.)    1.773    0.122   14.542    0.000    1.773    0.466 #>    .RS14    (.28.)    1.106    0.080   13.754    0.000    1.106    0.360 #>     RS                1.106    0.158    6.990    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.369 #>     RS2               0.432 #>     RS3               0.374 #>     RS4               0.400 #>     RS5               0.377 #>     RS6               0.497 #>     RS7               0.525 #>     RS8               0.458 #>     RS9               0.516 #>     RS10              0.467 #>     RS11              0.522 #>     RS12              0.430 #>     RS13              0.534 #>     RS14              0.640 #>  #>  #> Group 2 [Men]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.222    0.665 #>     RS2     (.p2.)    1.229    0.091   13.545    0.000    1.502    0.711 #>     RS3     (.p3.)    1.064    0.083   12.817    0.000    1.300    0.668 #>     RS4     (.p4.)    1.229    0.093   13.156    0.000    1.502    0.688 #>     RS5     (.p5.)    1.201    0.093   12.851    0.000    1.467    0.670 #>     RS6     (.p6.)    1.217    0.085   14.274    0.000    1.487    0.756 #>     RS7     (.p7.)    1.262    0.087   14.567    0.000    1.542    0.774 #>     RS8     (.p8.)    1.206    0.087   13.852    0.000    1.474    0.730 #>     RS9     (.p9.)    1.270    0.088   14.473    0.000    1.552    0.768 #>     RS10    (.10.)    1.192    0.085   13.957    0.000    1.456    0.736 #>     RS11    (.11.)    1.315    0.090   14.535    0.000    1.607    0.772 #>     RS12    (.12.)    1.156    0.085   13.524    0.000    1.413    0.710 #>     RS13    (.13.)    1.356    0.093   14.656    0.000    1.657    0.780 #>     RS14    (.14.)    1.334    0.085   15.610    0.000    1.630    0.840 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.30.)    4.874    0.090   54.448    0.000    4.874    2.651 #>    .RS2     (.31.)    4.249    0.104   40.833    0.000    4.249    2.013 #>    .RS3     (.32.)    4.190    0.095   44.193    0.000    4.190    2.154 #>    .RS4     (.33.)    4.482    0.107   41.915    0.000    4.482    2.054 #>    .RS5     (.34.)    3.838    0.107   35.963    0.000    3.838    1.754 #>    .RS6     (.35.)    5.068    0.098   51.613    0.000    5.068    2.576 #>    .RS7     (.36.)    4.969    0.100   49.707    0.000    4.969    2.493 #>    .RS8     (.37.)    4.381    0.100   43.790    0.000    4.381    2.170 #>    .RS9     (.38.)    4.265    0.101   42.167    0.000    4.265    2.112 #>    .RS10    (.39.)    4.652    0.098   47.397    0.000    4.652    2.353 #>    .RS11    (.40.)    4.519    0.104   43.296    0.000    4.519    2.171 #>    .RS12    (.41.)    5.035    0.098   51.359    0.000    5.035    2.531 #>    .RS13    (.42.)    4.928    0.107   46.139    0.000    4.928    2.318 #>    .RS14    (.43.)    4.783    0.099   48.219    0.000    4.783    2.466 #>     RS               -0.053    0.105   -0.504    0.614   -0.043   -0.043 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.15.)    1.887    0.124   15.241    0.000    1.887    0.558 #>    .RS2     (.16.)    2.200    0.146   15.025    0.000    2.200    0.494 #>    .RS3     (.17.)    2.094    0.138   15.226    0.000    2.094    0.553 #>    .RS4     (.18.)    2.507    0.166   15.141    0.000    2.507    0.526 #>    .RS5     (.19.)    2.638    0.173   15.218    0.000    2.638    0.551 #>    .RS6     (.20.)    1.660    0.113   14.742    0.000    1.660    0.429 #>    .RS7     (.21.)    1.593    0.109   14.593    0.000    1.593    0.401 #>    .RS8     (.22.)    1.905    0.128   14.918    0.000    1.905    0.467 #>    .RS9     (.23.)    1.673    0.114   14.643    0.000    1.673    0.410 #>    .RS10    (.24.)    1.789    0.120   14.878    0.000    1.789    0.458 #>    .RS11    (.25.)    1.752    0.120   14.610    0.000    1.752    0.404 #>    .RS12    (.26.)    1.961    0.130   15.032    0.000    1.961    0.496 #>    .RS13    (.27.)    1.773    0.122   14.542    0.000    1.773    0.392 #>    .RS14    (.28.)    1.106    0.080   13.754    0.000    1.106    0.294 #>     RS                1.493    0.216    6.917    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.442 #>     RS2               0.506 #>     RS3               0.447 #>     RS4               0.474 #>     RS5               0.449 #>     RS6               0.571 #>     RS7               0.599 #>     RS8               0.533 #>     RS9               0.590 #>     RS10              0.542 #>     RS11              0.596 #>     RS12              0.504 #>     RS13              0.608 #>     RS14              0.706"},{"path":"/articles/lecture_mgcfa.html","id":"add-the-fit-to-table-4","dir":"Articles","previous_headings":"","what":"Add the fit to table","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"table_fit[7, ] <- c(\"Strict Model\", round(fitmeasures(strict.fit,                                                   c(\"chisq\", \"df\", \"cfi\",                                                    \"rmsea\", \"srmr\")),3))  kable(table_fit)"},{"path":"/articles/lecture_mgcfa.html","id":"partial-invariance-1","dir":"Articles","previous_headings":"","what":"Partial Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"can see last step difference models based change CFI. Now, figure problem update model fix problem. look problems step .","code":""},{"path":"/articles/lecture_mgcfa.html","id":"partial-invariance-2","dir":"Articles","previous_headings":"","what":"Partial Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"metric =~ loadings scalar ~1 intercepts strict ~~ variances Change item one time note changes","code":""},{"path":"/articles/lecture_mgcfa.html","id":"partial-invariance-3","dir":"Articles","previous_headings":"","what":"Partial Invariance","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"##write out partial codes partial_syntax <- paste(colnames(res.data)[3:16], #all the columns                         \"~~\", #residuals                         colnames(res.data)[3:16]) #all columns again  partial_syntax #>  [1] \"RS1 ~~ RS1\"   \"RS2 ~~ RS2\"   \"RS3 ~~ RS3\"   \"RS4 ~~ RS4\"   \"RS5 ~~ RS5\"   #>  [6] \"RS6 ~~ RS6\"   \"RS7 ~~ RS7\"   \"RS8 ~~ RS8\"   \"RS9 ~~ RS9\"   \"RS10 ~~ RS10\" #> [11] \"RS11 ~~ RS11\" \"RS12 ~~ RS12\" \"RS13 ~~ RS13\" \"RS14 ~~ RS14\"  CFI_list  <- 1:length(partial_syntax) names(CFI_list) <- partial_syntax  for (i in 1:length(partial_syntax)){      temp <- cfa(model = overall.model,                data = res.data,               meanstructure = TRUE,               group = \"Sex\",               group.equal = c(\"loadings\", \"intercepts\", \"residuals\"),               group.partial = partial_syntax[i])      CFI_list[i] <- fitmeasures(temp, \"cfi\") }  CFI_list #>   RS1 ~~ RS1   RS2 ~~ RS2   RS3 ~~ RS3   RS4 ~~ RS4   RS5 ~~ RS5   RS6 ~~ RS6  #>    0.8877040    0.8892794    0.8881315    0.8876596    0.8875493    0.8874662  #>   RS7 ~~ RS7   RS8 ~~ RS8   RS9 ~~ RS9 RS10 ~~ RS10 RS11 ~~ RS11 RS12 ~~ RS12  #>    0.8873667    0.8897416    0.8906278    0.8882753    0.8888421    0.8896339  #> RS13 ~~ RS13 RS14 ~~ RS14  #>    0.8905766    0.8873834 which.max(CFI_list) #> RS9 ~~ RS9  #>          9"},{"path":"/articles/lecture_mgcfa.html","id":"free-up-those-paramers","dir":"Articles","previous_headings":"","what":"Free up those paramers!","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"strict.fit2 <- cfa(model = overall.model,                   data = res.data,                   meanstructure = TRUE,                   group = \"Sex\",                   group.equal = c(\"loadings\", \"intercepts\", \"residuals\"),                    group.partial = c(\"RS9 ~~ RS9\")) summary(strict.fit2,         standardized = TRUE,          rsquare = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 48 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        85 #>   Number of equality constraints                    40 #>  #>   Number of observations per group:                    #>     Women                                          266 #>     Men                                            244 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               646.559 #>   Degrees of freedom                               193 #>   P-value (Chi-square)                           0.000 #>   Test statistic for each group: #>     Women                                      310.363 #>     Men                                        336.196 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4328.931 #>   Degrees of freedom                               182 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.891 #>   Tucker-Lewis Index (TLI)                       0.897 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -13040.657 #>   Loglikelihood unrestricted model (H1)     -12717.377 #>                                                        #>   Akaike (AIC)                               26171.314 #>   Bayesian (BIC)                             26361.863 #>   Sample-size adjusted Bayesian (SABIC)      26219.026 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.096 #>   90 Percent confidence interval - lower         0.088 #>   90 Percent confidence interval - upper         0.104 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    0.999 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.062 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #>  #> Group 1 [Women]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.050    0.607 #>     RS2     (.p2.)    1.229    0.091   13.526    0.000    1.290    0.656 #>     RS3     (.p3.)    1.064    0.083   12.806    0.000    1.117    0.611 #>     RS4     (.p4.)    1.232    0.094   13.161    0.000    1.293    0.633 #>     RS5     (.p5.)    1.200    0.094   12.829    0.000    1.260    0.612 #>     RS6     (.p6.)    1.218    0.085   14.264    0.000    1.279    0.704 #>     RS7     (.p7.)    1.263    0.087   14.550    0.000    1.325    0.724 #>     RS8     (.p8.)    1.207    0.087   13.840    0.000    1.267    0.676 #>     RS9     (.p9.)    1.269    0.086   14.706    0.000    1.332    0.679 #>     RS10    (.10.)    1.194    0.086   13.965    0.000    1.254    0.684 #>     RS11    (.11.)    1.313    0.091   14.497    0.000    1.379    0.720 #>     RS12    (.12.)    1.160    0.086   13.546    0.000    1.218    0.657 #>     RS13    (.13.)    1.359    0.093   14.659    0.000    1.427    0.732 #>     RS14    (.14.)    1.334    0.086   15.592    0.000    1.401    0.799 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.30.)    4.874    0.089   54.484    0.000    4.874    2.818 #>    .RS2     (.31.)    4.249    0.104   40.863    0.000    4.249    2.160 #>    .RS3     (.32.)    4.190    0.095   44.218    0.000    4.190    2.291 #>    .RS4     (.33.)    4.482    0.107   41.924    0.000    4.482    2.194 #>    .RS5     (.34.)    3.838    0.107   35.990    0.000    3.838    1.866 #>    .RS6     (.35.)    5.068    0.098   51.642    0.000    5.068    2.792 #>    .RS7     (.36.)    4.969    0.100   49.744    0.000    4.969    2.714 #>    .RS8     (.37.)    4.381    0.100   43.816    0.000    4.381    2.338 #>    .RS9     (.38.)    4.290    0.101   42.586    0.000    4.290    2.187 #>    .RS10    (.39.)    4.652    0.098   47.405    0.000    4.652    2.539 #>    .RS11    (.40.)    4.519    0.104   43.349    0.000    4.519    2.361 #>    .RS12    (.41.)    5.035    0.098   51.351    0.000    5.035    2.717 #>    .RS13    (.42.)    4.928    0.107   46.152    0.000    4.928    2.527 #>    .RS14    (.43.)    4.783    0.099   48.258    0.000    4.783    2.730 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.15.)    1.889    0.124   15.247    0.000    1.889    0.632 #>    .RS2     (.16.)    2.203    0.147   15.033    0.000    2.203    0.570 #>    .RS3     (.17.)    2.095    0.138   15.231    0.000    2.095    0.627 #>    .RS4     (.18.)    2.502    0.165   15.141    0.000    2.502    0.599 #>    .RS5     (.19.)    2.643    0.174   15.226    0.000    2.643    0.625 #>    .RS6     (.20.)    1.660    0.113   14.748    0.000    1.660    0.504 #>    .RS7     (.21.)    1.595    0.109   14.603    0.000    1.595    0.476 #>    .RS8     (.22.)    1.905    0.128   14.924    0.000    1.905    0.543 #>    .RS9               2.072    0.192   10.787    0.000    2.072    0.539 #>    .RS10    (.24.)    1.783    0.120   14.876    0.000    1.783    0.531 #>    .RS11    (.25.)    1.762    0.120   14.631    0.000    1.762    0.481 #>    .RS12    (.26.)    1.951    0.130   15.027    0.000    1.951    0.568 #>    .RS13    (.27.)    1.768    0.122   14.541    0.000    1.768    0.465 #>    .RS14    (.28.)    1.108    0.080   13.770    0.000    1.108    0.361 #>     RS                1.102    0.158    6.983    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.368 #>     RS2               0.430 #>     RS3               0.373 #>     RS4               0.401 #>     RS5               0.375 #>     RS6               0.496 #>     RS7               0.524 #>     RS8               0.457 #>     RS9               0.461 #>     RS10              0.469 #>     RS11              0.519 #>     RS12              0.432 #>     RS13              0.535 #>     RS14              0.639 #>  #>  #> Group 2 [Men]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.222    0.664 #>     RS2     (.p2.)    1.229    0.091   13.526    0.000    1.501    0.711 #>     RS3     (.p3.)    1.064    0.083   12.806    0.000    1.300    0.668 #>     RS4     (.p4.)    1.232    0.094   13.161    0.000    1.505    0.689 #>     RS5     (.p5.)    1.200    0.094   12.829    0.000    1.466    0.670 #>     RS6     (.p6.)    1.218    0.085   14.264    0.000    1.488    0.756 #>     RS7     (.p7.)    1.263    0.087   14.550    0.000    1.543    0.774 #>     RS8     (.p8.)    1.207    0.087   13.840    0.000    1.475    0.730 #>     RS9     (.p9.)    1.269    0.086   14.706    0.000    1.551    0.813 #>     RS10    (.10.)    1.194    0.086   13.965    0.000    1.459    0.738 #>     RS11    (.11.)    1.313    0.091   14.497    0.000    1.605    0.771 #>     RS12    (.12.)    1.160    0.086   13.546    0.000    1.418    0.712 #>     RS13    (.13.)    1.359    0.093   14.659    0.000    1.661    0.781 #>     RS14    (.14.)    1.334    0.086   15.592    0.000    1.630    0.840 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.30.)    4.874    0.089   54.484    0.000    4.874    2.650 #>    .RS2     (.31.)    4.249    0.104   40.863    0.000    4.249    2.012 #>    .RS3     (.32.)    4.190    0.095   44.218    0.000    4.190    2.153 #>    .RS4     (.33.)    4.482    0.107   41.924    0.000    4.482    2.053 #>    .RS5     (.34.)    3.838    0.107   35.990    0.000    3.838    1.753 #>    .RS6     (.35.)    5.068    0.098   51.642    0.000    5.068    2.575 #>    .RS7     (.36.)    4.969    0.100   49.744    0.000    4.969    2.492 #>    .RS8     (.37.)    4.381    0.100   43.816    0.000    4.381    2.169 #>    .RS9     (.38.)    4.290    0.101   42.586    0.000    4.290    2.247 #>    .RS10    (.39.)    4.652    0.098   47.405    0.000    4.652    2.351 #>    .RS11    (.40.)    4.519    0.104   43.349    0.000    4.519    2.170 #>    .RS12    (.41.)    5.035    0.098   51.351    0.000    5.035    2.530 #>    .RS13    (.42.)    4.928    0.107   46.152    0.000    4.928    2.317 #>    .RS14    (.43.)    4.783    0.099   48.258    0.000    4.783    2.465 #>     RS               -0.053    0.105   -0.503    0.615   -0.043   -0.043 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.15.)    1.889    0.124   15.247    0.000    1.889    0.559 #>    .RS2     (.16.)    2.203    0.147   15.033    0.000    2.203    0.494 #>    .RS3     (.17.)    2.095    0.138   15.231    0.000    2.095    0.553 #>    .RS4     (.18.)    2.502    0.165   15.141    0.000    2.502    0.525 #>    .RS5     (.19.)    2.643    0.174   15.226    0.000    2.643    0.551 #>    .RS6     (.20.)    1.660    0.113   14.748    0.000    1.660    0.428 #>    .RS7     (.21.)    1.595    0.109   14.603    0.000    1.595    0.401 #>    .RS8     (.22.)    1.905    0.128   14.924    0.000    1.905    0.467 #>    .RS9               1.238    0.125    9.864    0.000    1.238    0.340 #>    .RS10    (.24.)    1.783    0.120   14.876    0.000    1.783    0.456 #>    .RS11    (.25.)    1.762    0.120   14.631    0.000    1.762    0.406 #>    .RS12    (.26.)    1.951    0.130   15.027    0.000    1.951    0.493 #>    .RS13    (.27.)    1.768    0.122   14.541    0.000    1.768    0.391 #>    .RS14    (.28.)    1.108    0.080   13.770    0.000    1.108    0.294 #>     RS                1.493    0.216    6.911    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.441 #>     RS2               0.506 #>     RS3               0.447 #>     RS4               0.475 #>     RS5               0.449 #>     RS6               0.572 #>     RS7               0.599 #>     RS8               0.533 #>     RS9               0.660 #>     RS10              0.544 #>     RS11              0.594 #>     RS12              0.507 #>     RS13              0.609 #>     RS14              0.706"},{"path":"/articles/lecture_mgcfa.html","id":"add-the-fit-to-table-5","dir":"Articles","previous_headings":"","what":"Add the fit to table","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"table_fit[8, ] <- c(\"Strict Model RS9\", round(fitmeasures(strict.fit2,                                                   c(\"chisq\", \"df\", \"cfi\",                                                    \"rmsea\", \"srmr\")),3))  kable(table_fit)"},{"path":"/articles/lecture_mgcfa.html","id":"find-one-more-free-parameter","dir":"Articles","previous_headings":"","what":"Find one more free parameter","title":"Multi-Group Confirmatory Factor Analysis","text":"want see can get .902 - .01 = .892 CFI.","code":"for (i in 1:length(partial_syntax)){      temp <- cfa(model = overall.model,                data = res.data,               meanstructure = TRUE,               group = \"Sex\",               group.equal = c(\"loadings\", \"intercepts\", \"residuals\"),               group.partial = c(\"RS9 ~~ RS9\", partial_syntax[i]))      CFI_list[i] <- fitmeasures(temp, \"cfi\") }  CFI_list #>   RS1 ~~ RS1   RS2 ~~ RS2   RS3 ~~ RS3   RS4 ~~ RS4   RS5 ~~ RS5   RS6 ~~ RS6  #>    0.8907350    0.8923563    0.8911657    0.8907125    0.8906552    0.8905857  #>   RS7 ~~ RS7   RS8 ~~ RS8   RS9 ~~ RS9 RS10 ~~ RS10 RS11 ~~ RS11 RS12 ~~ RS12  #>    0.8904937    0.8930016    0.8906278    0.8914713    0.8919005    0.8927904  #> RS13 ~~ RS13 RS14 ~~ RS14  #>    0.8936296    0.8904971 which.max(CFI_list) #> RS13 ~~ RS13  #>           13"},{"path":"/articles/lecture_mgcfa.html","id":"add-one-more-free-parameter","dir":"Articles","previous_headings":"","what":"Add one more free parameter","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"strict.fit3 <- cfa(model = overall.model,                   data = res.data,                   meanstructure = TRUE,                   group = \"Sex\",                   group.equal = c(\"loadings\", \"intercepts\", \"residuals\"),                    group.partial = c(\"RS9 ~~ RS9\", \"RS13 ~~ RS13\")) summary(strict.fit3,         standardized = TRUE,          rsquare = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 44 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        85 #>   Number of equality constraints                    39 #>  #>   Number of observations per group:                    #>     Women                                          266 #>     Men                                            244 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               633.111 #>   Degrees of freedom                               192 #>   P-value (Chi-square)                           0.000 #>   Test statistic for each group: #>     Women                                      305.696 #>     Men                                        327.415 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4328.931 #>   Degrees of freedom                               182 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.894 #>   Tucker-Lewis Index (TLI)                       0.899 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -13033.933 #>   Loglikelihood unrestricted model (H1)     -12717.377 #>                                                        #>   Akaike (AIC)                               26159.866 #>   Bayesian (BIC)                             26354.649 #>   Sample-size adjusted Bayesian (SABIC)      26208.638 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.095 #>   90 Percent confidence interval - lower         0.087 #>   90 Percent confidence interval - upper         0.103 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    0.999 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.062 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #>  #> Group 1 [Women]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.050    0.607 #>     RS2     (.p2.)    1.230    0.091   13.554    0.000    1.291    0.657 #>     RS3     (.p3.)    1.061    0.083   12.794    0.000    1.114    0.609 #>     RS4     (.p4.)    1.227    0.093   13.133    0.000    1.287    0.630 #>     RS5     (.p5.)    1.201    0.093   12.853    0.000    1.260    0.613 #>     RS6     (.p6.)    1.217    0.085   14.280    0.000    1.278    0.704 #>     RS7     (.p7.)    1.262    0.087   14.571    0.000    1.325    0.724 #>     RS8     (.p8.)    1.208    0.087   13.872    0.000    1.268    0.677 #>     RS9     (.p9.)    1.269    0.086   14.727    0.000    1.332    0.680 #>     RS10    (.10.)    1.192    0.085   13.961    0.000    1.251    0.683 #>     RS11    (.11.)    1.310    0.090   14.490    0.000    1.375    0.719 #>     RS12    (.12.)    1.162    0.086   13.583    0.000    1.220    0.658 #>     RS13    (.13.)    1.372    0.092   14.962    0.000    1.440    0.698 #>     RS14    (.14.)    1.334    0.085   15.613    0.000    1.400    0.799 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.30.)    4.874    0.089   54.505    0.000    4.874    2.819 #>    .RS2     (.31.)    4.249    0.104   40.874    0.000    4.249    2.162 #>    .RS3     (.32.)    4.190    0.095   44.263    0.000    4.190    2.292 #>    .RS4     (.33.)    4.482    0.107   41.983    0.000    4.482    2.194 #>    .RS5     (.34.)    3.838    0.107   36.001    0.000    3.838    1.867 #>    .RS6     (.35.)    5.068    0.098   51.674    0.000    5.068    2.793 #>    .RS7     (.36.)    4.969    0.100   49.770    0.000    4.969    2.716 #>    .RS8     (.37.)    4.382    0.100   43.826    0.000    4.382    2.340 #>    .RS9     (.38.)    4.290    0.101   42.614    0.000    4.290    2.189 #>    .RS10    (.39.)    4.652    0.098   47.454    0.000    4.652    2.540 #>    .RS11    (.40.)    4.520    0.104   43.401    0.000    4.520    2.362 #>    .RS12    (.41.)    5.035    0.098   51.354    0.000    5.035    2.718 #>    .RS13    (.42.)    4.915    0.107   45.827    0.000    4.915    2.382 #>    .RS14    (.43.)    4.783    0.099   48.290    0.000    4.783    2.732 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.15.)    1.888    0.124   15.250    0.000    1.888    0.632 #>    .RS2     (.16.)    2.198    0.146   15.035    0.000    2.198    0.569 #>    .RS3     (.17.)    2.101    0.138   15.242    0.000    2.101    0.629 #>    .RS4     (.18.)    2.516    0.166   15.158    0.000    2.516    0.603 #>    .RS5     (.19.)    2.639    0.173   15.228    0.000    2.639    0.624 #>    .RS6     (.20.)    1.659    0.112   14.755    0.000    1.659    0.504 #>    .RS7     (.21.)    1.593    0.109   14.609    0.000    1.593    0.476 #>    .RS8     (.22.)    1.899    0.127   14.925    0.000    1.899    0.541 #>    .RS9               2.066    0.192   10.773    0.000    2.066    0.538 #>    .RS10    (.24.)    1.789    0.120   14.891    0.000    1.789    0.534 #>    .RS11    (.25.)    1.771    0.121   14.652    0.000    1.771    0.484 #>    .RS12    (.26.)    1.944    0.129   15.025    0.000    1.944    0.566 #>    .RS13              2.184    0.204   10.693    0.000    2.184    0.513 #>    .RS14    (.28.)    1.106    0.080   13.781    0.000    1.106    0.361 #>     RS                1.101    0.158    6.989    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.368 #>     RS2               0.431 #>     RS3               0.371 #>     RS4               0.397 #>     RS5               0.376 #>     RS6               0.496 #>     RS7               0.524 #>     RS8               0.459 #>     RS9               0.462 #>     RS10              0.466 #>     RS11              0.516 #>     RS12              0.434 #>     RS13              0.487 #>     RS14              0.639 #>  #>  #> Group 2 [Men]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   RS =~                                                                  #>     RS1               1.000                               1.224    0.665 #>     RS2     (.p2.)    1.230    0.091   13.554    0.000    1.505    0.712 #>     RS3     (.p3.)    1.061    0.083   12.794    0.000    1.299    0.667 #>     RS4     (.p4.)    1.227    0.093   13.133    0.000    1.501    0.687 #>     RS5     (.p5.)    1.201    0.093   12.853    0.000    1.469    0.671 #>     RS6     (.p6.)    1.217    0.085   14.280    0.000    1.490    0.756 #>     RS7     (.p7.)    1.262    0.087   14.571    0.000    1.545    0.774 #>     RS8     (.p8.)    1.208    0.087   13.872    0.000    1.478    0.731 #>     RS9     (.p9.)    1.269    0.086   14.727    0.000    1.553    0.813 #>     RS10    (.10.)    1.192    0.085   13.961    0.000    1.458    0.737 #>     RS11    (.11.)    1.310    0.090   14.490    0.000    1.603    0.769 #>     RS12    (.12.)    1.162    0.086   13.583    0.000    1.422    0.714 #>     RS13    (.13.)    1.372    0.092   14.962    0.000    1.679    0.826 #>     RS14    (.14.)    1.334    0.085   15.613    0.000    1.632    0.841 #>  #> Intercepts: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.30.)    4.874    0.089   54.505    0.000    4.874    2.649 #>    .RS2     (.31.)    4.249    0.104   40.874    0.000    4.249    2.012 #>    .RS3     (.32.)    4.190    0.095   44.263    0.000    4.190    2.153 #>    .RS4     (.33.)    4.482    0.107   41.983    0.000    4.482    2.052 #>    .RS5     (.34.)    3.838    0.107   36.001    0.000    3.838    1.753 #>    .RS6     (.35.)    5.068    0.098   51.674    0.000    5.068    2.574 #>    .RS7     (.36.)    4.969    0.100   49.770    0.000    4.969    2.491 #>    .RS8     (.37.)    4.382    0.100   43.826    0.000    4.382    2.168 #>    .RS9     (.38.)    4.290    0.101   42.614    0.000    4.290    2.246 #>    .RS10    (.39.)    4.652    0.098   47.454    0.000    4.652    2.351 #>    .RS11    (.40.)    4.520    0.104   43.401    0.000    4.520    2.169 #>    .RS12    (.41.)    5.035    0.098   51.354    0.000    5.035    2.528 #>    .RS13    (.42.)    4.915    0.107   45.827    0.000    4.915    2.419 #>    .RS14    (.43.)    4.783    0.099   48.290    0.000    4.783    2.463 #>     RS               -0.053    0.105   -0.507    0.612   -0.043   -0.043 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .RS1     (.15.)    1.888    0.124   15.250    0.000    1.888    0.558 #>    .RS2     (.16.)    2.198    0.146   15.035    0.000    2.198    0.493 #>    .RS3     (.17.)    2.101    0.138   15.242    0.000    2.101    0.555 #>    .RS4     (.18.)    2.516    0.166   15.158    0.000    2.516    0.528 #>    .RS5     (.19.)    2.639    0.173   15.228    0.000    2.639    0.550 #>    .RS6     (.20.)    1.659    0.112   14.755    0.000    1.659    0.428 #>    .RS7     (.21.)    1.593    0.109   14.609    0.000    1.593    0.400 #>    .RS8     (.22.)    1.899    0.127   14.925    0.000    1.899    0.465 #>    .RS9               1.239    0.125    9.900    0.000    1.239    0.339 #>    .RS10    (.24.)    1.789    0.120   14.891    0.000    1.789    0.457 #>    .RS11    (.25.)    1.771    0.121   14.652    0.000    1.771    0.408 #>    .RS12    (.26.)    1.944    0.129   15.025    0.000    1.944    0.490 #>    .RS13              1.310    0.134    9.779    0.000    1.310    0.317 #>    .RS14    (.28.)    1.106    0.080   13.781    0.000    1.106    0.294 #>     RS                1.497    0.216    6.917    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     RS1               0.442 #>     RS2               0.507 #>     RS3               0.445 #>     RS4               0.472 #>     RS5               0.450 #>     RS6               0.572 #>     RS7               0.600 #>     RS8               0.535 #>     RS9               0.661 #>     RS10              0.543 #>     RS11              0.592 #>     RS12              0.510 #>     RS13              0.683 #>     RS14              0.706"},{"path":"/articles/lecture_mgcfa.html","id":"partial-invariance-met","dir":"Articles","previous_headings":"","what":"Partial Invariance Met!","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"table_fit[9, ] <- c(\"Strict Model RS9 + 13\", round(fitmeasures(strict.fit3,                                                   c(\"chisq\", \"df\", \"cfi\",                                                    \"rmsea\", \"srmr\")),3))  kable(table_fit)"},{"path":"/articles/lecture_mgcfa.html","id":"interpretation","dir":"Articles","previous_headings":"","what":"Interpretation","title":"Multi-Group Confirmatory Factor Analysis","text":"RS mostly invariant – structure, loadings, intercepts, error variances across men women. RS9 RS13 keep interested things. life meaning.","code":""},{"path":"/articles/lecture_mgcfa.html","id":"lavpredict-on-the-items","dir":"Articles","previous_headings":"","what":"lavPredict on the items","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"predicted_scores <- lavPredict(strict.fit3, type = \"ov\")  table(res.data$Sex) #>  #>   Men Women  #>   244   266  predicted_scores <- as.data.frame(do.call(rbind, predicted_scores)) predicted_scores$Sex <- c(rep(\"Women\", 266), rep(\"Men\", 244))  predicted_scores$sum <- apply(predicted_scores[ , 1:14], 1, sum) head(predicted_scores) #>        RS1      RS2      RS3      RS4      RS5      RS6      RS7      RS8 #> 1 4.398945 3.664178 3.685068 3.898936 3.267464 4.489343 4.369067 3.807156 #> 2 5.812096 5.401971 5.184889 5.632255 4.964156 6.209687 6.153074 5.514666 #> 3 6.517211 6.269071 5.933248 6.497123 5.810748 7.068081 7.043233 6.366655 #> 4 5.458156 4.966720 4.809240 5.198124 4.539199 5.778806 5.706248 5.086999 #> 5 4.051653 3.237102 3.316476 3.472960 2.850489 4.066555 3.930633 3.387523 #> 6 4.625009 3.942175 3.924997 4.176218 3.538887 4.764549 4.654457 4.080309 #>        RS9     RS10     RS11     RS12     RS13     RS14   Sex      sum #> 1 3.686699 4.085209 3.896680 4.482787 4.262883 4.148736 Women 56.14315 #> 2 5.479791 5.769280 5.747900 6.125213 6.201545 6.033519 Women 80.23004 #> 3 6.374483 6.609575 6.671596 6.944727 7.168871 6.973961 Women 92.24858 #> 4 5.030689 5.347484 5.284240 5.713847 5.715984 5.561452 Women 74.19719 #> 5 3.246033 3.671336 3.441728 4.079148 3.786442 3.685536 Women 50.22361 #> 6 3.973543 4.354613 4.192822 4.745528 4.573014 4.450247 Women 59.99637  tapply(predicted_scores$sum, predicted_scores$Sex, mean) #>      Men    Women  #> 63.34281 64.24733"},{"path":"/articles/lecture_mgcfa.html","id":"lavpredict-on-the-factor-scores","dir":"Articles","previous_headings":"","what":"lavPredict on the factor scores","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"res.data$sum <- apply(res.data[ , 3:16], 1, sum)  tapply(res.data$sum, res.data$Sex, mean) #>      Men    Women  #> 63.47131 64.10902  latent_means <- lavPredict(strict.fit3)  latent_means <- as.data.frame(do.call(rbind, latent_means)) latent_means$Sex <- c(rep(\"Women\", 266), rep(\"Men\", 244))  options(scipen = 999) tapply(latent_means$RS, latent_means$Sex, mean)  #>              Men            Women  #> -0.0530675184545 -0.0000004953854  tapply(latent_means$RS, latent_means$Sex, mean) * #latent mean   tapply(res.data$sum, res.data$Sex, sd, na.rm = T) + #real sum   tapply(res.data$sum, res.data$Sex, mean, na.rm = T) #real sd #>      Men    Women  #> 62.32949 64.10901"},{"path":"/articles/lecture_mgcfa.html","id":"calculate-your-favorite-stat","dir":"Articles","previous_headings":"","what":"Calculate your favorite stat","title":"Multi-Group Confirmatory Factor Analysis","text":"","code":"library(MOTE) M <- tapply(predicted_scores$sum, predicted_scores$Sex, mean) SD <- tapply(predicted_scores$sum, predicted_scores$Sex, sd) N <- tapply(predicted_scores$sum, predicted_scores$Sex, length)  effect_size <- d.ind.t(M[1], M[2], SD[1], SD[2], N[1], N[2], a = .05) effect_size$estimate #> [1] \"$d_s$ = -0.05, 95\\\\% CI [-0.22, 0.13]\" effect_size$statistic #> [1] \"$t$(508) = -0.54, $p$ = .587\""},{"path":"/articles/lecture_mgcfa.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Multi-Group Confirmatory Factor Analysis","text":"lecture ’ve learned: concepts terminology multigroup model implement multigroup model assess partial invariance assess latent means potential differences","code":""},{"path":"/articles/lecture_mtmm.html","id":"multi-trait-multi-method-sem","dir":"Articles","previous_headings":"","what":"Multi-Trait Multi-Method SEM","title":"Multi-Trait Multi-Method","text":"Another good resource includes: http://davidakenny.net/cm/mtmm.htm Traits: latent factors trying measure. Methods: way measuring latent factors.","code":""},{"path":"/articles/lecture_mtmm.html","id":"when-to-use","dir":"Articles","previous_headings":"","what":"When to Use","title":"Multi-Trait Multi-Method","text":"like measurement 1 measurement 2 assess trait equally well. like measurement 1 measurement 2 assess trait differently. together indicate measurement equally useful.","code":""},{"path":"/articles/lecture_mtmm.html","id":"when-to-use-1","dir":"Articles","previous_headings":"","what":"When to Use","title":"Multi-Trait Multi-Method","text":"want measurement 1 measure trait 1 trait 2 differently.","code":""},{"path":"/articles/lecture_mtmm.html","id":"logic-of-mtmm","dir":"Articles","previous_headings":"","what":"Logic of MTMM","title":"Multi-Trait Multi-Method","text":"also use AIC ECVI, generally chi-square test preferred. Another popular model comparison rule Δ\\Delta CFI > .01 considered significant change fit.","code":""},{"path":"/articles/lecture_mtmm.html","id":"the-steps","dir":"Articles","previous_headings":"","what":"The Steps","title":"Multi-Trait Multi-Method","text":"Widaman (1985) approach Correlated uniqueness approach","code":""},{"path":"/articles/lecture_mtmm.html","id":"the-steps-1","dir":"Articles","previous_headings":"","what":"The Steps","title":"Multi-Trait Multi-Method","text":"Let’s note model specification slightly different normal CFA. use normal scaling manifest variables. instead set variance latent one using std.lv = TRUE.","code":""},{"path":"/articles/lecture_mtmm.html","id":"model-1-correlated-traits-correlated-methods","dir":"Articles","previous_headings":"","what":"Model 1: Correlated Traits / Correlated Methods","title":"Multi-Trait Multi-Method","text":"correlate traits . correlate methods . cross correlate methods traits. latent ~~ 0*latent","code":""},{"path":[]},{"path":"/articles/lecture_mtmm.html","id":"model-1-correlated-traits-correlated-methods-2","dir":"Articles","previous_headings":"","what":"Model 1: Correlated Traits / Correlated Methods","title":"Multi-Trait Multi-Method","text":"One big concerns/complaints traditional MTMM steps Model 1 likely create Heywood case negative error variance. Generally, models complex … latent variables correlated purpose, may hard estimate.","code":""},{"path":"/articles/lecture_mtmm.html","id":"model-1-correlated-traits-correlated-methods-3","dir":"Articles","previous_headings":"","what":"Model 1: Correlated Traits / Correlated Methods","title":"Multi-Trait Multi-Method","text":"know variance real data, set . Set value something small positive. Set value equal another small positive parameter.","code":""},{"path":"/articles/lecture_mtmm.html","id":"model-1-correlated-traits-correlated-methods-4","dir":"Articles","previous_headings":"","what":"Model 1: Correlated Traits / Correlated Methods","title":"Multi-Trait Multi-Method","text":"models compared Model 1. Model 1 represents best case scenario, wherein traits correlated perfectly estimated measurements perfectly.","code":""},{"path":"/articles/lecture_mtmm.html","id":"example-data","dir":"Articles","previous_headings":"","what":"Example Data","title":"Multi-Trait Multi-Method","text":"Meaning: MLQ 1, 2, 5, 10 PIL 4, 12, 17 Purpose: MLQ 3, 4, 6, 8, 9, PIL 3, 8, 20 Purpose Life Test (p questions) Meaning Life Questionnaire (m questions)","code":"library(lavaan) library(semPlot) library(rio)  meaning.data <- import(\"data/lecture_mtmm.csv\") str(meaning.data) #> 'data.frame':    567 obs. of  70 variables: #>  $ p1 : int  5 1 6 5 5 4 5 6 6 6 ... #>  $ p2 : num  5 3 6 5 6 5 6 6 6 5 ... #>  $ p3 : int  7 5 7 5 5 6 6 7 5 7 ... #>  $ p4 : int  7 5 6 6 7 7 7 7 5 5 ... #>  $ p5 : int  4 2 7 3 6 4 6 5 4 6 ... #>  $ p6 : int  5 5 7 5 6 4 5 6 3 6 ... #>  $ p7 : num  7 5 7 6 5 6 7 7 5 7 ... #>  $ p8 : int  5 5 6 5 5 5 5 6 5 7 ... #>  $ p9 : int  7 3 6 5 7 5 6 7 4 6 ... #>  $ p10: int  6 3 7 6 7 5 6 6 3 4 ... #>  $ p11: int  7 4 6 6 7 7 6 7 4 6 ... #>  $ p12: int  4 5 7 5 1 5 6 5 4 5 ... #>  $ p13: num  7 5 6 6 6 5 7 4 6 7 ... #>  $ p14: num  4 7 6 5 6 5 7 7 7 1 ... #>  $ p15: num  7 5 6 5 7 3 6 3 2 5 ... #>  $ p16: int  7 7 7 5 7 7 6 7 2 6 ... #>  $ p17: num  5 5 5 6 7 5 6 7 5 5 ... #>  $ p18: int  1 7 7 6 7 5 1 1 6 5 ... #>  $ p19: int  5 3 6 5 6 4 5 6 4 6 ... #>  $ p20: int  6 5 7 5 5 6 6 7 4 5 ... #>  $ l1 : num  1 0 1 1 1 1 1 1 1 1 ... #>  $ l2 : int  1 0 1 1 1 1 1 0 1 1 ... #>  $ l3 : int  1 1 1 1 1 1 1 1 1 1 ... #>  $ l4 : num  1 0 1 1 1 1 1 1 1 1 ... #>  $ l5 : int  0 0 1 0 1 0 1 0 1 1 ... #>  $ l6 : int  1 0 1 1 1 1 0 1 0 1 ... #>  $ l7 : num  1 1 1 1 1 1 1 1 1 1 ... #>  $ l8 : int  1 1 1 1 0 0 0 1 1 1 ... #>  $ l9 : int  1 0 1 1 1 1 1 1 1 1 ... #>  $ l10: num  1 0 1 0.77 1 1 0 1 0 0 ... #>  $ l11: int  1 0 1 1 1 1 1 1 0 1 ... #>  $ l12: num  1 1 1 1 1 1 1 1 1 1 ... #>  $ l13: int  1 1 1 1 1 1 1 1 1 1 ... #>  $ l14: int  0 1 1 1 1 1 1 1 1 0 ... #>  $ l15: num  1 0 1 0 1 0 0 0 0 0 ... #>  $ l16: int  1 1 1 1 1 1 1 1 0 1 ... #>  $ l17: num  1 0 1 1 1 1 1 1 1 1 ... #>  $ l18: int  0 1 1 0 1 0 1 1 1 1 ... #>  $ l19: num  0 0 1 1 1 0 1 0 1 1 ... #>  $ l20: int  1 0 1 1 1 1 1 1 0 1 ... #>  $ m1 : int  6 4 6 6 7 5 7 6 5 5 ... #>  $ m2 : int  5 7 4 5 3 7 4 5 6 2 ... #>  $ m3 : int  2 7 3 6 2 6 5 5 6 5 ... #>  $ m4 : int  4 3 5 6 7 5 4 6 4 5 ... #>  $ m5 : int  6 4 6 6 7 6 6 7 4 6 ... #>  $ m6 : int  4 1 6 5 7 6 3 7 3 5 ... #>  $ m7 : int  2 6 2 5 3 4 4 5 6 2 ... #>  $ m8 : int  4 3 3 5 2 6 6 4 6 5 ... #>  $ m9 : int  7 6 7 6 7 6 7 7 5 3 ... #>  $ m10: int  4 3 1 6 2 4 7 1 5 3 ... #>  $ s1 : num  2 3 5 3 5 4 4 7 4 1 ... #>  $ s2 : num  2 3 5 3 5 4 4 6 4 1 ... #>  $ s3 : num  1 5 2 3 3 2 2 5 5 2 ... #>  $ s4 : num  1 3 2 2 2 4 3 2 5 2 ... #>  $ s5 : num  2 5 3 3 1 4 6 4 5 5 ... #>  $ s6 : num  3 7 6 4 6 6 7 7 7 6 ... #>  $ s7 : int  5 7 7 4 4 6 7 5 6 6 ... #>  $ s8 : num  1 7 2 3 1 2 2 6 7 6 ... #>  $ s9 : num  1 7 2 3 2 2 2 1 2 2 ... #>  $ s10: num  2 7 4 4 5 2 6 5 5 5 ... #>  $ s11: num  1 2 2 3 2 2 2 1 4 3 ... #>  $ s12: num  1 7 2 3 2 2 3 3 2 4 ... #>  $ s13: int  1 7 2 4 3 2 2 2 4 3 ... #>  $ s14: num  3 2 2 2 3 4 2 5 5 2 ... #>  $ s15: num  1 2 2 2 5 4 2 6 6 2 ... #>  $ s16: int  2 3 2 2 3 5 4 2 5 2 ... #>  $ s17: num  1 5 1 4 2 5 6 4 6 2 ... #>  $ s18: num  1 2 4 4 6 6 4 7 4 3 ... #>  $ s19: num  2 7 4 3 2 2 4 6 5 4 ... #>  $ s20: num  1 2 5 2 5 4 5 6 4 3 ..."},{"path":"/articles/lecture_mtmm.html","id":"build-the-measurement-models","dir":"Articles","previous_headings":"","what":"Build the Measurement Models","title":"Multi-Trait Multi-Method","text":"","code":"methods.model <- ' mlq =~ m1 + m2 + m3 + m4 + m5 + m6 + m8 + m9 + m10 pil =~ p3 + p4 + p8 + p12 + p17 + p20 '  traits.model <- ' meaning =~ m1 + m2 + m5 + m10 + p4 + p12 + p17 purpose =~ m3 + m4 + m6 + m8 + m9 + p3 + p8 + p20 '"},{"path":"/articles/lecture_mtmm.html","id":"analyze-the-measurement-models","dir":"Articles","previous_headings":"","what":"Analyze the Measurement Models","title":"Multi-Trait Multi-Method","text":"","code":"methods.fit <- cfa(model = methods.model,                     data = meaning.data,                    std.lv = TRUE) traits.fit <- cfa(model = traits.model,                   data = meaning.data,                   std.lv = TRUE) #> Warning: lavaan->lav_object_post_check():   #>    covariance matrix of latent variables is not positive definite ; use  #>    lavInspect(fit, \"cov.lv\") to investigate.  lavInspect(traits.fit, \"cor.lv\") #>         meanng purpos #> meaning  1.000        #> purpose -1.011  1.000"},{"path":"/articles/lecture_mtmm.html","id":"summarize-the-measurement-models","dir":"Articles","previous_headings":"","what":"Summarize the Measurement Models","title":"Multi-Trait Multi-Method","text":"","code":"summary(methods.fit,          rsquare = TRUE,          standardized = TRUE,         fit.measures = TRUE) #> lavaan 0.6-19 ended normally after 23 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        31 #>  #>   Number of observations                           567 #>  #> Model Test User Model: #>                                                        #>   Test statistic                              1126.227 #>   Degrees of freedom                                89 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4415.161 #>   Degrees of freedom                               105 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.759 #>   Tucker-Lewis Index (TLI)                       0.716 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -13021.841 #>   Loglikelihood unrestricted model (H1)     -12458.727 #>                                                        #>   Akaike (AIC)                               26105.681 #>   Bayesian (BIC)                             26240.232 #>   Sample-size adjusted Bayesian (SABIC)      26141.822 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.143 #>   90 Percent confidence interval - lower         0.136 #>   90 Percent confidence interval - upper         0.151 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.121 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   mlq =~                                                                 #>     m1                1.084    0.048   22.360    0.000    1.084    0.802 #>     m2               -0.578    0.076   -7.656    0.000   -0.578   -0.328 #>     m3               -0.352    0.075   -4.681    0.000   -0.352   -0.204 #>     m4                1.121    0.049   23.095    0.000    1.121    0.819 #>     m5                0.977    0.043   22.480    0.000    0.977    0.805 #>     m6                1.149    0.052   22.247    0.000    1.149    0.799 #>     m8               -0.454    0.073   -6.212    0.000   -0.454   -0.269 #>     m9                0.980    0.057   17.238    0.000    0.980    0.665 #>     m10              -0.906    0.078  -11.552    0.000   -0.906   -0.477 #>   pil =~                                                                 #>     p3                0.776    0.041   19.140    0.000    0.776    0.725 #>     p4                0.799    0.040   20.033    0.000    0.799    0.749 #>     p8                0.626    0.040   15.793    0.000    0.626    0.626 #>     p12               0.890    0.060   14.819    0.000    0.890    0.595 #>     p17               0.663    0.046   14.371    0.000    0.663    0.580 #>     p20               0.923    0.038   24.430    0.000    0.923    0.858 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   mlq ~~                                                                 #>     pil               0.768    0.023   33.098    0.000    0.768    0.768 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .m1                0.653    0.048   13.696    0.000    0.653    0.357 #>    .m2                2.779    0.167   16.631    0.000    2.779    0.893 #>    .m3                2.851    0.170   16.763    0.000    2.851    0.958 #>    .m4                0.616    0.046   13.273    0.000    0.616    0.329 #>    .m5                0.519    0.038   13.631    0.000    0.519    0.352 #>    .m6                0.747    0.054   13.756    0.000    0.747    0.361 #>    .m8                2.656    0.159   16.704    0.000    2.656    0.928 #>    .m9                1.212    0.078   15.468    0.000    1.212    0.558 #>    .m10               2.779    0.170   16.331    0.000    2.779    0.772 #>    .p3                0.543    0.038   14.444    0.000    0.543    0.474 #>    .p4                0.498    0.035   14.060    0.000    0.498    0.438 #>    .p8                0.608    0.039   15.460    0.000    0.608    0.608 #>    .p12               1.444    0.092   15.670    0.000    1.444    0.646 #>    .p17               0.867    0.055   15.758    0.000    0.867    0.663 #>    .p20               0.306    0.028   10.814    0.000    0.306    0.264 #>     mlq               1.000                               1.000    1.000 #>     pil               1.000                               1.000    1.000 #>  #> R-Square: #>                    Estimate #>     m1                0.643 #>     m2                0.107 #>     m3                0.042 #>     m4                0.671 #>     m5                0.648 #>     m6                0.639 #>     m8                0.072 #>     m9                0.442 #>     m10               0.228 #>     p3                0.526 #>     p4                0.562 #>     p8                0.392 #>     p12               0.354 #>     p17               0.337 #>     p20               0.736  summary(traits.fit,          rsquare = TRUE,          standardized = TRUE,         fit.measures = TRUE) #> lavaan 0.6-19 ended normally after 26 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        31 #>  #>   Number of observations                           567 #>  #> Model Test User Model: #>                                                        #>   Test statistic                              1427.750 #>   Degrees of freedom                                89 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4415.161 #>   Degrees of freedom                               105 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.689 #>   Tucker-Lewis Index (TLI)                       0.634 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -13172.602 #>   Loglikelihood unrestricted model (H1)     -12458.727 #>                                                        #>   Akaike (AIC)                               26407.205 #>   Bayesian (BIC)                             26541.756 #>   Sample-size adjusted Bayesian (SABIC)      26443.345 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.163 #>   90 Percent confidence interval - lower         0.155 #>   90 Percent confidence interval - upper         0.170 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.127 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   meaning =~                                                             #>     m1                1.041    0.049   21.190    0.000    1.041    0.770 #>     m2               -0.572    0.075   -7.637    0.000   -0.572   -0.324 #>     m5                0.949    0.044   21.661    0.000    0.949    0.782 #>     m10              -0.872    0.078  -11.166    0.000   -0.872   -0.460 #>     p4                0.741    0.040   18.398    0.000    0.741    0.695 #>     p12               0.887    0.059   15.036    0.000    0.887    0.593 #>     p17               0.644    0.046   14.125    0.000    0.644    0.563 #>   purpose =~                                                             #>     m3                0.354    0.075    4.756    0.000    0.354    0.205 #>     m4               -1.076    0.049  -21.856    0.000   -1.076   -0.786 #>     m6               -1.106    0.052  -21.158    0.000   -1.106   -0.769 #>     m8                0.437    0.073    6.017    0.000    0.437    0.258 #>     m9               -0.950    0.057  -16.676    0.000   -0.950   -0.644 #>     p3               -0.623    0.042  -14.694    0.000   -0.623   -0.582 #>     p8               -0.536    0.040  -13.301    0.000   -0.536   -0.535 #>     p20              -0.800    0.040  -20.170    0.000   -0.800   -0.743 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   meaning ~~                                                             #>     purpose          -1.011    0.011  -95.053    0.000   -1.011   -1.011 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .m1                0.744    0.051   14.635    0.000    0.744    0.407 #>    .m2                2.786    0.167   16.678    0.000    2.786    0.895 #>    .m5                0.573    0.040   14.436    0.000    0.573    0.389 #>    .m10               2.839    0.172   16.468    0.000    2.839    0.789 #>    .p4                0.588    0.038   15.483    0.000    0.588    0.517 #>    .p12               1.449    0.090   16.074    0.000    1.449    0.648 #>    .p17               0.893    0.055   16.188    0.000    0.893    0.683 #>    .m3                2.850    0.170   16.777    0.000    2.850    0.958 #>    .m4                0.716    0.050   14.430    0.000    0.716    0.382 #>    .m6                0.846    0.058   14.708    0.000    0.846    0.409 #>    .m8                2.672    0.160   16.739    0.000    2.672    0.933 #>    .m9                1.270    0.080   15.828    0.000    1.270    0.585 #>    .p3                0.758    0.047   16.116    0.000    0.758    0.661 #>    .p8                0.714    0.044   16.274    0.000    0.714    0.713 #>    .p20               0.518    0.034   15.038    0.000    0.518    0.448 #>     meaning           1.000                               1.000    1.000 #>     purpose           1.000                               1.000    1.000 #>  #> R-Square: #>                    Estimate #>     m1                0.593 #>     m2                0.105 #>     m5                0.611 #>     m10               0.211 #>     p4                0.483 #>     p12               0.352 #>     p17               0.317 #>     m3                0.042 #>     m4                0.618 #>     m6                0.591 #>     m8                0.067 #>     m9                0.415 #>     p3                0.339 #>     p8                0.287 #>     p20               0.552"},{"path":"/articles/lecture_mtmm.html","id":"diagram-the-measurement-models","dir":"Articles","previous_headings":"","what":"Diagram the Measurement Models","title":"Multi-Trait Multi-Method","text":"","code":"semPaths(methods.fit,           whatLabels = \"std\",           layout = \"tree\",           edge.label.cex = 1) semPaths(traits.fit,           whatLabels = \"std\",           layout = \"tree\",           edge.label.cex = 1)"},{"path":"/articles/lecture_mtmm.html","id":"model-1-correlated-traits-correlated-methods-5","dir":"Articles","previous_headings":"","what":"Model 1: Correlated Traits / Correlated Methods","title":"Multi-Trait Multi-Method","text":"","code":"step1.model <- ' mlq =~ m1 + m2 + m3 + m4 + m5 + m6 + m8 + m9 + m10 pil =~ p3 + p4 + p8 + p12 + p17 + p20 meaning =~ m1 + m2 + m5 + m10 + p4 + p12 + p17 purpose =~ m3 + m4 + m6 + m8 + m9 + p3 + p8 + p20  ##fix the covariances mlq ~~ 0*meaning pil ~~ 0*meaning mlq ~~ 0*purpose pil ~~ 0*purpose '"},{"path":"/articles/lecture_mtmm.html","id":"model-1-correlated-traits-correlated-methods-6","dir":"Articles","previous_headings":"","what":"Model 1: Correlated Traits / Correlated Methods","title":"Multi-Trait Multi-Method","text":"","code":"step1.fit <- cfa(model = step1.model,                   data = meaning.data,                  std.lv = TRUE)  summary(step1.fit,          rsquare = TRUE,          standardized = TRUE,         fit.measures = TRUE) #> lavaan 0.6-19 ended normally after 61 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        47 #>  #>   Number of observations                           567 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               144.899 #>   Degrees of freedom                                73 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4415.161 #>   Degrees of freedom                               105 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.983 #>   Tucker-Lewis Index (TLI)                       0.976 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -12531.177 #>   Loglikelihood unrestricted model (H1)     -12458.727 #>                                                        #>   Akaike (AIC)                               25156.354 #>   Bayesian (BIC)                             25360.351 #>   Sample-size adjusted Bayesian (SABIC)      25211.148 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.042 #>   90 Percent confidence interval - lower         0.032 #>   90 Percent confidence interval - upper         0.052 #>   P-value H_0: RMSEA <= 0.050                    0.915 #>   P-value H_0: RMSEA >= 0.080                    0.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.024 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   mlq =~                                                                 #>     m1                0.072    0.079    0.912    0.362    0.072    0.053 #>     m2               -1.276    0.071  -17.912    0.000   -1.276   -0.723 #>     m3               -1.364    0.066  -20.783    0.000   -1.364   -0.791 #>     m4               -0.011    0.082   -0.133    0.894   -0.011   -0.008 #>     m5                0.044    0.071    0.624    0.533    0.044    0.036 #>     m6                0.097    0.084    1.146    0.252    0.097    0.067 #>     m8               -1.332    0.065  -20.402    0.000   -1.332   -0.787 #>     m9                0.066    0.081    0.813    0.416    0.066    0.045 #>     m10              -1.302    0.082  -15.838    0.000   -1.302   -0.686 #>   pil =~                                                                 #>     p3                0.708    0.043   16.321    0.000    0.708    0.660 #>     p4                0.441    0.041   10.812    0.000    0.441    0.413 #>     p8                0.463    0.042   10.942    0.000    0.463    0.463 #>     p12               0.344    0.063    5.456    0.000    0.344    0.230 #>     p17               0.255    0.049    5.236    0.000    0.255    0.223 #>     p20               0.628    0.038   16.314    0.000    0.628    0.583 #>   meaning =~                                                             #>     m1               -1.090    0.049  -22.359    0.000   -1.090   -0.806 #>     m2                0.414    0.111    3.728    0.000    0.414    0.234 #>     m5               -0.987    0.044  -22.682    0.000   -0.987   -0.813 #>     m10               0.745    0.115    6.503    0.000    0.745    0.392 #>     p4               -0.658    0.042  -15.527    0.000   -0.658   -0.617 #>     p12              -0.824    0.061  -13.569    0.000   -0.824   -0.551 #>     p17              -0.609    0.047  -13.032    0.000   -0.609   -0.532 #>   purpose =~                                                             #>     m3               -0.157    0.115   -1.369    0.171   -0.157   -0.091 #>     m4                1.149    0.048   23.832    0.000    1.149    0.839 #>     m6                1.147    0.052   21.965    0.000    1.147    0.798 #>     m8               -0.273    0.112   -2.435    0.015   -0.273   -0.161 #>     m9                0.980    0.057   17.147    0.000    0.980    0.665 #>     p3                0.486    0.045   10.692    0.000    0.486    0.453 #>     p8                0.441    0.042   10.443    0.000    0.441    0.441 #>     p20               0.693    0.043   16.270    0.000    0.693    0.643 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   mlq ~~                                                                 #>     meaning           0.000                               0.000    0.000 #>   pil ~~                                                                 #>     meaning           0.000                               0.000    0.000 #>   mlq ~~                                                                 #>     purpose           0.000                               0.000    0.000 #>   pil ~~                                                                 #>     purpose           0.000                               0.000    0.000 #>   mlq ~~                                                                 #>     pil               0.147    0.083    1.763    0.078    0.147    0.147 #>   meaning ~~                                                             #>     purpose          -0.987    0.011  -92.538    0.000   -0.987   -0.987 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .m1                0.635    0.048   13.361    0.000    0.635    0.347 #>    .m2                1.316    0.100   13.204    0.000    1.316    0.423 #>    .m3                1.089    0.094   11.580    0.000    1.089    0.366 #>    .m4                0.553    0.045   12.200    0.000    0.553    0.295 #>    .m5                0.498    0.038   13.148    0.000    0.498    0.338 #>    .m6                0.743    0.055   13.632    0.000    0.743    0.359 #>    .m8                1.015    0.088   11.561    0.000    1.015    0.355 #>    .m9                1.208    0.078   15.433    0.000    1.208    0.556 #>    .m10               1.352    0.104   13.050    0.000    1.352    0.375 #>    .p3                0.411    0.041    9.997    0.000    0.411    0.358 #>    .p4                0.511    0.035   14.556    0.000    0.511    0.449 #>    .p8                0.592    0.039   15.029    0.000    0.592    0.591 #>    .p12               1.440    0.090   16.075    0.000    1.440    0.644 #>    .p17               0.872    0.054   16.150    0.000    0.872    0.667 #>    .p20               0.287    0.030    9.490    0.000    0.287    0.247 #>     mlq               1.000                               1.000    1.000 #>     pil               1.000                               1.000    1.000 #>     meaning           1.000                               1.000    1.000 #>     purpose           1.000                               1.000    1.000 #>  #> R-Square: #>                    Estimate #>     m1                0.653 #>     m2                0.577 #>     m3                0.634 #>     m4                0.705 #>     m5                0.662 #>     m6                0.641 #>     m8                0.645 #>     m9                0.444 #>     m10               0.625 #>     p3                0.642 #>     p4                0.551 #>     p8                0.409 #>     p12               0.356 #>     p17               0.333 #>     p20               0.753"},{"path":"/articles/lecture_mtmm.html","id":"model-1-correlated-traits-correlated-methods-7","dir":"Articles","previous_headings":"","what":"Model 1: Correlated Traits / Correlated Methods","title":"Multi-Trait Multi-Method","text":"","code":"semPaths(step1.fit,           whatLabels = \"std\",           layout = \"tree\",           edge.label.cex = 1)"},{"path":"/articles/lecture_mtmm.html","id":"model-2-no-traits-correlated-methods","dir":"Articles","previous_headings":"","what":"Model 2: No Traits / Correlated Methods","title":"Multi-Trait Multi-Method","text":"Completely delete trait latent side. testing methods model. Convergent validity: Independent measures trait correlated want Model 2 worse Model 1. Model 1 traits methods, Model 2 methods. Model 2 good, means methods best estimate data traits useful (’s bad).","code":""},{"path":[]},{"path":"/articles/lecture_mtmm.html","id":"model-2-no-traits-correlated-methods-2","dir":"Articles","previous_headings":"","what":"Model 2: No Traits / Correlated Methods","title":"Multi-Trait Multi-Method","text":"Model 2 worse Model 1, implying need traits interpret data, supporting convergent validity.","code":"##model 2 is the methods model  ##we've already checked it out anova(step1.fit, methods.fit) #>  #> Chi-Squared Difference Test #>  #>             Df   AIC   BIC  Chisq Chisq diff  RMSEA Df diff Pr(>Chisq)     #> step1.fit   73 25156 25360  144.9                                          #> methods.fit 89 26106 26240 1126.2     981.33 0.3262      16  < 2.2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  fitmeasures(step1.fit, \"cfi\")  #>   cfi  #> 0.983 fitmeasures(methods.fit, \"cfi\") #>   cfi  #> 0.759"},{"path":"/articles/lecture_mtmm.html","id":"model-3-perfectly-correlated-traits-freely-correlated-methods","dir":"Articles","previous_headings":"","what":"Model 3: Perfectly Correlated Traits / Freely Correlated Methods","title":"Multi-Trait Multi-Method","text":"model, include traits methods. set covariances traits one. setting one, testing traits exactly , indicate measuring separate variables. Discriminant validity: want traits measure different things. Therefore, want Model 1 better Model 3.","code":""},{"path":[]},{"path":"/articles/lecture_mtmm.html","id":"model-3-perfectly-correlated-traits-freely-correlated-methods-2","dir":"Articles","previous_headings":"","what":"Model 3: Perfectly Correlated Traits / Freely Correlated Methods","title":"Multi-Trait Multi-Method","text":"","code":"step3.model <- ' mlq =~ m1 + m2 + m3 + m4 + m5 + m6 + m8 + m9 + m10 pil =~ p3 + p4 + p8 + p12 + p17 + p20 meaning =~ m1 + m2 + m5 + m10 + p4 + p12 + p17 purpose =~ m3 + m4 + m6 + m8 + m9 + p3 + p8 + p20  ##fix the covariances mlq ~~ 0*meaning pil ~~ 0*meaning mlq ~~ 0*purpose pil ~~ 0*purpose meaning ~~ 1*purpose '"},{"path":"/articles/lecture_mtmm.html","id":"model-3-perfectly-correlated-traits-freely-correlated-methods-3","dir":"Articles","previous_headings":"","what":"Model 3: Perfectly Correlated Traits / Freely Correlated Methods","title":"Multi-Trait Multi-Method","text":"","code":"step3.fit <- cfa(model = step3.model,                  data = meaning.data,                  std.lv = TRUE)  summary(step3.fit,          rsquare = TRUE,          standardized = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 47 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        46 #>  #>   Number of observations                           567 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               146.376 #>   Degrees of freedom                                74 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4415.161 #>   Degrees of freedom                               105 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.983 #>   Tucker-Lewis Index (TLI)                       0.976 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -12531.915 #>   Loglikelihood unrestricted model (H1)     -12458.727 #>                                                        #>   Akaike (AIC)                               25155.830 #>   Bayesian (BIC)                             25355.487 #>   Sample-size adjusted Bayesian (SABIC)      25209.458 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.042 #>   90 Percent confidence interval - lower         0.032 #>   90 Percent confidence interval - upper         0.051 #>   P-value H_0: RMSEA <= 0.050                    0.920 #>   P-value H_0: RMSEA >= 0.080                    0.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.024 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   mlq =~                                                                 #>     m1                0.072    0.078    0.922    0.357    0.072    0.053 #>     m2               -1.276    0.071  -17.952    0.000   -1.276   -0.723 #>     m3               -1.364    0.066  -20.794    0.000   -1.364   -0.791 #>     m4               -0.011    0.080   -0.134    0.894   -0.011   -0.008 #>     m5                0.045    0.070    0.635    0.525    0.045    0.037 #>     m6                0.096    0.083    1.164    0.245    0.096    0.067 #>     m8               -1.331    0.065  -20.426    0.000   -1.331   -0.787 #>     m9                0.065    0.080    0.818    0.413    0.065    0.044 #>     m10              -1.302    0.082  -15.883    0.000   -1.302   -0.686 #>   pil =~                                                                 #>     p3                0.710    0.043   16.354    0.000    0.710    0.663 #>     p4                0.436    0.041   10.710    0.000    0.436    0.409 #>     p8                0.464    0.042   10.963    0.000    0.464    0.464 #>     p12               0.339    0.063    5.380    0.000    0.339    0.227 #>     p17               0.251    0.049    5.152    0.000    0.251    0.220 #>     p20               0.627    0.039   16.280    0.000    0.627    0.583 #>   meaning =~                                                             #>     m1                1.087    0.049   22.366    0.000    1.087    0.804 #>     m2               -0.409    0.110   -3.716    0.000   -0.409   -0.232 #>     m5                0.984    0.043   22.665    0.000    0.984    0.810 #>     m10              -0.743    0.114   -6.543    0.000   -0.743   -0.391 #>     p4                0.654    0.042   15.439    0.000    0.654    0.613 #>     p12               0.819    0.061   13.511    0.000    0.819    0.548 #>     p17               0.606    0.047   12.981    0.000    0.606    0.530 #>   purpose =~                                                             #>     m3               -0.155    0.113   -1.364    0.173   -0.155   -0.090 #>     m4                1.145    0.048   23.825    0.000    1.145    0.836 #>     m6                1.142    0.052   21.918    0.000    1.142    0.794 #>     m8               -0.270    0.111   -2.440    0.015   -0.270   -0.160 #>     m9                0.978    0.057   17.147    0.000    0.978    0.664 #>     p3                0.482    0.045   10.631    0.000    0.482    0.450 #>     p8                0.440    0.042   10.424    0.000    0.440    0.440 #>     p20               0.689    0.043   16.198    0.000    0.689    0.640 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   mlq ~~                                                                 #>     meaning           0.000                               0.000    0.000 #>   pil ~~                                                                 #>     meaning           0.000                               0.000    0.000 #>   mlq ~~                                                                 #>     purpose           0.000                               0.000    0.000 #>   pil ~~                                                                 #>     purpose           0.000                               0.000    0.000 #>   meaning ~~                                                             #>     purpose           1.000                               1.000    1.000 #>   mlq ~~                                                                 #>     pil               0.146    0.082    1.785    0.074    0.146    0.146 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .m1                0.641    0.047   13.742    0.000    0.641    0.351 #>    .m2                1.319    0.100   13.211    0.000    1.319    0.424 #>    .m3                1.090    0.094   11.582    0.000    1.090    0.366 #>    .m4                0.563    0.044   12.730    0.000    0.563    0.301 #>    .m5                0.505    0.037   13.580    0.000    0.505    0.342 #>    .m6                0.754    0.054   13.959    0.000    0.754    0.365 #>    .m8                1.017    0.088   11.584    0.000    1.017    0.355 #>    .m9                1.212    0.078   15.528    0.000    1.212    0.558 #>    .m10               1.353    0.103   13.078    0.000    1.353    0.376 #>    .p3                0.409    0.041    9.882    0.000    0.409    0.357 #>    .p4                0.520    0.035   14.875    0.000    0.520    0.457 #>    .p8                0.592    0.039   15.020    0.000    0.592    0.592 #>    .p12               1.449    0.090   16.172    0.000    1.449    0.648 #>    .p17               0.877    0.054   16.236    0.000    0.877    0.671 #>    .p20               0.290    0.030    9.587    0.000    0.290    0.251 #>     mlq               1.000                               1.000    1.000 #>     pil               1.000                               1.000    1.000 #>     meaning           1.000                               1.000    1.000 #>     purpose           1.000                               1.000    1.000 #>  #> R-Square: #>                    Estimate #>     m1                0.649 #>     m2                0.576 #>     m3                0.634 #>     m4                0.699 #>     m5                0.658 #>     m6                0.635 #>     m8                0.645 #>     m9                0.442 #>     m10               0.624 #>     p3                0.643 #>     p4                0.543 #>     p8                0.408 #>     p12               0.352 #>     p17               0.329 #>     p20               0.749"},{"path":"/articles/lecture_mtmm.html","id":"model-3-perfectly-correlated-traits-freely-correlated-methods-4","dir":"Articles","previous_headings":"","what":"Model 3: Perfectly Correlated Traits / Freely Correlated Methods","title":"Multi-Trait Multi-Method","text":"","code":"semPaths(step3.fit,           whatLabels = \"std\",           layout = \"tree\",           edge.label.cex = 1)"},{"path":"/articles/lecture_mtmm.html","id":"model-3-perfectly-correlated-traits-freely-correlated-methods-5","dir":"Articles","previous_headings":"","what":"Model 3: Perfectly Correlated Traits / Freely Correlated Methods","title":"Multi-Trait Multi-Method","text":"Oh ! models exactly . result implies traits discriminating (meaning = purpose).","code":"anova(step1.fit, step3.fit) #>  #> Chi-Squared Difference Test #>  #>           Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(>Chisq) #> step1.fit 73 25156 25360 144.90                                        #> step3.fit 74 25156 25356 146.38     1.4763 0.028983       1     0.2244  fitmeasures(step1.fit, \"cfi\") #>   cfi  #> 0.983 fitmeasures(step3.fit, \"cfi\") #>   cfi  #> 0.983"},{"path":"/articles/lecture_mtmm.html","id":"model-4-freely-correlated-traits-uncorrelated-methods","dir":"Articles","previous_headings":"","what":"Model 4: Freely Correlated Traits / Uncorrelated Methods","title":"Multi-Trait Multi-Method","text":"last step, set covariances methods zero. result imply methods measuring exactly way. Discriminant validity methods: different measures assess different parts latent variables want find Model 1 Model 4 equal.","code":""},{"path":[]},{"path":"/articles/lecture_mtmm.html","id":"model-4-freely-correlated-traits-uncorrelated-methods-2","dir":"Articles","previous_headings":"","what":"Model 4: Freely Correlated Traits / Uncorrelated Methods","title":"Multi-Trait Multi-Method","text":"","code":"step4.model <- ' mlq =~ m1 + m2 + m3 + m4 + m5 + m6 + m8 + m9 + m10 pil =~ p3 + p4 + p8 + p12 + p17 + p20 meaning =~ m1 + m2 + m5 + m10 + p4 + p12 + p17 purpose =~ m3 + m4 + m6 + m8 + m9 + p3 + p8 + p20  ##fix the covariances mlq ~~ 0*meaning pil ~~ 0*meaning mlq ~~ 0*purpose pil ~~ 0*purpose pil ~~ 0*mlq '"},{"path":"/articles/lecture_mtmm.html","id":"model-4-freely-correlated-traits-uncorrelated-methods-3","dir":"Articles","previous_headings":"","what":"Model 4: Freely Correlated Traits / Uncorrelated Methods","title":"Multi-Trait Multi-Method","text":"","code":"step4.fit <- cfa(model = step4.model,                    data = meaning.data,                   std.lv = TRUE)  summary(step4.fit,          rsquare = TRUE,          standardized = TRUE,          fit.measure = TRUE) #> lavaan 0.6-19 ended normally after 38 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        46 #>  #>   Number of observations                           567 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               147.707 #>   Degrees of freedom                                74 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              4415.161 #>   Degrees of freedom                               105 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.983 #>   Tucker-Lewis Index (TLI)                       0.976 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -12532.581 #>   Loglikelihood unrestricted model (H1)     -12458.727 #>                                                        #>   Akaike (AIC)                               25157.162 #>   Bayesian (BIC)                             25356.818 #>   Sample-size adjusted Bayesian (SABIC)      25210.790 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.042 #>   90 Percent confidence interval - lower         0.032 #>   90 Percent confidence interval - upper         0.052 #>   P-value H_0: RMSEA <= 0.050                    0.910 #>   P-value H_0: RMSEA >= 0.080                    0.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.025 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   mlq =~                                                                 #>     m1                0.028    0.056    0.494    0.621    0.028    0.020 #>     m2                1.235    0.069   18.000    0.000    1.235    0.700 #>     m3                1.346    0.066   20.543    0.000    1.346    0.780 #>     m4                0.117    0.056    2.067    0.039    0.117    0.085 #>     m5                0.045    0.050    0.907    0.365    0.045    0.037 #>     m6                0.010    0.060    0.162    0.871    0.010    0.007 #>     m8                1.301    0.064   20.254    0.000    1.301    0.769 #>     m9                0.025    0.063    0.396    0.692    0.025    0.017 #>     m10               1.226    0.074   16.589    0.000    1.226    0.646 #>   pil =~                                                                 #>     p3                0.703    0.043   16.242    0.000    0.703    0.656 #>     p4                0.435    0.040   10.835    0.000    0.435    0.408 #>     p8                0.461    0.042   10.945    0.000    0.461    0.461 #>     p12               0.332    0.062    5.326    0.000    0.332    0.222 #>     p17               0.261    0.049    5.384    0.000    0.261    0.228 #>     p20               0.617    0.038   16.364    0.000    0.617    0.573 #>   meaning =~                                                             #>     m1                1.092    0.048   22.555    0.000    1.092    0.808 #>     m2               -0.528    0.088   -6.002    0.000   -0.528   -0.299 #>     m5                0.987    0.043   22.747    0.000    0.987    0.813 #>     m10              -0.861    0.091   -9.510    0.000   -0.861   -0.453 #>     p4                0.662    0.042   15.744    0.000    0.662    0.620 #>     p12               0.828    0.060   13.708    0.000    0.828    0.554 #>     p17               0.602    0.047   12.897    0.000    0.602    0.526 #>   purpose =~                                                             #>     m3                0.282    0.090    3.134    0.002    0.282    0.163 #>     m4               -1.143    0.048  -23.610    0.000   -1.143   -0.835 #>     m6               -1.151    0.052  -22.272    0.000   -1.151   -0.800 #>     m8                0.394    0.087    4.509    0.000    0.394    0.233 #>     m9               -0.982    0.057  -17.277    0.000   -0.982   -0.666 #>     p3               -0.493    0.045  -11.050    0.000   -0.493   -0.460 #>     p8               -0.445    0.042  -10.624    0.000   -0.445   -0.444 #>     p20              -0.701    0.042  -16.729    0.000   -0.701   -0.650 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   mlq ~~                                                                 #>     meaning           0.000                               0.000    0.000 #>   pil ~~                                                                 #>     meaning           0.000                               0.000    0.000 #>   mlq ~~                                                                 #>     purpose           0.000                               0.000    0.000 #>   pil ~~                                                                 #>     purpose           0.000                               0.000    0.000 #>   mlq ~~                                                                 #>     pil               0.000                               0.000    0.000 #>   meaning ~~                                                             #>     purpose          -0.988    0.010  -95.659    0.000   -0.988   -0.988 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .m1                0.635    0.047   13.375    0.000    0.635    0.347 #>    .m2                1.311    0.100   13.172    0.000    1.311    0.421 #>    .m3                1.085    0.094   11.539    0.000    1.085    0.365 #>    .m4                0.554    0.045   12.225    0.000    0.554    0.295 #>    .m5                0.498    0.038   13.172    0.000    0.498    0.338 #>    .m6                0.743    0.054   13.632    0.000    0.743    0.359 #>    .m8                1.014    0.088   11.541    0.000    1.014    0.354 #>    .m9                1.207    0.078   15.432    0.000    1.207    0.556 #>    .m10               1.359    0.104   13.072    0.000    1.359    0.377 #>    .p3                0.410    0.041    9.904    0.000    0.410    0.357 #>    .p4                0.512    0.035   14.562    0.000    0.512    0.449 #>    .p8                0.591    0.039   14.988    0.000    0.591    0.590 #>    .p12               1.440    0.089   16.097    0.000    1.440    0.644 #>    .p17               0.878    0.054   16.167    0.000    0.878    0.671 #>    .p20               0.289    0.030    9.567    0.000    0.289    0.249 #>     mlq               1.000                               1.000    1.000 #>     pil               1.000                               1.000    1.000 #>     meaning           1.000                               1.000    1.000 #>     purpose           1.000                               1.000    1.000 #>  #> R-Square: #>                    Estimate #>     m1                0.653 #>     m2                0.579 #>     m3                0.635 #>     m4                0.705 #>     m5                0.662 #>     m6                0.641 #>     m8                0.646 #>     m9                0.444 #>     m10               0.623 #>     p3                0.643 #>     p4                0.551 #>     p8                0.410 #>     p12               0.356 #>     p17               0.329 #>     p20               0.751"},{"path":"/articles/lecture_mtmm.html","id":"model-4-freely-correlated-traits-uncorrelated-methods-4","dir":"Articles","previous_headings":"","what":"Model 4: Freely Correlated Traits / Uncorrelated Methods","title":"Multi-Trait Multi-Method","text":"","code":"semPaths(step4.fit,           whatLabels = \"std\",           layout = \"tree\",           edge.label.cex = 1)"},{"path":"/articles/lecture_mtmm.html","id":"model-4-freely-correlated-traits-uncorrelated-methods-5","dir":"Articles","previous_headings":"","what":"Model 4: Freely Correlated Traits / Uncorrelated Methods","title":"Multi-Trait Multi-Method","text":"two models equal, good thing, indicating two scales assess measures differently.","code":"anova(step1.fit, step4.fit) #>  #> Chi-Squared Difference Test #>  #>           Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(>Chisq)   #> step1.fit 73 25156 25360 144.90                                          #> step4.fit 74 25157 25357 147.71      2.808 0.056469       1     0.0938 . #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1  fitmeasures(step1.fit, \"cfi\") #>   cfi  #> 0.983 fitmeasures(step4.fit, \"cfi\") #>   cfi  #> 0.983"},{"path":"/articles/lecture_mtmm.html","id":"correlated-uniqueness","dir":"Articles","previous_headings":"","what":"Correlated Uniqueness","title":"Multi-Trait Multi-Method","text":"Another type analysis examine convergent discriminant validity using correlated uniqueness approach. create traits side model, first traits.model created. Instead using latent variables predict observed items measurement side, simply correlate error terms observed items scale. model, interpret parameters (can also Model 1 analysis).","code":""},{"path":"/articles/lecture_mtmm.html","id":"interpreting-parameters","dir":"Articles","previous_headings":"","what":"Interpreting Parameters","title":"Multi-Trait Multi-Method","text":"Convergent validity: trait loadings higher methods loadings. finding tell trait effect greater method effects.","code":"parameterestimates(step1.fit, standardized = T) #>        lhs op     rhs    est    se       z pvalue ci.lower ci.upper std.lv #> 1      mlq =~      m1  0.072 0.079   0.912  0.362   -0.082    0.226  0.072 #> 2      mlq =~      m2 -1.276 0.071 -17.912  0.000   -1.415   -1.136 -1.276 #> 3      mlq =~      m3 -1.364 0.066 -20.783  0.000   -1.493   -1.236 -1.364 #> 4      mlq =~      m4 -0.011 0.082  -0.133  0.894   -0.171    0.150 -0.011 #> 5      mlq =~      m5  0.044 0.071   0.624  0.533   -0.095    0.183  0.044 #> 6      mlq =~      m6  0.097 0.084   1.146  0.252   -0.069    0.262  0.097 #> 7      mlq =~      m8 -1.332 0.065 -20.402  0.000   -1.460   -1.204 -1.332 #> 8      mlq =~      m9  0.066 0.081   0.813  0.416   -0.093    0.225  0.066 #> 9      mlq =~     m10 -1.302 0.082 -15.838  0.000   -1.463   -1.141 -1.302 #> 10     pil =~      p3  0.708 0.043  16.321  0.000    0.623    0.793  0.708 #> 11     pil =~      p4  0.441 0.041  10.812  0.000    0.361    0.521  0.441 #> 12     pil =~      p8  0.463 0.042  10.942  0.000    0.380    0.546  0.463 #> 13     pil =~     p12  0.344 0.063   5.456  0.000    0.220    0.467  0.344 #> 14     pil =~     p17  0.255 0.049   5.236  0.000    0.160    0.351  0.255 #> 15     pil =~     p20  0.628 0.038  16.314  0.000    0.552    0.703  0.628 #> 16 meaning =~      m1 -1.090 0.049 -22.359  0.000   -1.186   -0.995 -1.090 #> 17 meaning =~      m2  0.414 0.111   3.728  0.000    0.196    0.631  0.414 #> 18 meaning =~      m5 -0.987 0.044 -22.682  0.000   -1.072   -0.902 -0.987 #> 19 meaning =~     m10  0.745 0.115   6.503  0.000    0.520    0.969  0.745 #> 20 meaning =~      p4 -0.658 0.042 -15.527  0.000   -0.741   -0.575 -0.658 #> 21 meaning =~     p12 -0.824 0.061 -13.569  0.000   -0.943   -0.705 -0.824 #> 22 meaning =~     p17 -0.609 0.047 -13.032  0.000   -0.701   -0.517 -0.609 #> 23 purpose =~      m3 -0.157 0.115  -1.369  0.171   -0.382    0.068 -0.157 #> 24 purpose =~      m4  1.149 0.048  23.832  0.000    1.055    1.244  1.149 #> 25 purpose =~      m6  1.147 0.052  21.965  0.000    1.045    1.249  1.147 #> 26 purpose =~      m8 -0.273 0.112  -2.435  0.015   -0.492   -0.053 -0.273 #> 27 purpose =~      m9  0.980 0.057  17.147  0.000    0.868    1.092  0.980 #> 28 purpose =~      p3  0.486 0.045  10.692  0.000    0.397    0.575  0.486 #> 29 purpose =~      p8  0.441 0.042  10.443  0.000    0.359    0.524  0.441 #> 30 purpose =~     p20  0.693 0.043  16.270  0.000    0.609    0.776  0.693 #> 31     mlq ~~ meaning  0.000 0.000      NA     NA    0.000    0.000  0.000 #> 32     pil ~~ meaning  0.000 0.000      NA     NA    0.000    0.000  0.000 #> 33     mlq ~~ purpose  0.000 0.000      NA     NA    0.000    0.000  0.000 #> 34     pil ~~ purpose  0.000 0.000      NA     NA    0.000    0.000  0.000 #> 35      m1 ~~      m1  0.635 0.048  13.361  0.000    0.542    0.728  0.635 #> 36      m2 ~~      m2  1.316 0.100  13.204  0.000    1.121    1.512  1.316 #> 37      m3 ~~      m3  1.089 0.094  11.580  0.000    0.905    1.274  1.089 #> 38      m4 ~~      m4  0.553 0.045  12.200  0.000    0.464    0.642  0.553 #> 39      m5 ~~      m5  0.498 0.038  13.148  0.000    0.424    0.572  0.498 #> 40      m6 ~~      m6  0.743 0.055  13.632  0.000    0.636    0.850  0.743 #> 41      m8 ~~      m8  1.015 0.088  11.561  0.000    0.843    1.187  1.015 #> 42      m9 ~~      m9  1.208 0.078  15.433  0.000    1.054    1.361  1.208 #> 43     m10 ~~     m10  1.352 0.104  13.050  0.000    1.149    1.556  1.352 #> 44      p3 ~~      p3  0.411 0.041   9.997  0.000    0.331    0.492  0.411 #> 45      p4 ~~      p4  0.511 0.035  14.556  0.000    0.443    0.580  0.511 #> 46      p8 ~~      p8  0.592 0.039  15.029  0.000    0.515    0.670  0.592 #> 47     p12 ~~     p12  1.440 0.090  16.075  0.000    1.264    1.615  1.440 #> 48     p17 ~~     p17  0.872 0.054  16.150  0.000    0.766    0.978  0.872 #> 49     p20 ~~     p20  0.287 0.030   9.490  0.000    0.228    0.346  0.287 #> 50     mlq ~~     mlq  1.000 0.000      NA     NA    1.000    1.000  1.000 #> 51     pil ~~     pil  1.000 0.000      NA     NA    1.000    1.000  1.000 #> 52 meaning ~~ meaning  1.000 0.000      NA     NA    1.000    1.000  1.000 #> 53 purpose ~~ purpose  1.000 0.000      NA     NA    1.000    1.000  1.000 #> 54     mlq ~~     pil  0.147 0.083   1.763  0.078   -0.016    0.310  0.147 #> 55 meaning ~~ purpose -0.987 0.011 -92.538  0.000   -1.008   -0.967 -0.987 #>    std.all #> 1    0.053 #> 2   -0.723 #> 3   -0.791 #> 4   -0.008 #> 5    0.036 #> 6    0.067 #> 7   -0.787 #> 8    0.045 #> 9   -0.686 #> 10   0.660 #> 11   0.413 #> 12   0.463 #> 13   0.230 #> 14   0.223 #> 15   0.583 #> 16  -0.806 #> 17   0.234 #> 18  -0.813 #> 19   0.392 #> 20  -0.617 #> 21  -0.551 #> 22  -0.532 #> 23  -0.091 #> 24   0.839 #> 25   0.798 #> 26  -0.161 #> 27   0.665 #> 28   0.453 #> 29   0.441 #> 30   0.643 #> 31   0.000 #> 32   0.000 #> 33   0.000 #> 34   0.000 #> 35   0.347 #> 36   0.423 #> 37   0.366 #> 38   0.295 #> 39   0.338 #> 40   0.359 #> 41   0.355 #> 42   0.556 #> 43   0.375 #> 44   0.358 #> 45   0.449 #> 46   0.591 #> 47   0.644 #> 48   0.667 #> 49   0.247 #> 50   1.000 #> 51   1.000 #> 52   1.000 #> 53   1.000 #> 54   0.147 #> 55  -0.987"},{"path":"/articles/lecture_mtmm.html","id":"interpreting-parameters-1","dir":"Articles","previous_headings":"","what":"Interpreting Parameters","title":"Multi-Trait Multi-Method","text":"Discriminant validity: want traits correlations low, implies traits different things. Discriminant validity: want methods correlations low, implies methods exactly .","code":"parameterestimates(step1.fit, standardized = T) #>        lhs op     rhs    est    se       z pvalue ci.lower ci.upper std.lv #> 1      mlq =~      m1  0.072 0.079   0.912  0.362   -0.082    0.226  0.072 #> 2      mlq =~      m2 -1.276 0.071 -17.912  0.000   -1.415   -1.136 -1.276 #> 3      mlq =~      m3 -1.364 0.066 -20.783  0.000   -1.493   -1.236 -1.364 #> 4      mlq =~      m4 -0.011 0.082  -0.133  0.894   -0.171    0.150 -0.011 #> 5      mlq =~      m5  0.044 0.071   0.624  0.533   -0.095    0.183  0.044 #> 6      mlq =~      m6  0.097 0.084   1.146  0.252   -0.069    0.262  0.097 #> 7      mlq =~      m8 -1.332 0.065 -20.402  0.000   -1.460   -1.204 -1.332 #> 8      mlq =~      m9  0.066 0.081   0.813  0.416   -0.093    0.225  0.066 #> 9      mlq =~     m10 -1.302 0.082 -15.838  0.000   -1.463   -1.141 -1.302 #> 10     pil =~      p3  0.708 0.043  16.321  0.000    0.623    0.793  0.708 #> 11     pil =~      p4  0.441 0.041  10.812  0.000    0.361    0.521  0.441 #> 12     pil =~      p8  0.463 0.042  10.942  0.000    0.380    0.546  0.463 #> 13     pil =~     p12  0.344 0.063   5.456  0.000    0.220    0.467  0.344 #> 14     pil =~     p17  0.255 0.049   5.236  0.000    0.160    0.351  0.255 #> 15     pil =~     p20  0.628 0.038  16.314  0.000    0.552    0.703  0.628 #> 16 meaning =~      m1 -1.090 0.049 -22.359  0.000   -1.186   -0.995 -1.090 #> 17 meaning =~      m2  0.414 0.111   3.728  0.000    0.196    0.631  0.414 #> 18 meaning =~      m5 -0.987 0.044 -22.682  0.000   -1.072   -0.902 -0.987 #> 19 meaning =~     m10  0.745 0.115   6.503  0.000    0.520    0.969  0.745 #> 20 meaning =~      p4 -0.658 0.042 -15.527  0.000   -0.741   -0.575 -0.658 #> 21 meaning =~     p12 -0.824 0.061 -13.569  0.000   -0.943   -0.705 -0.824 #> 22 meaning =~     p17 -0.609 0.047 -13.032  0.000   -0.701   -0.517 -0.609 #> 23 purpose =~      m3 -0.157 0.115  -1.369  0.171   -0.382    0.068 -0.157 #> 24 purpose =~      m4  1.149 0.048  23.832  0.000    1.055    1.244  1.149 #> 25 purpose =~      m6  1.147 0.052  21.965  0.000    1.045    1.249  1.147 #> 26 purpose =~      m8 -0.273 0.112  -2.435  0.015   -0.492   -0.053 -0.273 #> 27 purpose =~      m9  0.980 0.057  17.147  0.000    0.868    1.092  0.980 #> 28 purpose =~      p3  0.486 0.045  10.692  0.000    0.397    0.575  0.486 #> 29 purpose =~      p8  0.441 0.042  10.443  0.000    0.359    0.524  0.441 #> 30 purpose =~     p20  0.693 0.043  16.270  0.000    0.609    0.776  0.693 #> 31     mlq ~~ meaning  0.000 0.000      NA     NA    0.000    0.000  0.000 #> 32     pil ~~ meaning  0.000 0.000      NA     NA    0.000    0.000  0.000 #> 33     mlq ~~ purpose  0.000 0.000      NA     NA    0.000    0.000  0.000 #> 34     pil ~~ purpose  0.000 0.000      NA     NA    0.000    0.000  0.000 #> 35      m1 ~~      m1  0.635 0.048  13.361  0.000    0.542    0.728  0.635 #> 36      m2 ~~      m2  1.316 0.100  13.204  0.000    1.121    1.512  1.316 #> 37      m3 ~~      m3  1.089 0.094  11.580  0.000    0.905    1.274  1.089 #> 38      m4 ~~      m4  0.553 0.045  12.200  0.000    0.464    0.642  0.553 #> 39      m5 ~~      m5  0.498 0.038  13.148  0.000    0.424    0.572  0.498 #> 40      m6 ~~      m6  0.743 0.055  13.632  0.000    0.636    0.850  0.743 #> 41      m8 ~~      m8  1.015 0.088  11.561  0.000    0.843    1.187  1.015 #> 42      m9 ~~      m9  1.208 0.078  15.433  0.000    1.054    1.361  1.208 #> 43     m10 ~~     m10  1.352 0.104  13.050  0.000    1.149    1.556  1.352 #> 44      p3 ~~      p3  0.411 0.041   9.997  0.000    0.331    0.492  0.411 #> 45      p4 ~~      p4  0.511 0.035  14.556  0.000    0.443    0.580  0.511 #> 46      p8 ~~      p8  0.592 0.039  15.029  0.000    0.515    0.670  0.592 #> 47     p12 ~~     p12  1.440 0.090  16.075  0.000    1.264    1.615  1.440 #> 48     p17 ~~     p17  0.872 0.054  16.150  0.000    0.766    0.978  0.872 #> 49     p20 ~~     p20  0.287 0.030   9.490  0.000    0.228    0.346  0.287 #> 50     mlq ~~     mlq  1.000 0.000      NA     NA    1.000    1.000  1.000 #> 51     pil ~~     pil  1.000 0.000      NA     NA    1.000    1.000  1.000 #> 52 meaning ~~ meaning  1.000 0.000      NA     NA    1.000    1.000  1.000 #> 53 purpose ~~ purpose  1.000 0.000      NA     NA    1.000    1.000  1.000 #> 54     mlq ~~     pil  0.147 0.083   1.763  0.078   -0.016    0.310  0.147 #> 55 meaning ~~ purpose -0.987 0.011 -92.538  0.000   -1.008   -0.967 -0.987 #>    std.all #> 1    0.053 #> 2   -0.723 #> 3   -0.791 #> 4   -0.008 #> 5    0.036 #> 6    0.067 #> 7   -0.787 #> 8    0.045 #> 9   -0.686 #> 10   0.660 #> 11   0.413 #> 12   0.463 #> 13   0.230 #> 14   0.223 #> 15   0.583 #> 16  -0.806 #> 17   0.234 #> 18  -0.813 #> 19   0.392 #> 20  -0.617 #> 21  -0.551 #> 22  -0.532 #> 23  -0.091 #> 24   0.839 #> 25   0.798 #> 26  -0.161 #> 27   0.665 #> 28   0.453 #> 29   0.441 #> 30   0.643 #> 31   0.000 #> 32   0.000 #> 33   0.000 #> 34   0.000 #> 35   0.347 #> 36   0.423 #> 37   0.366 #> 38   0.295 #> 39   0.338 #> 40   0.359 #> 41   0.355 #> 42   0.556 #> 43   0.375 #> 44   0.358 #> 45   0.449 #> 46   0.591 #> 47   0.644 #> 48   0.667 #> 49   0.247 #> 50   1.000 #> 51   1.000 #> 52   1.000 #> 53   1.000 #> 54   0.147 #> 55  -0.987"},{"path":"/articles/lecture_mtmm.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Multi-Trait Multi-Method","text":"lecture ’ve learned: examine confirmatory discriminant validity using multi-trait multi-method analysis. examine parameter estimates confirmation model findings. compare models. add cross-loadings set zero one.","code":""},{"path":"/articles/lecture_path.html","id":"estimation","dir":"Articles","previous_headings":"","what":"Estimation","title":"Path Analysis","text":"Estimation math goes behind scenes give parameter numbers Maximum Likelihood (ML) Asymptotically Distribution Free (ADF) Unweighted Least Squares (ULS) Two stage least squares (TSLS)","code":""},{"path":"/articles/lecture_path.html","id":"maximum-likelihood","dir":"Articles","previous_headings":"","what":"Maximum Likelihood","title":"Path Analysis","text":"Estimates ones maximize likelihood data drawn population Check normality assumption! types estimations may work better non-normal endogenous variables Partial information methods calculate part estimates, use calculate rest","code":""},{"path":"/articles/lecture_path.html","id":"maximum-likelihood-1","dir":"Articles","previous_headings":"","what":"Maximum Likelihood","title":"Path Analysis","text":"Fit function - relationship sample covariances estimated covariances High measuring much match (goodness fit) Low measuring much mismatch (residuals)","code":""},{"path":"/articles/lecture_path.html","id":"maximum-likelihood-2","dir":"Articles","previous_headings":"","what":"Maximum Likelihood","title":"Path Analysis","text":"ML iterative process computer calculates possible start solution runs several times create largest ML match Usually generated computer can enter values problems converging solution","code":""},{"path":"/articles/lecture_path.html","id":"maximum-likelihood-3","dir":"Articles","previous_headings":"","what":"Maximum Likelihood","title":"Path Analysis","text":"Means change scale linear transform, model still Otherwise standardizing standardized estimates Loadings/path coefficients - just like regression coefficients Covariance/correlation - much two items vary together Error variances tell much variance accounted model (want small) SMCs amount variance accounted endogenous variable","code":""},{"path":"/articles/lecture_path.html","id":"other-methods","dir":"Articles","previous_headings":"","what":"Other Methods","title":"Path Analysis","text":"Generalized Least Squares (GLS) Unweighted Least Squares (ULS) Fully Weighted Least Squares (WLS)","code":""},{"path":"/articles/lecture_path.html","id":"generalized-least-squares","dir":"Articles","previous_headings":"","what":"Generalized Least Squares","title":"Path Analysis","text":"Scale free Faster computation time commonly used? runs usually ML","code":""},{"path":"/articles/lecture_path.html","id":"unweighted-least-squares","dir":"Articles","previous_headings":"","what":"Unweighted Least Squares","title":"Path Analysis","text":"require positive definite matrices Robust initial estimates scale free efficient variables scale","code":""},{"path":"/articles/lecture_path.html","id":"other-methods-1","dir":"Articles","previous_headings":"","what":"Other Methods","title":"Path Analysis","text":"ML, estimates might accurate standard errors may large Model fit tends overestimated Satorra-Bentler statistic: Adjusts chi square value standard ML degree kurtosis/skew Corrected model test statistic","code":""},{"path":"/articles/lecture_path.html","id":"asymptotically-distribution-free","dir":"Articles","previous_headings":"","what":"Asymptotically Distribution Free","title":"Path Analysis","text":"books call arbitrary distribution free Estimates skew/kurtosis data generate model May converge number parameters estimate","code":""},{"path":"/articles/lecture_path.html","id":"errors","dir":"Articles","previous_headings":"","what":"Errors","title":"Path Analysis","text":"Inadmissible solutions - get numbers output clearly parameters Parameter estimates illogical (huge) Just variances, covariances can negative Correlation estimates 1 (SMCs)","code":""},{"path":"/articles/lecture_path.html","id":"why-errors-happen","dir":"Articles","previous_headings":"","what":"Why Errors Happen","title":"Path Analysis","text":"Specification error Nonidentification Outliers Small samples Two indicators per latent (always better) Bad start values (especially errors) low high correlations (empirical identification)","code":""},{"path":"/articles/lecture_path.html","id":"path-models","dir":"Articles","previous_headings":"","what":"Path Models","title":"Path Analysis","text":"Circles latent (unobserved) variables Squares manifest (observed) variables Triangles can used represent intercepts (specific models use ) model manifest variables (measured/squares)","code":""},{"path":"/articles/lecture_path.html","id":"path-models-1","dir":"Articles","previous_headings":"","what":"Path Models","title":"Path Analysis","text":"Non-standardized solution -> b slope values Standardized solution -> beta values Non-standardized -> covariance Standardized -> correlation","code":""},{"path":"/articles/lecture_path.html","id":"path-models-2","dir":"Articles","previous_headings":"","what":"Path Models","title":"Path Analysis","text":"endogenous variables error term exogenous variables variance exogenous variables automatically correlated lavaan","code":""},{"path":"/articles/lecture_path.html","id":"solutions","dir":"Articles","previous_headings":"","what":"Solutions","title":"Path Analysis","text":"Allows compare path coefficients strength relationship Makes non-directional relationships correlation (interpretable) Matches values used thinking (EFA) Allows estimate future scores interpret path coefficients, original scale data","code":""},{"path":"/articles/lecture_path.html","id":"lets-do-it","dir":"Articles","previous_headings":"","what":"Let’s do it!","title":"Path Analysis","text":"Install lavaan: latent variable analysis Install semPlot: helps us make pictures","code":"install.packages(\"lavaan\") install.packages(\"semPlot\")"},{"path":"/articles/lecture_path.html","id":"syntax","dir":"Articles","previous_headings":"","what":"Syntax","title":"Path Analysis","text":"Note: ’s cheat sheet guide Canvas Best practice: name saved variable something like name.model name descriptive name model building ~ indicates regression ~~ indicates covariance/correlation =~ indicates latent variable","code":""},{"path":"/articles/lecture_path.html","id":"syntax-1","dir":"Articles","previous_headings":"","what":"Syntax","title":"Path Analysis","text":"Automatically adds error term endogenous variable Automatically constrains path 1 marker variable useful analyses get later always required path models, constrain zero Important note ’s often unexpected","code":""},{"path":"/articles/lecture_path.html","id":"a-path-example","dir":"Articles","previous_headings":"","what":"A Path Example","title":"Path Analysis","text":"","code":"library(rio) eval.data <- import(\"data/lecture_evals.csv\")"},{"path":[]},{"path":"/articles/lecture_path.html","id":"estimate-df","dir":"Articles","previous_headings":"","what":"Estimate df","title":"Path Analysis","text":"Possible = 4×(4+1)2=10\\frac{4 \\times (4+1)}{2} =  10 4 regressions 2 error variances 2 variances 1 covariance DF = 10 - 9 = 1","code":""},{"path":"/articles/lecture_path.html","id":"build-the-model","dir":"Articles","previous_headings":"","what":"Build the model","title":"Path Analysis","text":"","code":"library(lavaan) #> This is lavaan 0.6-19 #> lavaan is FREE software! Please report any bugs. eval.model <- ' q4 ~ q12 + q2 q1 ~ q4 + q12 '"},{"path":"/articles/lecture_path.html","id":"important-notes","dir":"Articles","previous_headings":"","what":"Important Notes","title":"Path Analysis","text":"important Spacing important can use # code, ignore line just like R","code":"eval.model #> [1] \"\\nq4 ~ q12 + q2\\nq1 ~ q4 + q12\\n\""},{"path":"/articles/lecture_path.html","id":"run-the-model","dir":"Articles","previous_headings":"","what":"Run the Model","title":"Path Analysis","text":"can use sem() function apply model data Save analysis, use good naming like eval.output default estimator normal theory maximum likelihood Use ?sem learn options","code":"eval.output <- sem(model = eval.model,                    data = eval.data)"},{"path":"/articles/lecture_path.html","id":"view-the-output","dir":"Articles","previous_headings":"","what":"View the Output","title":"Path Analysis","text":"","code":"summary(eval.output) #> lavaan 0.6-19 ended normally after 2 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         6 #>  #>   Number of observations                          3585 #>  #> Model Test User Model: #>                                                        #>   Test statistic                              2152.922 #>   Degrees of freedom                                 1 #>   P-value (Chi-square)                           0.000 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   q4 ~                                                 #>     q12               0.096    0.008   12.479    0.000 #>     q2                0.435    0.010   45.655    0.000 #>   q1 ~                                                 #>     q4                0.840    0.017   49.923    0.000 #>     q12               0.263    0.010   26.551    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .q4                0.094    0.002   42.338    0.000 #>    .q1                0.151    0.004   42.338    0.000"},{"path":"/articles/lecture_path.html","id":"output","dir":"Articles","previous_headings":"","what":"Output","title":"Path Analysis","text":"Sample size Estimator Minimum function test statistic χ2\\chi^2, df, p Unstandardized parameter estimates, standard error, Z, p","code":""},{"path":"/articles/lecture_path.html","id":"improved-output","dir":"Articles","previous_headings":"","what":"Improved Output","title":"Path Analysis","text":"","code":"summary(eval.output,          standardized = TRUE, # for the standardized solution         fit.measures = TRUE, # for model fit         rsquare = TRUE) # for SMCs #> lavaan 0.6-19 ended normally after 2 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         6 #>  #>   Number of observations                          3585 #>  #> Model Test User Model: #>                                                        #>   Test statistic                              2152.922 #>   Degrees of freedom                                 1 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              7293.347 #>   Degrees of freedom                                 5 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.705 #>   Tucker-Lewis Index (TLI)                      -0.476 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -2539.494 #>   Loglikelihood unrestricted model (H1)      -1463.033 #>                                                        #>   Akaike (AIC)                                5090.988 #>   Bayesian (BIC)                              5128.095 #>   Sample-size adjusted Bayesian (SABIC)       5109.030 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.775 #>   90 Percent confidence interval - lower         0.747 #>   90 Percent confidence interval - upper         0.802 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.105 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   q4 ~                                                                   #>     q12               0.096    0.008   12.479    0.000    0.096    0.164 #>     q2                0.435    0.010   45.655    0.000    0.435    0.598 #>   q1 ~                                                                   #>     q4                0.840    0.017   49.923    0.000    0.840    0.585 #>     q12               0.263    0.010   26.551    0.000    0.263    0.311 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .q4                0.094    0.002   42.338    0.000    0.094    0.553 #>    .q1                0.151    0.004   42.338    0.000    0.151    0.431 #>  #> R-Square: #>                    Estimate #>     q4                0.447 #>     q1                0.569"},{"path":"/articles/lecture_path.html","id":"create-a-picture","dir":"Articles","previous_headings":"","what":"Create a Picture","title":"Path Analysis","text":"Sometimes ’s hard know modeled intended diagram Let’s create plot make sure ’s meant ","code":"library(semPlot) semPaths(eval.output, # the analyzed model           whatLabels = \"par\", # what to add as the numbers, std for standardized          edge.label.cex = 1, # make the font bigger          layout = \"spring\") # change the layout tree, circle, spring, tree2, circle2"},{"path":"/articles/lecture_path.html","id":"another-example","dir":"Articles","previous_headings":"","what":"Another Example","title":"Path Analysis","text":"don’t actually data run analysis correlation matrix, can still run analysis ’s always better use raw data !","code":"regression.cor <- lav_matrix_lower2full(c(1.00,                                          0.20,1.00,                                          0.24,0.30,1.00,                                          0.70,0.80,0.30,1.00))  # name the variables in the matrix colnames(regression.cor) <-   rownames(regression.cor) <-   c(\"X1\", \"X2\", \"X3\", \"Y\")"},{"path":"/articles/lecture_path.html","id":"build-the-model-1","dir":"Articles","previous_headings":"","what":"Build the Model","title":"Path Analysis","text":"Let’s look can label paths","code":"regression.model <- ' # structural model for Y Y ~ a*X1 + b*X2 + c*X3  # label the residual variance of Y Y ~~ z*Y  '"},{"path":"/articles/lecture_path.html","id":"analyze-the-model","dir":"Articles","previous_headings":"","what":"Analyze the Model","title":"Path Analysis","text":"","code":"regression.fit <- sem(model = regression.model,                       sample.cov = regression.cor, # instead of data                       sample.nobs = 1000) # number of data points"},{"path":"/articles/lecture_path.html","id":"view-the-summary","dir":"Articles","previous_headings":"","what":"View the Summary","title":"Path Analysis","text":"","code":"summary(regression.fit,          standardized = TRUE,         fit.measures = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 1 iteration #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         4 #>  #>   Number of observations                          1000 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              2913.081 #>   Degrees of freedom                                 3 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    1.000 #>   Tucker-Lewis Index (TLI)                       1.000 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)                 38.102 #>   Loglikelihood unrestricted model (H1)         38.102 #>                                                        #>   Akaike (AIC)                                 -68.205 #>   Bayesian (BIC)                               -48.574 #>   Sample-size adjusted Bayesian (SABIC)        -61.278 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.000 #>   90 Percent confidence interval - lower         0.000 #>   90 Percent confidence interval - upper         0.000 #>   P-value H_0: RMSEA <= 0.050                       NA #>   P-value H_0: RMSEA >= 0.080                       NA #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.000 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   Y ~                                                                    #>     X1         (a)    0.571    0.008   74.539    0.000    0.571    0.571 #>     X2         (b)    0.700    0.008   89.724    0.000    0.700    0.700 #>     X3         (c)   -0.047    0.008   -5.980    0.000   -0.047   -0.047 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Y          (z)    0.054    0.002   22.361    0.000    0.054    0.054 #>  #> R-Square: #>                    Estimate #>     Y                 0.946"},{"path":"/articles/lecture_path.html","id":"create-a-picture-1","dir":"Articles","previous_headings":"","what":"Create a Picture","title":"Path Analysis","text":"","code":"semPaths(regression.fit,           whatLabels=\"par\",           edge.label.cex = 1,          layout=\"tree\")"},{"path":"/articles/lecture_path.html","id":"mediation-models","dir":"Articles","previous_headings":"","what":"Mediation Models","title":"Path Analysis","text":"Mediation models regression models imply two variables related (X Y) add mediator variable (M), diminishes relationship X Y can model regression structural layout","code":"beaujean.cov <- lav_matrix_lower2full(c(648.07,                                          30.05, 8.64,                                          140.18, 25.57, 233.21)) colnames(beaujean.cov) <-   rownames(beaujean.cov) <-   c(\"salary\", \"school\", \"iq\")"},{"path":"/articles/lecture_path.html","id":"build-the-model-2","dir":"Articles","previous_headings":"","what":"Build the Model","title":"Path Analysis","text":"","code":"beaujean.model <- ' salary ~ a*school + c*iq iq ~ b*school # this is reversed in first printing of the book  ind:= b*c # this is the mediation part  '"},{"path":"/articles/lecture_path.html","id":"analyze-the-model-1","dir":"Articles","previous_headings":"","what":"Analyze the Model","title":"Path Analysis","text":"","code":"beaujean.fit <- sem(model = beaujean.model,                      sample.cov = beaujean.cov,                      sample.nobs = 300)"},{"path":"/articles/lecture_path.html","id":"view-the-summary-1","dir":"Articles","previous_headings":"","what":"View the Summary","title":"Path Analysis","text":"","code":"summary(beaujean.fit,          standardized = TRUE,         fit.measures = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 1 iteration #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         5 #>  #>   Number of observations                           300 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 0.000 #>   Degrees of freedom                                 0 #>  #> Model Test Baseline Model: #>  #>   Test statistic                               179.791 #>   Degrees of freedom                                 3 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    1.000 #>   Tucker-Lewis Index (TLI)                       1.000 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -2549.357 #>   Loglikelihood unrestricted model (H1)      -2549.357 #>                                                        #>   Akaike (AIC)                                5108.713 #>   Bayesian (BIC)                              5127.232 #>   Sample-size adjusted Bayesian (SABIC)       5111.375 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.000 #>   90 Percent confidence interval - lower         0.000 #>   90 Percent confidence interval - upper         0.000 #>   P-value H_0: RMSEA <= 0.050                       NA #>   P-value H_0: RMSEA >= 0.080                       NA #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.000 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   salary ~                                                               #>     school     (a)    2.515    0.549    4.585    0.000    2.515    0.290 #>     iq         (c)    0.325    0.106    3.081    0.002    0.325    0.195 #>   iq ~                                                                   #>     school     (b)    2.959    0.247   12.005    0.000    2.959    0.570 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .salary          525.128   42.877   12.247    0.000  525.128    0.813 #>    .iq              157.011   12.820   12.247    0.000  157.011    0.676 #>  #> R-Square: #>                    Estimate #>     salary            0.187 #>     iq                0.324 #>  #> Defined Parameters: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>     ind               0.963    0.323    2.984    0.003    0.963    0.111"},{"path":"/articles/lecture_path.html","id":"create-a-picture-2","dir":"Articles","previous_headings":"","what":"Create a Picture","title":"Path Analysis","text":"","code":"semPaths(beaujean.fit,           whatLabels=\"par\",           edge.label.cex = 1,          layout=\"tree\")"},{"path":"/articles/lecture_path.html","id":"fit-indices","dir":"Articles","previous_headings":"","what":"Fit Indices","title":"Path Analysis","text":"Now ’ve done examples, can know model representative data? can use fit indices! lot imply perfectly right guidelines perfect either People misuse .","code":""},{"path":"/articles/lecture_path.html","id":"limitations","dir":"Articles","previous_headings":"","what":"Limitations","title":"Path Analysis","text":"Fit statistics indicate overall/average model fit means can bad sections, overall fit good one magical number/summary tell misspecification occurs tell predictive value model tell ’s theoretically meaningful","code":""},{"path":"/articles/lecture_path.html","id":"rules","dir":"Articles","previous_headings":"","what":"“Rules”","title":"Path Analysis","text":"Everyone cites Hu Bentler (1999) golden standards. Practically, rules represent problems rules p values effect sizes Excellent > .95, Good > .90 Excellent < .06, Good < .08, Acceptable < .10","code":""},{"path":"/articles/lecture_path.html","id":"model-test-statistic","dir":"Articles","previous_headings":"","what":"Model Test Statistic","title":"Path Analysis","text":"χ2\\chi^2 Examines reproduced correlation matrix matches sample correlation matrix Sometimes called “badness fit” want value small measures “mismatch” error","code":""},{"path":"/articles/lecture_path.html","id":"model-test-statistic-1","dir":"Articles","previous_headings":"","what":"Model Test Statistic","title":"Path Analysis","text":"Generally, reject null show ’s unlikely, therefore, research hypothesis likely However, want support model represents data, predicting null , ’s odd use χ2\\chi^2 “support” null","code":""},{"path":"/articles/lecture_path.html","id":"model-test-statistic-2","dir":"Articles","previous_headings":"","what":"Model Test Statistic","title":"Path Analysis","text":"FML minimum fit function maximum likelihood estimation p values based df model chi-square distribution want value non-significant (minus issues predicting null) However, non-significance problematic!","code":""},{"path":"/articles/lecture_path.html","id":"model-test-statistic-3","dir":"Articles","previous_headings":"","what":"Model Test Statistic","title":"Path Analysis","text":"Multivariate non-normality Correlation size - large correlations can hard estimate Unique variance Sample size big one","code":""},{"path":"/articles/lecture_path.html","id":"model-test-statistic-4","dir":"Articles","previous_headings":"","what":"Model Test Statistic","title":"Path Analysis","text":"One suggestion normed chi-square χ2df\\frac{\\chi^2}{df} - however, known just problematic Satorra-Bentler Yuan-Bentler","code":""},{"path":"/articles/lecture_path.html","id":"alternative-fit-indices","dir":"Articles","previous_headings":"","what":"Alternative Fit Indices","title":"Path Analysis","text":"Model Independence - model assuming relationships variables (.e., parameters significant) Saturated - model assuming parameters exist (.e., df = 0). can use ratio model compared possible models way determine fit. less black white, apply “rules” ’s easy “cherry-pick” best indices support model","code":""},{"path":"/articles/lecture_path.html","id":"alternative-fit-indices-1","dir":"Articles","previous_headings":"","what":"Alternative Fit Indices","title":"Path Analysis","text":"Absolute Fit Indices Incremental Fit Indices Parsimony-adjusted Indices Predictive Fit Indices","code":""},{"path":"/articles/lecture_path.html","id":"absolute-fit-indices","dir":"Articles","previous_headings":"","what":"Absolute fit indices","title":"Path Analysis","text":"Proportion covariance matrix explained model can think sort R2R^2 Want values high 1−VresidualVtotal1 - \\frac{V_{residual}}{V_{total}} residual variance explained model, total total amount variance Research indicates positively biased, recommended use","code":""},{"path":"/articles/lecture_path.html","id":"absolute-fit-indices-1","dir":"Articles","previous_headings":"","what":"Absolute Fit Indices","title":"Path Analysis","text":"Standardized root mean residual (SRMR) Want small values, ’s badness fit index","code":""},{"path":"/articles/lecture_path.html","id":"incremental-fit-indices","dir":"Articles","previous_headings":"","what":"Incremental Fit Indices","title":"Path Analysis","text":"Values 0 1, want high values model compared improvement independence model (relationships) 1−χM2−dfMχB2−dfB1 - \\frac{\\chi^2_{M} - df_{M}}{\\chi^2_{B} - df_{B}} variation CFI, said underestimate small samples 1−χmodel2χbaseline21 - \\frac{\\chi^2_{model}}{\\chi^2_{baseline}}","code":""},{"path":"/articles/lecture_path.html","id":"incremental-fit-indices-1","dir":"Articles","previous_headings":"","what":"Incremental Fit Indices","title":"Path Analysis","text":"Also known Bollen’s Non-normed fit index Modified NFI decreases emphasis sample size χbaseline2−χmodel2χbaseline2−dfmodel\\frac{\\chi^2_{baseline} - \\chi^2_{model}}{\\chi^2_{baseline} - df_{model}} Also known Bollen’s Normed Fit Index χmodel2dfmodelχbaseline2dfbaseline\\frac{\\frac{\\chi^2_{model}}{df_{model}}}{\\frac{\\chi^2_{baseline}}{df_{baseline}}}","code":""},{"path":"/articles/lecture_path.html","id":"incremental-fit-indices-2","dir":"Articles","previous_headings":"","what":"Incremental Fit Indices","title":"Path Analysis","text":"Also known Bentler-Bonet Non-Normed Fit Index popular fit index χbaseline2dfbaseline−χmodel2dfmodelχbaseline2dfbaseline−1\\frac{\\frac{\\chi^2_{baseline}}{df_{baseline}} - \\frac{\\chi^2_{model}}{df_{model}}}{\\frac{\\chi^2_{baseline}}{df_{baseline}} - 1}","code":""},{"path":"/articles/lecture_path.html","id":"parsimony-adjusted-indices","dir":"Articles","previous_headings":"","what":"Parsimony Adjusted Indices","title":"Path Analysis","text":"include penalties model complexity added paths often result better fit (overfitting) smaller values simpler models popular index Report confidence interval χ2−dfmodeldfmodel×(N−1)\\frac{\\sqrt{\\chi^2 - df_{model}}}{\\sqrt{df_{model}\\times(N - 1)}}","code":""},{"path":"/articles/lecture_path.html","id":"predictive-fit-indices","dir":"Articles","previous_headings":"","what":"Predictive Fit Indices","title":"Path Analysis","text":"Estimate model fit hypothetical replication study sample size randomly drawn population often used model comparisons, rather “fit” Often also considered parsimony adjusted indices","code":""},{"path":"/articles/lecture_path.html","id":"model-comparisons","dir":"Articles","previous_headings":"","what":"Model Comparisons","title":"Path Analysis","text":"can compare adjusted model original model determine adjustment better can compare fits see better","code":""},{"path":"/articles/lecture_path.html","id":"model-comparisons-1","dir":"Articles","previous_headings":"","what":"Model Comparisons","title":"Path Analysis","text":"can create one model another addition subtraction parameters, nested Model said nested within Model B, Model B complicated version Model example, one-factor model nested within two-factor one-factor model can viewed two-factor model correlation factors perfect","code":""},{"path":"/articles/lecture_path.html","id":"model-comparisons-nested-models","dir":"Articles","previous_headings":"","what":"Model Comparisons Nested Models","title":"Path Analysis","text":"Chi-square difference test - difference models significant? yes, say model lower chi-square better , say go simpler model","code":"chi_difference <- 12.6 - 4.3 df_difference <- 14 - 12 pchisq(chi_difference, df_difference, lower.tail = F) #> [1] 0.01576442"},{"path":"/articles/lecture_path.html","id":"model-comparisons-nested-models-1","dir":"Articles","previous_headings":"","what":"Model Comparisons Nested Models","title":"Path Analysis","text":"CFI difference test Subtract CFI model 1 - CFI model 2 change .01, models considered different version biased sample size issues chi-square","code":""},{"path":"/articles/lecture_path.html","id":"model-comparisons-nested-models-2","dir":"Articles","previous_headings":"","what":"Model Comparisons Nested Models","title":"Path Analysis","text":"can tell change? Just change one thing time! Tell chi-square change added path suggested Based χ2(1)\\chi^2(1) (Lagrange Multiplier) Anything χ2(1)\\chi^2(1) > 3.84 p < .05","code":""},{"path":"/articles/lecture_path.html","id":"model-comparisons-nested-models-3","dir":"Articles","previous_headings":"","what":"Model Comparisons Nested Models","title":"Path Analysis","text":"Instead math, can use anova() function anova(model1.fit, model2.fit)","code":""},{"path":"/articles/lecture_path.html","id":"model-comparisons-non-nested-models","dir":"Articles","previous_headings":"","what":"Model Comparisons Non-Nested Models","title":"Path Analysis","text":"Akaike Information Criterion (AIC) Bayesian Information Criterion (BIC) penalize complex models. things equal, biased simpler model. compare, pick model lower value (always lower, even negative)","code":""},{"path":"/articles/lecture_path.html","id":"model-comparisons-non-nested-models-1","dir":"Articles","previous_headings":"","what":"Model Comparisons Non-Nested Models","title":"Path Analysis","text":"FML+2×tN−p−2FML + \\frac{2\\times t}{N - p - 2} t number parameters estimated p number squares (measured variables) rules, pick smallest value","code":""},{"path":"/articles/lecture_path.html","id":"so-what-to-report","dir":"Articles","previous_headings":"","what":"So what to report?","title":"Path Analysis","text":"use RMSEA, SRMR, CFI Generally, people also report chi-square df Determine type model change use right model comparison statistic","code":""},{"path":"/articles/lecture_path.html","id":"path-comparison-example","dir":"Articles","previous_headings":"","what":"Path Comparison Example","title":"Path Analysis","text":"","code":"compare.data <- lav_matrix_lower2full(c(1.00,                                         .53,    1.00,                                            .15,    .18,    1.00,                                                .52,    .29,    -.05,   1.00,                                            .30,    .34,    .23,    .09,    1.00))  colnames(compare.data) <-    rownames(compare.data) <-    c(\"morale\", \"illness\", \"neuro\", \"relationship\", \"SES\")"},{"path":"/articles/lecture_path.html","id":"build-a-model","dir":"Articles","previous_headings":"","what":"Build a Model","title":"Path Analysis","text":"","code":"#model 1 compare.model1 = ' illness ~ morale relationship ~ morale morale ~ SES + neuro '  #model 2 compare.model2 = ' SES ~ illness + neuro morale ~ SES + illness relationship ~ morale + neuro '"},{"path":"/articles/lecture_path.html","id":"analyze-the-model-2","dir":"Articles","previous_headings":"","what":"Analyze the Model","title":"Path Analysis","text":"","code":"compare.model1.fit <- sem(compare.model1,                            sample.cov = compare.data,                            sample.nobs = 469)  summary(compare.model1.fit,          standardized = TRUE,         fit.measures = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 5 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         8 #>  #>   Number of observations                           469 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                40.303 #>   Degrees of freedom                                 4 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                               390.816 #>   Degrees of freedom                                 9 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.905 #>   Tucker-Lewis Index (TLI)                       0.786 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -1819.689 #>   Loglikelihood unrestricted model (H1)      -1799.537 #>                                                        #>   Akaike (AIC)                                3655.377 #>   Bayesian (BIC)                              3688.582 #>   Sample-size adjusted Bayesian (SABIC)       3663.192 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.139 #>   90 Percent confidence interval - lower         0.102 #>   90 Percent confidence interval - upper         0.180 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    0.995 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.065 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   illness ~                                                              #>     morale            0.530    0.039   13.535    0.000    0.530    0.530 #>   relationship ~                                                         #>     morale            0.520    0.039   13.184    0.000    0.520    0.520 #>   morale ~                                                               #>     SES               0.280    0.045    6.217    0.000    0.280    0.280 #>     neuro             0.086    0.045    1.897    0.058    0.086    0.086 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>  .illness ~~                                                             #>    .relationship      0.014    0.033    0.430    0.667    0.014    0.020 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .illness           0.718    0.047   15.313    0.000    0.718    0.719 #>    .relationship      0.728    0.048   15.313    0.000    0.728    0.730 #>    .morale            0.901    0.059   15.313    0.000    0.901    0.903 #>  #> R-Square: #>                    Estimate #>     illness           0.281 #>     relationship      0.270 #>     morale            0.097"},{"path":"/articles/lecture_path.html","id":"analyze-the-model-3","dir":"Articles","previous_headings":"","what":"Analyze the Model","title":"Path Analysis","text":"","code":"compare.model2.fit <- sem(compare.model2,                            sample.cov = compare.data,                            sample.nobs = 469)  summary(compare.model2.fit,          standardized = TRUE,         fit.measures = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 1 iteration #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                         9 #>  #>   Number of observations                           469 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                 3.245 #>   Degrees of freedom                                 3 #>   P-value (Chi-square)                           0.355 #>  #> Model Test Baseline Model: #>  #>   Test statistic                               400.859 #>   Degrees of freedom                                 9 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.999 #>   Tucker-Lewis Index (TLI)                       0.998 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -1796.138 #>   Loglikelihood unrestricted model (H1)      -1794.516 #>                                                        #>   Akaike (AIC)                                3610.276 #>   Bayesian (BIC)                              3647.632 #>   Sample-size adjusted Bayesian (SABIC)       3619.068 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.013 #>   90 Percent confidence interval - lower         0.000 #>   90 Percent confidence interval - upper         0.080 #>   P-value H_0: RMSEA <= 0.050                    0.742 #>   P-value H_0: RMSEA >= 0.080                    0.050 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.016 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   SES ~                                                                  #>     illness           0.309    0.043    7.110    0.000    0.309    0.309 #>     neuro             0.174    0.043    4.019    0.000    0.174    0.174 #>   morale ~                                                               #>     SES               0.135    0.041    3.291    0.001    0.135    0.135 #>     illness           0.484    0.041   11.756    0.000    0.484    0.484 #>   relationship ~                                                         #>     morale            0.540    0.039   13.745    0.000    0.540    0.538 #>     neuro            -0.131    0.039   -3.335    0.001   -0.131   -0.131 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .SES               0.853    0.056   15.313    0.000    0.853    0.855 #>    .morale            0.701    0.046   15.313    0.000    0.701    0.703 #>    .relationship      0.711    0.046   15.313    0.000    0.711    0.710 #>  #> R-Square: #>                    Estimate #>     SES               0.145 #>     morale            0.297 #>     relationship      0.290"},{"path":"/articles/lecture_path.html","id":"view-the-model","dir":"Articles","previous_headings":"","what":"View the Model","title":"Path Analysis","text":"","code":"semPaths(compare.model1.fit,           whatLabels=\"par\",           edge.label.cex = 1,          layout=\"spring\") semPaths(compare.model2.fit,           whatLabels=\"par\",           edge.label.cex = 1,          layout=\"spring\")"},{"path":"/articles/lecture_path.html","id":"compare-the-models","dir":"Articles","previous_headings":"","what":"Compare the Models","title":"Path Analysis","text":"","code":"anova(compare.model1.fit, compare.model2.fit) #>  #> Chi-Squared Difference Test #>  #>                    Df    AIC    BIC   Chisq Chisq diff   RMSEA Df diff #> compare.model2.fit  3 3610.3 3647.6  3.2454                            #> compare.model1.fit  4 3655.4 3688.6 40.3031     37.058 0.27728       1 #>                    Pr(>Chisq)     #> compare.model2.fit                #> compare.model1.fit  1.147e-09 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 fitmeasures(compare.model1.fit, c(\"aic\", \"ecvi\")) #>      aic     ecvi  #> 3655.377    0.120 fitmeasures(compare.model2.fit, c(\"aic\", \"ecvi\")) #>      aic     ecvi  #> 3610.276    0.045"},{"path":"/articles/lecture_path.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Path Analysis","text":"lecture ’ve learned: write basic lavaan model syntax analyze path model summarize path model create sem pictures Estimation fit indices","code":""},{"path":"/articles/lecture_secondcfa.html","id":"second-order-latent-models","dir":"Articles","previous_headings":"","what":"Second Order Latent Models","title":"Confirmatory Factor Analysis: Second Order","text":"second order model one structured order rank. Two types consider: Higher-order: Describes latent variables structured latents influence latents levels. Bi-factor: Generally used describe CFA two sets latent variables (hierarchical).","code":""},{"path":"/articles/lecture_secondcfa.html","id":"second-order-models","dir":"Articles","previous_headings":"","what":"Second Order Models","title":"Confirmatory Factor Analysis: Second Order","text":"idea higher order model : latent variables measured observed variables. portion variance latent variables can explained second (set) latent variables. Therefore, switching covariances factors using another latent explain .","code":""},{"path":"/articles/lecture_secondcfa.html","id":"second-order-models-1","dir":"Articles","previous_headings":"","what":"Second Order Models","title":"Confirmatory Factor Analysis: Second Order","text":"used? multiple latent variables covary (lot). second set latents explains covariance.","code":""},{"path":[]},{"path":"/articles/lecture_secondcfa.html","id":"higher-order-models","dir":"Articles","previous_headings":"","what":"Higher Order Models","title":"Confirmatory Factor Analysis: Second Order","text":"covariance first order accounted second order plus specific factor. Specific factors error explained second order latents. higher order thought indirectly influence manifest variables first order.","code":""},{"path":"/articles/lecture_secondcfa.html","id":"identification","dir":"Articles","previous_headings":"","what":"Identification","title":"Confirmatory Factor Analysis: Second Order","text":"Remember portion model identified. section latent variable identified. section latents identified.","code":""},{"path":"/articles/lecture_secondcfa.html","id":"identification-1","dir":"Articles","previous_headings":"","what":"Identification","title":"Confirmatory Factor Analysis: Second Order","text":"can achieve identification couple ways: Set loadings upper portion model equal giving name. can set variance upper latent one. can set error variances latents lower portion equal.","code":""},{"path":"/articles/lecture_secondcfa.html","id":"bi-factor-models","dir":"Articles","previous_headings":"","what":"Bi-Factor Models","title":"Confirmatory Factor Analysis: Second Order","text":"Special type model two sets latents, hierarchically structured. Best used : General factor accounts variance manifest variables. Domain specific areas thought influence manifest variables.","code":""},{"path":[]},{"path":"/articles/lecture_secondcfa.html","id":"bi-factor-models-2","dir":"Articles","previous_headings":"","what":"Bi-Factor Models","title":"Confirmatory Factor Analysis: Second Order","text":"One thing note latent variables left uncorrelated type model. structure represents domain specific part interpretation.","code":""},{"path":"/articles/lecture_secondcfa.html","id":"model-differences","dir":"Articles","previous_headings":"","what":"Model Differences","title":"Confirmatory Factor Analysis: Second Order","text":"Differences bi-factor hierarchical: hierarchical models, second order influences first order, two sets latents bi-factors uncorrelated. allow test differently?","code":""},{"path":"/articles/lecture_secondcfa.html","id":"model-differences-1","dir":"Articles","previous_headings":"","what":"Model Differences","title":"Confirmatory Factor Analysis: Second Order","text":"Advantages: Allows see first order latent variables influence manifest variables separately latent variables. accounting general latent variable, domain specific items still accounting variance? can compare models without domain specific areas","code":""},{"path":"/articles/lecture_secondcfa.html","id":"examples-wisc","dir":"Articles","previous_headings":"","what":"Examples: WISC","title":"Confirmatory Factor Analysis: Second Order","text":"Going back WISC data, time subscales available.","code":"library(lavaan) library(semPlot)  ##import the data wisc4.cov <- lav_matrix_lower2full(c(8.29,                                     5.37,9.06,                                     2.83,4.44,8.35,                                     2.83,3.32,3.36,8.88,                                     5.50,6.66,4.20,3.43,9.18,                                     6.18,6.73,4.01,3.33,6.77,9.12,                                     3.52,3.77,3.19,2.75,3.88,4.05,8.88,                                     3.79,4.50,3.72,3.39,4.53,4.70,4.54,8.94,                                     2.30,2.67,2.40,2.38,2.06,2.59,2.65,2.83,8.76,                                     3.06,4.04,3.70,2.79,3.59,3.67,3.44,4.20,4.53,9.73))  wisc4.sd <- c(2.88,3.01,2.89,2.98,3.03,3.02,2.98,2.99,2.96,3.12)   names(wisc4.sd) <-    colnames(wisc4.cov) <-    rownames(wisc4.cov) <- c(\"Comprehension\", \"Information\",                           \"Matrix.Reasoning\", \"Picture.Concepts\",                           \"Similarities\", \"Vocabulary\",  \"Digit.Span\",                           \"Letter.Number\",  \"Coding\", \"Symbol.Search\")"},{"path":"/articles/lecture_secondcfa.html","id":"examples-wisc-1","dir":"Articles","previous_headings":"","what":"Examples: WISC","title":"Confirmatory Factor Analysis: Second Order","text":"Let’s start first order model WISC. starting measurement model, can ensure measurement section identified works correctly. also check correlations/covariances factors make sure even related.","code":""},{"path":"/articles/lecture_secondcfa.html","id":"examples-wisc-build-the-model","dir":"Articles","previous_headings":"","what":"Examples: WISC Build the Model","title":"Confirmatory Factor Analysis: Second Order","text":"","code":"##first order model wisc4.fourFactor.model <- ' gc =~ Comprehension + Information +  Similarities + Vocabulary  gf =~ Matrix.Reasoning + Picture.Concepts gsm =~  Digit.Span + Letter.Number gs =~ Coding + Symbol.Search '"},{"path":"/articles/lecture_secondcfa.html","id":"examples-wisc-analyze-the-model","dir":"Articles","previous_headings":"","what":"Examples: WISC Analyze the Model","title":"Confirmatory Factor Analysis: Second Order","text":"","code":"wisc4.fourFactor.fit <- cfa(model = wisc4.fourFactor.model,                              sample.cov = wisc4.cov,                              sample.nobs = 550)"},{"path":"/articles/lecture_secondcfa.html","id":"examples-wisc-summarize-the-model","dir":"Articles","previous_headings":"","what":"Examples: WISC Summarize the Model","title":"Confirmatory Factor Analysis: Second Order","text":"Logical solution: Positive variances SMCs + Correlations < 1 error messages SEs “huge” Estimates: questions load appropriately? Model fit: fit indices indicate? Can improve model fit without overfitting?","code":"summary(wisc4.fourFactor.fit,          fit.measure = TRUE,          standardized = TRUE,         rsquare = TRUE) #> lavaan 0.6-19 ended normally after 83 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        26 #>  #>   Number of observations                           550 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                52.226 #>   Degrees of freedom                                29 #>   P-value (Chi-square)                           0.005 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              2554.487 #>   Degrees of freedom                                45 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.991 #>   Tucker-Lewis Index (TLI)                       0.986 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -12562.889 #>   Loglikelihood unrestricted model (H1)     -12536.776 #>                                                        #>   Akaike (AIC)                               25177.778 #>   Bayesian (BIC)                             25289.836 #>   Sample-size adjusted Bayesian (SABIC)      25207.301 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.038 #>   90 Percent confidence interval - lower         0.021 #>   90 Percent confidence interval - upper         0.055 #>   P-value H_0: RMSEA <= 0.050                    0.876 #>   P-value H_0: RMSEA >= 0.080                    0.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.020 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   gc =~                                                                  #>     Comprehension     1.000                               2.193    0.762 #>     Information       1.160    0.056   20.802    0.000    2.544    0.846 #>     Similarities      1.166    0.056   20.772    0.000    2.558    0.845 #>     Vocabulary        1.218    0.056   21.860    0.000    2.670    0.885 #>   gf =~                                                                  #>     Matrix.Reasnng    1.000                               2.000    0.693 #>     Picture.Cncpts    0.839    0.078   10.775    0.000    1.677    0.563 #>   gsm =~                                                                 #>     Digit.Span        1.000                               1.967    0.661 #>     Letter.Number     1.172    0.086   13.560    0.000    2.304    0.771 #>   gs =~                                                                  #>     Coding            1.000                               1.779    0.601 #>     Symbol.Search     1.429    0.139   10.283    0.000    2.542    0.816 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   gc ~~                                                                  #>     gf                3.412    0.337   10.130    0.000    0.778    0.778 #>     gsm               3.302    0.336    9.826    0.000    0.766    0.766 #>     gs                2.180    0.290    7.511    0.000    0.559    0.559 #>   gf ~~                                                                  #>     gsm               3.245    0.352    9.220    0.000    0.825    0.825 #>     gs                2.507    0.329    7.608    0.000    0.705    0.705 #>   gsm ~~                                                                 #>     gs                2.474    0.324    7.627    0.000    0.707    0.707 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Comprehension     3.467    0.240   14.452    0.000    3.467    0.419 #>    .Information       2.571    0.203   12.637    0.000    2.571    0.284 #>    .Similarities      2.622    0.207   12.672    0.000    2.622    0.286 #>    .Vocabulary        1.973    0.181   10.920    0.000    1.973    0.217 #>    .Matrix.Reasnng    4.336    0.424   10.224    0.000    4.336    0.520 #>    .Picture.Cncpts    6.051    0.434   13.944    0.000    6.051    0.683 #>    .Digit.Span        4.996    0.377   13.239    0.000    4.996    0.564 #>    .Letter.Number     3.614    0.381    9.499    0.000    3.614    0.405 #>    .Coding            5.581    0.428   13.053    0.000    5.581    0.638 #>    .Symbol.Search     3.249    0.573    5.668    0.000    3.249    0.335 #>     gc                4.807    0.468   10.270    0.000    1.000    1.000 #>     gf                3.999    0.544    7.353    0.000    1.000    1.000 #>     gsm               3.868    0.497    7.790    0.000    1.000    1.000 #>     gs                3.163    0.484    6.535    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     Comprehension     0.581 #>     Information       0.716 #>     Similarities      0.714 #>     Vocabulary        0.783 #>     Matrix.Reasnng    0.480 #>     Picture.Cncpts    0.317 #>     Digit.Span        0.436 #>     Letter.Number     0.595 #>     Coding            0.362 #>     Symbol.Search     0.665"},{"path":"/articles/lecture_secondcfa.html","id":"examples-wisc-diagram-the-model","dir":"Articles","previous_headings":"","what":"Examples: WISC Diagram the Model","title":"Confirmatory Factor Analysis: Second Order","text":"","code":"semPaths(wisc4.fourFactor.fit,           whatLabels=\"std\",           edge.label.cex = 1,          edge.color = \"black\",          what = \"std\",          layout=\"tree\")"},{"path":"/articles/lecture_secondcfa.html","id":"examples-wisc-interpretation","dir":"Articles","previous_headings":"","what":"Examples: WISC Interpretation","title":"Confirmatory Factor Analysis: Second Order","text":"first order model, find correlations pretty high. ’s good sign maybe second order model appropriate. factors collapsed together, distinct.","code":""},{"path":"/articles/lecture_secondcfa.html","id":"examples-second-order-wisc","dir":"Articles","previous_headings":"","what":"Examples: Second-Order WISC","title":"Confirmatory Factor Analysis: Second Order","text":"Build model:","code":"wisc4.higherOrder.model <- ' gc =~ Comprehension + Information + Similarities + Vocabulary  gf =~ Matrix.Reasoning + Picture.Concepts gsm =~  Digit.Span + Letter.Number gs =~ Coding + Symbol.Search  g =~ gf + gc  + gsm + gs  '"},{"path":"/articles/lecture_secondcfa.html","id":"examples-second-order-wisc-1","dir":"Articles","previous_headings":"","what":"Examples: Second-Order WISC","title":"Confirmatory Factor Analysis: Second Order","text":"Analyze model:","code":"wisc4.higherOrder.fit <- cfa(model = wisc4.higherOrder.model,                               sample.cov = wisc4.cov,                               sample.nobs = 550)"},{"path":"/articles/lecture_secondcfa.html","id":"examples-second-order-wisc-2","dir":"Articles","previous_headings":"","what":"Examples: Second-Order WISC","title":"Confirmatory Factor Analysis: Second Order","text":"Summarize model: Note: fit indices change. Let’s look loadings, shift happened.","code":"summary(wisc4.higherOrder.fit,          fit.measure=TRUE,          standardized=TRUE,          rsquare = TRUE) #> lavaan 0.6-19 ended normally after 67 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        24 #>  #>   Number of observations                           550 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                58.180 #>   Degrees of freedom                                31 #>   P-value (Chi-square)                           0.002 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              2554.487 #>   Degrees of freedom                                45 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.989 #>   Tucker-Lewis Index (TLI)                       0.984 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -12565.866 #>   Loglikelihood unrestricted model (H1)     -12536.776 #>                                                        #>   Akaike (AIC)                               25179.732 #>   Bayesian (BIC)                             25283.170 #>   Sample-size adjusted Bayesian (SABIC)      25206.984 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.040 #>   90 Percent confidence interval - lower         0.024 #>   90 Percent confidence interval - upper         0.056 #>   P-value H_0: RMSEA <= 0.050                    0.846 #>   P-value H_0: RMSEA >= 0.080                    0.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.023 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   gc =~                                                                  #>     Comprehension     1.000                               2.194    0.763 #>     Information       1.160    0.056   20.832    0.000    2.545    0.846 #>     Similarities      1.164    0.056   20.767    0.000    2.555    0.844 #>     Vocabulary        1.217    0.056   21.882    0.000    2.670    0.885 #>   gf =~                                                                  #>     Matrix.Reasnng    1.000                               1.991    0.690 #>     Picture.Cncpts    0.846    0.079   10.758    0.000    1.685    0.566 #>   gsm =~                                                                 #>     Digit.Span        1.000                               1.966    0.660 #>     Letter.Number     1.173    0.087   13.503    0.000    2.306    0.772 #>   gs =~                                                                  #>     Coding            1.000                               1.771    0.599 #>     Symbol.Search     1.441    0.142   10.149    0.000    2.553    0.819 #>   g =~                                                                   #>     gf                1.000                               0.930    0.930 #>     gc                0.968    0.079   12.241    0.000    0.817    0.817 #>     gsm               0.984    0.087   11.319    0.000    0.927    0.927 #>     gs                0.698    0.080    8.689    0.000    0.730    0.730 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Comprehension     3.460    0.240   14.440    0.000    3.460    0.418 #>    .Information       2.565    0.203   12.614    0.000    2.565    0.284 #>    .Similarities      2.635    0.208   12.692    0.000    2.635    0.288 #>    .Vocabulary        1.973    0.181   10.906    0.000    1.973    0.217 #>    .Matrix.Reasnng    4.372    0.424   10.306    0.000    4.372    0.525 #>    .Picture.Cncpts    6.025    0.434   13.871    0.000    6.025    0.680 #>    .Digit.Span        5.000    0.378   13.219    0.000    5.000    0.564 #>    .Letter.Number     3.608    0.382    9.441    0.000    3.608    0.404 #>    .Coding            5.606    0.430   13.040    0.000    5.606    0.641 #>    .Symbol.Search     3.197    0.584    5.473    0.000    3.197    0.329 #>    .gc                1.599    0.226    7.082    0.000    0.332    0.332 #>    .gf                0.534    0.340    1.569    0.117    0.135    0.135 #>    .gsm               0.540    0.241    2.246    0.025    0.140    0.140 #>    .gs                1.467    0.262    5.590    0.000    0.468    0.468 #>     g                 3.429    0.452    7.591    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     Comprehension     0.582 #>     Information       0.716 #>     Similarities      0.712 #>     Vocabulary        0.783 #>     Matrix.Reasnng    0.475 #>     Picture.Cncpts    0.320 #>     Digit.Span        0.436 #>     Letter.Number     0.596 #>     Coding            0.359 #>     Symbol.Search     0.671 #>     gc                0.668 #>     gf                0.865 #>     gsm               0.860 #>     gs                0.532"},{"path":"/articles/lecture_secondcfa.html","id":"examples-second-order-wisc-3","dir":"Articles","previous_headings":"","what":"Examples: Second-Order WISC","title":"Confirmatory Factor Analysis: Second Order","text":"","code":"semPaths(wisc4.higherOrder.fit,           whatLabels=\"std\",           edge.label.cex = 1,          edge.color = \"black\",          what = \"std\",          layout=\"tree\")"},{"path":"/articles/lecture_secondcfa.html","id":"examples-bi-factor-wisc","dir":"Articles","previous_headings":"","what":"Examples: Bi-factor WISC","title":"Confirmatory Factor Analysis: Second Order","text":"Build model: particular model, two indicators, set equal identification bifactor run properly? second order model? define measurement model, define second latent every variable related . Note: variables latents like hierarchical model.","code":"wisc4.bifactor.model <- ' gc =~ Comprehension + Information +  Similarities + Vocabulary  gf =~ a*Matrix.Reasoning + a*Picture.Concepts   gsm =~  b*Digit.Span + b*Letter.Number gs =~ c*Coding + c*Symbol.Search  g =~ Information + Comprehension + Matrix.Reasoning + Picture.Concepts + Similarities + Vocabulary +  Digit.Span + Letter.Number + Coding + Symbol.Search '"},{"path":"/articles/lecture_secondcfa.html","id":"examples-bi-factor-wisc-1","dir":"Articles","previous_headings":"","what":"Examples: Bi-factor WISC","title":"Confirmatory Factor Analysis: Second Order","text":"Analyze model: Last, want turn automatic exogenous correlations, orthogonal = TRUE model. argument correlations normally seen domain specific latents due general factor.","code":"wisc4.bifactor.fit <- cfa(model = wisc4.bifactor.model,                            sample.cov = wisc4.cov,                           sample.nobs = 550,                            orthogonal = TRUE)"},{"path":"/articles/lecture_secondcfa.html","id":"examples-bi-factor-wisc-2","dir":"Articles","previous_headings":"","what":"Examples: Bi-factor WISC","title":"Confirmatory Factor Analysis: Second Order","text":"Summarize model: Notice covariances zero. Notice equal loading estimates variables set .","code":"summary(wisc4.bifactor.fit,          fit.measure = TRUE,          rsquare = TRUE,          standardized = TRUE) #> lavaan 0.6-19 ended normally after 64 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        27 #>  #>   Number of observations                           550 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                50.833 #>   Degrees of freedom                                28 #>   P-value (Chi-square)                           0.005 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              2554.487 #>   Degrees of freedom                                45 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.991 #>   Tucker-Lewis Index (TLI)                       0.985 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)             -12562.192 #>   Loglikelihood unrestricted model (H1)     -12536.776 #>                                                        #>   Akaike (AIC)                               25178.385 #>   Bayesian (BIC)                             25294.753 #>   Sample-size adjusted Bayesian (SABIC)      25209.043 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.039 #>   90 Percent confidence interval - lower         0.021 #>   90 Percent confidence interval - upper         0.055 #>   P-value H_0: RMSEA <= 0.050                    0.864 #>   P-value H_0: RMSEA >= 0.080                    0.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.022 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   gc =~                                                                  #>     Comprehnsn        1.000                               1.426    0.496 #>     Informatin        0.883    0.100    8.820    0.000    1.259    0.419 #>     Similarits        0.965    0.106    9.134    0.000    1.375    0.454 #>     Vocabulary        1.168    0.121    9.670    0.000    1.665    0.552 #>   gf =~                                                                  #>     Mtrx.Rsnng (a)    1.000                               0.634    0.220 #>     Pctr.Cncpt (a)    1.000                               0.634    0.213 #>   gsm =~                                                                 #>     Digit.Span (b)    1.000                               0.863    0.290 #>     Lettr.Nmbr (b)    1.000                               0.863    0.289 #>   gs =~                                                                  #>     Coding     (c)    1.000                               1.461    0.494 #>     Symbl.Srch (c)    1.000                               1.461    0.469 #>   g =~                                                                   #>     Informatin        1.000                               2.191    0.728 #>     Comprehnsn        0.780    0.051   15.388    0.000    1.709    0.594 #>     Mtrx.Rsnng        0.859    0.066   13.107    0.000    1.883    0.652 #>     Pctr.Cncpt        0.716    0.067   10.704    0.000    1.568    0.527 #>     Similarits        0.972    0.051   18.880    0.000    2.130    0.704 #>     Vocabulary        0.974    0.047   20.528    0.000    2.134    0.707 #>     Digit.Span        0.818    0.068   11.971    0.000    1.792    0.602 #>     Lettr.Nmbr        0.965    0.069   13.900    0.000    2.113    0.707 #>     Coding            0.584    0.065    8.975    0.000    1.280    0.433 #>     Symbl.Srch        0.851    0.069   12.262    0.000    1.865    0.598 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   gc ~~                                                                  #>     gf                0.000                               0.000    0.000 #>     gsm               0.000                               0.000    0.000 #>     gs                0.000                               0.000    0.000 #>     g                 0.000                               0.000    0.000 #>   gf ~~                                                                  #>     gsm               0.000                               0.000    0.000 #>     gs                0.000                               0.000    0.000 #>     g                 0.000                               0.000    0.000 #>   gsm ~~                                                                 #>     gs                0.000                               0.000    0.000 #>     g                 0.000                               0.000    0.000 #>   gs ~~                                                                  #>     g                 0.000                               0.000    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .Comprehension     3.323    0.250   13.275    0.000    3.323    0.402 #>    .Information       2.660    0.202   13.179    0.000    2.660    0.294 #>    .Similarities      2.737    0.212   12.914    0.000    2.737    0.299 #>    .Vocabulary        1.777    0.208    8.560    0.000    1.777    0.195 #>    .Matrix.Reasnng    4.388    0.383   11.450    0.000    4.388    0.526 #>    .Picture.Cncpts    6.004    0.444   13.534    0.000    6.004    0.677 #>    .Digit.Span        4.908    0.381   12.890    0.000    4.908    0.554 #>    .Letter.Number     3.713    0.345   10.751    0.000    3.713    0.416 #>    .Coding            4.971    0.408   12.174    0.000    4.971    0.568 #>    .Symbol.Search     4.100    0.391   10.489    0.000    4.100    0.422 #>     gc                2.032    0.373    5.450    0.000    1.000    1.000 #>     gf                0.402    0.280    1.437    0.151    1.000    1.000 #>     gsm               0.745    0.282    2.640    0.008    1.000    1.000 #>     gs                2.136    0.333    6.412    0.000    1.000    1.000 #>     g                 4.798    0.543    8.841    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     Comprehension     0.598 #>     Information       0.706 #>     Similarities      0.701 #>     Vocabulary        0.805 #>     Matrix.Reasnng    0.474 #>     Picture.Cncpts    0.323 #>     Digit.Span        0.446 #>     Letter.Number     0.584 #>     Coding            0.432 #>     Symbol.Search     0.578"},{"path":"/articles/lecture_secondcfa.html","id":"examples-bi-factor-wisc-3","dir":"Articles","previous_headings":"","what":"Examples: Bi-factor WISC","title":"Confirmatory Factor Analysis: Second Order","text":"Diagram model:","code":"semPaths(wisc4.bifactor.fit,           whatLabels=\"std\",           edge.label.cex = 1,          edge.color = \"black\",          what = \"std\",          layout=\"tree\")"},{"path":"/articles/lecture_secondcfa.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Confirmatory Factor Analysis: Second Order","text":"lecture ’ve learned: account strong correlations latent variables program run higher-order model (hierarchical) program run bi-factor model (hierarchical)","code":""},{"path":"/articles/lecture_sem.html","id":"full-structural-models","dir":"Articles","previous_headings":"","what":"Full Structural Models","title":"Full SEM","text":"fully latent full structural model two parts: Measurement model: latent variable ’s measured variables (CFA) Structural model: relationship variables measurement model structural model may second order latent, like last week structural model include predictors simply convert correlations specific prediction direction part structural model","code":""},{"path":[]},{"path":"/articles/lecture_sem.html","id":"full-structural-models-1","dir":"Articles","previous_headings":"","what":"Full Structural Models","title":"Full SEM","text":"reminder earlier semester: Reflective indicators: assume latent variables cause, exogenous Formative indicators: assume latent variables criterion, endogenous","code":""},{"path":"/articles/lecture_sem.html","id":"full-structural-models-2","dir":"Articles","previous_headings":"","what":"Full Structural Models","title":"Full SEM","text":"Example formative indicator Income, education level, occupation predict SES Stress caused outside factors names people use : Composite causes MIMIC – multiple indicators, multiple causes models","code":""},{"path":[]},{"path":"/articles/lecture_sem.html","id":"identification","dir":"Articles","previous_headings":"","what":"Identification","title":"Full SEM","text":"Identification rules thumb: Latent variables four indicators Latent variables three indicators Error variances covary Latent variables two indicators Error variances covary Loadings set equal .","code":""},{"path":"/articles/lecture_sem.html","id":"structural-model-identification","dir":"Articles","previous_headings":"","what":"Structural Model Identification","title":"Full SEM","text":"Scaling also required identify structural part 2+ emitted paths rule Composite variable must direct effects two endogenous variables","code":""},{"path":"/articles/lecture_sem.html","id":"things-to-consider","dir":"Articles","previous_headings":"","what":"Things to Consider","title":"Full SEM","text":"Parceling large structural models, can complex fit Full SEM latent variable lots indicators (items). Parceling occurs create subsets items able get model run balance number indicators latent. moment, topic still pretty controversial.","code":""},{"path":"/articles/lecture_sem.html","id":"how-to-model","dir":"Articles","previous_headings":"","what":"How to Model","title":"Full SEM","text":"Test CFA piece separately make sure run. CFAs bad, suddenly make good models structural component added! Slowly add structural paths see can get full model work. , try parceling. Drop non-significant paths.","code":""},{"path":"/articles/lecture_sem.html","id":"how-to-model-1","dir":"Articles","previous_headings":"","what":"How to Model","title":"Full SEM","text":"add structural components, see big change loadings indicators , means model invariant Causes interpretation difficulties","code":""},{"path":"/articles/lecture_sem.html","id":"when-to-stop","dir":"Articles","previous_headings":"","what":"When to Stop?","title":"Full SEM","text":"discussed using modificationindices() tricks improve model fit theory, add paths model “perfect” stopping rule ? Based theory Fit indices greatly improve Parsimony","code":""},{"path":[]},{"path":"/articles/lecture_sem.html","id":"example-model-setup","dir":"Articles","previous_headings":"","what":"Example Model: Setup","title":"Full SEM","text":"","code":"library(lavaan) library(semPlot)  family.cor <- lav_matrix_lower2full(c(1.00,                                        .74,  1.00,                                          .27,  .42,    1.00,                                          .31,  .40,    .79,    1.00,                                          .32,  .35,    .66,    .59,    1.00)) family.sd <- c(32.94,   22.75, 13.39,   13.68,  14.38) rownames(family.cor) <-    colnames(family.cor) <-   names(family.sd) <- c(\"father\", \"mother\", \"famo\", \"problems\", \"intimacy\")  family.cov <- cor2cov(family.cor, family.sd)"},{"path":"/articles/lecture_sem.html","id":"example-model-build-the-cfa","dir":"Articles","previous_headings":"","what":"Example Model: Build the CFA","title":"Full SEM","text":"First, going test measurement model – just CFAs covariance . , going change full SEM, predicting direction relationship latents. ensure measurement model change significantly.","code":""},{"path":"/articles/lecture_sem.html","id":"example-model-build-the-cfa-1","dir":"Articles","previous_headings":"","what":"Example Model: Build the CFA","title":"Full SEM","text":"","code":"family.model <- ' adjust =~ problems + intimacy family =~ father + mother + famo'"},{"path":"/articles/lecture_sem.html","id":"example-model-analyze-the-cfa","dir":"Articles","previous_headings":"","what":"Example Model: Analyze the CFA","title":"Full SEM","text":"","code":"family.fit <- cfa(model = family.model,                   sample.cov = family.cov,                   sample.nobs = 203) #> Warning: lavaan->lav_object_post_check():   #>    covariance matrix of latent variables is not positive definite ; use  #>    lavInspect(fit, \"cov.lv\") to investigate."},{"path":"/articles/lecture_sem.html","id":"example-model-deal-with-error","dir":"Articles","previous_headings":"","what":"Example Model: Deal with Error","title":"Full SEM","text":"","code":"inspect(family.fit, \"cov.lv\") #>         adjust  family #> adjust 129.870         #> family 152.798 160.332 inspect(family.fit, \"cor.lv\") #>        adjust family #> adjust  1.000        #> family  1.059  1.000"},{"path":"/articles/lecture_sem.html","id":"example-model-analyze-the-cfa-1","dir":"Articles","previous_headings":"","what":"Example Model: Analyze the CFA","title":"Full SEM","text":"","code":"family.fit <- cfa(model = family.model,                   sample.cov = family.cor,                   sample.nobs = 203)"},{"path":"/articles/lecture_sem.html","id":"example-model-summarize-the-model","dir":"Articles","previous_headings":"","what":"Example Model: Summarize the Model","title":"Full SEM","text":"","code":"summary(family.fit,          rsquare = TRUE,          standardized = TRUE,         fit.measures = TRUE) #> lavaan 0.6-19 ended normally after 23 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        11 #>  #>   Number of observations                           203 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               197.939 #>   Degrees of freedom                                 4 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                               533.051 #>   Degrees of freedom                                10 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.629 #>   Tucker-Lewis Index (TLI)                       0.073 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -1270.160 #>   Loglikelihood unrestricted model (H1)      -1171.191 #>                                                        #>   Akaike (AIC)                                2562.321 #>   Bayesian (BIC)                              2598.766 #>   Sample-size adjusted Bayesian (SABIC)       2563.915 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.489 #>   90 Percent confidence interval - lower         0.432 #>   90 Percent confidence interval - upper         0.548 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.186 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   adjust =~                                                              #>     problems          1.000                               0.807    0.809 #>     intimacy          0.901    0.134    6.741    0.000    0.727    0.729 #>   family =~                                                              #>     father            1.000                               0.789    0.790 #>     mother            1.143    0.112   10.165    0.000    0.901    0.903 #>     famo              0.630    0.092    6.847    0.000    0.497    0.498 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   adjust ~~                                                              #>     family            0.384    0.071    5.441    0.000    0.604    0.604 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .problems          0.344    0.092    3.720    0.000    0.344    0.345 #>    .intimacy          0.466    0.084    5.571    0.000    0.466    0.468 #>    .father            0.373    0.062    6.035    0.000    0.373    0.375 #>    .mother            0.183    0.066    2.758    0.006    0.183    0.184 #>    .famo              0.748    0.079    9.530    0.000    0.748    0.752 #>     adjust            0.651    0.126    5.157    0.000    1.000    1.000 #>     family            0.622    0.104    5.974    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     problems          0.655 #>     intimacy          0.532 #>     father            0.625 #>     mother            0.816 #>     famo              0.248"},{"path":"/articles/lecture_sem.html","id":"example-model-improve-the-model","dir":"Articles","previous_headings":"","what":"Example Model: Improve the Model?","title":"Full SEM","text":"","code":"modificationindices(family.fit, sort = T) #>         lhs op    rhs      mi    epc sepc.lv sepc.all sepc.nox #> 16   adjust =~   famo 136.963  1.467   1.184    1.187    1.187 #> 26   father ~~ mother 136.963  1.785   1.785    6.825    6.825 #> 22 problems ~~   famo  58.596  0.371   0.371    0.732    0.732 #> 27   father ~~   famo  25.310 -0.283  -0.283   -0.536   -0.536 #> 15   adjust =~ mother  25.310 -0.767  -0.619   -0.621   -0.621 #> 25 intimacy ~~   famo  13.685  0.183   0.183    0.310    0.310 #> 14   adjust =~ father   7.800 -0.370  -0.299   -0.299   -0.299 #> 28   mother ~~   famo   7.800 -0.179  -0.179   -0.482   -0.482 #> 20 problems ~~ father   6.001 -0.103  -0.103   -0.287   -0.287 #> 24 intimacy ~~ mother   4.674 -0.096  -0.096   -0.329   -0.329 #> 21 problems ~~ mother   2.723 -0.076  -0.076   -0.302   -0.302 #> 23 intimacy ~~ father   0.117  0.014   0.014    0.034    0.034"},{"path":"/articles/lecture_sem.html","id":"example-model-improve-the-model-1","dir":"Articles","previous_headings":"","what":"Example Model: Improve the Model?","title":"Full SEM","text":"","code":"family.model2 <- ' adjust =~ problems + intimacy family =~ father + mother + famo father ~~ mother'  family.fit2 <- cfa(model = family.model2,                   sample.cov = family.cov,                   sample.nobs = 203) #> Warning: lavaan->lav_object_post_check():   #>    covariance matrix of latent variables is not positive definite ; use  #>    lavInspect(fit, \"cov.lv\") to investigate.  inspect(family.fit2, \"cor.lv\") #>        adjust family #> adjust  1.000        #> family  1.038  1.000"},{"path":"/articles/lecture_sem.html","id":"example-model-diagram-the-model","dir":"Articles","previous_headings":"","what":"Example Model: Diagram the Model","title":"Full SEM","text":"","code":"semPaths(family.fit,           whatLabels=\"std\",           layout=\"tree\",           edge.label.cex = 1)"},{"path":"/articles/lecture_sem.html","id":"example-model-build-full-sem","dir":"Articles","previous_headings":"","what":"Example Model: Build Full SEM","title":"Full SEM","text":"","code":"predict.model <- ' adjust =~ problems + intimacy family =~ father + mother + famo adjust~family'"},{"path":"/articles/lecture_sem.html","id":"example-model-analyze-full-sem","dir":"Articles","previous_headings":"","what":"Example Model: Analyze Full SEM","title":"Full SEM","text":"","code":"predict.fit <- sem(model = predict.model,                    sample.cov = family.cor,                    sample.nobs = 203)"},{"path":"/articles/lecture_sem.html","id":"example-model-summarize-full-sem","dir":"Articles","previous_headings":"","what":"Example Model: Summarize Full SEM","title":"Full SEM","text":"","code":"summary(predict.fit,          rsquare = TRUE,          standardized = TRUE,         fit.measures = TRUE) #> lavaan 0.6-19 ended normally after 20 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        11 #>  #>   Number of observations                           203 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               197.939 #>   Degrees of freedom                                 4 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                               533.051 #>   Degrees of freedom                                10 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.629 #>   Tucker-Lewis Index (TLI)                       0.073 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -1270.160 #>   Loglikelihood unrestricted model (H1)      -1171.191 #>                                                        #>   Akaike (AIC)                                2562.321 #>   Bayesian (BIC)                              2598.766 #>   Sample-size adjusted Bayesian (SABIC)       2563.915 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.489 #>   90 Percent confidence interval - lower         0.432 #>   90 Percent confidence interval - upper         0.548 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.186 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   adjust =~                                                              #>     problems          1.000                               0.807    0.809 #>     intimacy          0.901    0.134    6.741    0.000    0.727    0.729 #>   family =~                                                              #>     father            1.000                               0.789    0.790 #>     mother            1.143    0.112   10.165    0.000    0.901    0.903 #>     famo              0.630    0.092    6.847    0.000    0.497    0.498 #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   adjust ~                                                               #>     family            0.618    0.092    6.705    0.000    0.604    0.604 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .problems          0.344    0.092    3.720    0.000    0.344    0.345 #>    .intimacy          0.466    0.084    5.571    0.000    0.466    0.468 #>    .father            0.373    0.062    6.035    0.000    0.373    0.375 #>    .mother            0.183    0.066    2.758    0.006    0.183    0.184 #>    .famo              0.748    0.079    9.530    0.000    0.748    0.752 #>    .adjust            0.414    0.095    4.364    0.000    0.636    0.636 #>     family            0.622    0.104    5.974    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     problems          0.655 #>     intimacy          0.532 #>     father            0.625 #>     mother            0.816 #>     famo              0.248 #>     adjust            0.364"},{"path":"/articles/lecture_sem.html","id":"example-model-diagram-full-sem","dir":"Articles","previous_headings":"","what":"Example Model: Diagram Full SEM","title":"Full SEM","text":"","code":"semPaths(predict.fit,           whatLabels=\"std\",           layout=\"tree\",           edge.label.cex = 1)"},{"path":[]},{"path":"/articles/lecture_sem.html","id":"example-2-setup","dir":"Articles","previous_headings":"","what":"Example 2: Setup","title":"Full SEM","text":"","code":"family.cor <- lav_matrix_lower2full(c(1.00,                                       .42,   1.00,                                      -.43,   -.50,   1.00,                                      -.39,   -.43,   .78,    1.00,                                        -.24,   -.37,   .69,    .73,    1.00,                                      -.31,   -.33,   .63,    .87,    .72,    1.00,                                        -.25,   -.25,   .49,    .53,    .60,    .59,    1.00,                                       -.25,  -.26,   .42,    .42,    .44,    .45,    .77,    1.00,                                         -.16,  -.18,   .23,    .36,    .38,    .38,    .59,    .58, 1.00))  family.sd <- c(13.00,   13.50,  13.10,  12.50,  13.50,  14.20,  9.50,   11.10,  8.70)  rownames(family.cor) <-    colnames(family.cor) <-   names(family.sd) <- c(\"parent_psych\",\"low_SES\",\"verbal\",                         \"reading\",\"math\",\"spelling\",\"motivation\",\"harmony\",\"stable\")  family.cov <- cor2cov(family.cor, family.sd)"},{"path":"/articles/lecture_sem.html","id":"example-2-build-the-model","dir":"Articles","previous_headings":"","what":"Example 2: Build the Model","title":"Full SEM","text":"define composite variable? using =~ define latent variable predicts manifest variables. Use <~ create composite variable predicted manifest variables.","code":""},{"path":"/articles/lecture_sem.html","id":"example-2-build-the-model-1","dir":"Articles","previous_headings":"","what":"Example 2: Build the Model","title":"Full SEM","text":"","code":"composite.model <- ' risk <~ low_SES + parent_psych + verbal achieve =~ reading + math + spelling adjustment =~ motivation + harmony + stable risk =~ achieve + adjustment '"},{"path":"/articles/lecture_sem.html","id":"example-2-analyze-the-model","dir":"Articles","previous_headings":"","what":"Example 2: Analyze the Model","title":"Full SEM","text":"","code":"composite.fit <- sem(model = composite.model,                        sample.cov = family.cov,                        sample.nobs = 158)"},{"path":"/articles/lecture_sem.html","id":"example-2-summarize-the-model","dir":"Articles","previous_headings":"","what":"Example 2: Summarize the Model","title":"Full SEM","text":"","code":"summary(composite.fit,          rsquare = TRUE,          standardized = TRUE,         fit.measures = TRUE) #> lavaan 0.6-19 ended normally after 78 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        16 #>  #>   Number of observations                           158 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                94.965 #>   Degrees of freedom                                23 #>   P-value (Chi-square)                           0.000 #>  #> Model Test Baseline Model: #>  #>   Test statistic                               852.585 #>   Degrees of freedom                                33 #>   P-value                                        0.000 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.912 #>   Tucker-Lewis Index (TLI)                       0.874 #>  #> Loglikelihood and Information Criteria: #>  #>   Loglikelihood user model (H0)              -3270.643 #>   Loglikelihood unrestricted model (H1)      -3223.160 #>                                                        #>   Akaike (AIC)                                6573.286 #>   Bayesian (BIC)                              6622.287 #>   Sample-size adjusted Bayesian (SABIC)       6571.640 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.141 #>   90 Percent confidence interval - lower         0.112 #>   90 Percent confidence interval - upper         0.171 #>   P-value H_0: RMSEA <= 0.050                    0.000 #>   P-value H_0: RMSEA >= 0.080                    1.000 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.089 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   achieve =~                                                             #>     reading           1.000                              12.183    0.978 #>     math              0.843    0.062   13.545    0.000   10.265    0.763 #>     spelling          1.030    0.053   19.583    0.000   12.546    0.886 #>   adjustment =~                                                          #>     motivation        1.000                               8.632    0.912 #>     harmony           1.089    0.092   11.861    0.000    9.402    0.850 #>     stable            0.654    0.074    8.828    0.000    5.642    0.651 #>   risk =~                                                                #>     achieve           1.000                               0.794    0.794 #>     adjustment        0.457    0.074    6.206    0.000    0.511    0.511 #>  #> Composites: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   risk <~                                                                #>     low_SES          -0.033    0.050   -0.647    0.518   -0.003   -0.045 #>     parent_psych     -0.055    0.050   -1.095    0.274   -0.006   -0.074 #>     verbal            0.698    0.055   12.576    0.000    0.072    0.942 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .reading           6.831    3.848    1.775    0.076    6.831    0.044 #>    .math             75.732    9.120    8.304    0.000   75.732    0.418 #>    .spelling         42.954    6.357    6.757    0.000   42.954    0.214 #>    .motivation       15.162    4.920    3.082    0.002   15.162    0.169 #>    .harmony          34.034    6.723    5.062    0.000   34.034    0.278 #>    .stable           43.376    5.381    8.060    0.000   43.376    0.577 #>     risk              0.000                               0.000    0.000 #>    .achieve          54.931    7.452    7.372    0.000    0.370    0.370 #>    .adjustment       55.023    8.469    6.497    0.000    0.738    0.738 #>  #> R-Square: #>                    Estimate #>     reading           0.956 #>     math              0.582 #>     spelling          0.786 #>     motivation        0.831 #>     harmony           0.722 #>     stable            0.423 #>     achieve           0.630 #>     adjustment        0.262"},{"path":"/articles/lecture_sem.html","id":"example-2-improve-the-model","dir":"Articles","previous_headings":"","what":"Example 2: Improve the Model?","title":"Full SEM","text":"","code":"modificationindices(composite.fit, sort = T) #>             lhs op          rhs     mi     epc sepc.lv sepc.all sepc.nox #> 29         risk =~     spelling 22.091  -0.607  -5.874   -0.415   -0.415 #> 39      reading ~~         math 22.090 -27.301 -27.301   -1.200   -1.200 #> 54         risk ~~      achieve 15.658  42.911      NA       NA       NA #> 56      achieve ~~   adjustment 15.658  19.593   0.356    0.356    0.356 #> 18         risk ~~         risk 15.658  42.911   0.000    0.000    0.000 #> 55         risk ~~   adjustment 15.658  19.593      NA       NA       NA #> 37   adjustment =~         math 12.595   0.345   2.979    0.221    0.221 #> 33      achieve =~   motivation  8.857   0.140   1.710    0.181    0.181 #> 48     spelling ~~   motivation  8.351   8.974   8.974    0.352    0.352 #> 40      reading ~~     spelling  8.135  25.450  25.450    1.486    1.486 #> 28         risk =~         math  8.135   0.379   3.665    0.272    0.272 #> 45         math ~~   motivation  7.454  10.793  10.793    0.319    0.319 #> 38   adjustment =~     spelling  7.118   0.208   1.794    0.127    0.127 #> 44         math ~~     spelling  6.189  15.229  15.229    0.267    0.267 #> 27         risk =~      reading  6.189   0.320   3.090    0.248    0.248 #> 51   motivation ~~      harmony  4.044 -27.644 -27.644   -1.217   -1.217 #> 32         risk =~       stable  4.044  -0.138  -1.331   -0.153   -0.153 #> 41      reading ~~   motivation  3.494  -4.224  -4.224   -0.415   -0.415 #> 30         risk =~   motivation  2.929   0.122   1.183    0.125    0.125 #> 53      harmony ~~       stable  2.929  10.495  10.495    0.273    0.273 #> 43      reading ~~       stable  1.885   3.814   3.814    0.222    0.222 #> 36   adjustment =~      reading  1.618  -0.073  -0.634   -0.051   -0.051 #> 34      achieve =~      harmony  1.128  -0.059  -0.717   -0.065   -0.065 #> 46         math ~~      harmony  0.939  -4.694  -4.694   -0.092   -0.092 #> 31         risk =~      harmony  0.121  -0.028  -0.269   -0.024   -0.024 #> 52   motivation ~~       stable  0.121  -2.010  -2.010   -0.078   -0.078 #> 50     spelling ~~       stable  0.070   1.008   1.008    0.023    0.023 #> 49     spelling ~~      harmony  0.033  -0.687  -0.687   -0.018   -0.018 #> 42      reading ~~      harmony  0.021  -0.400  -0.400   -0.026   -0.026 #> 35      achieve =~       stable  0.017   0.007   0.082    0.009    0.009 #> 47         math ~~       stable  0.009   0.448   0.448    0.008    0.008 #> 58      low_SES  ~       verbal  0.000   0.000   0.000    0.000    0.000 #> 57      low_SES  ~ parent_psych  0.000   0.000   0.000    0.000    0.000 #> 60 parent_psych  ~       verbal  0.000   0.000   0.000    0.000    0.000 #> 59 parent_psych  ~      low_SES  0.000   0.000   0.000    0.000    0.000 #> 62       verbal  ~ parent_psych  0.000   0.000   0.000    0.000    0.000 #> 26       verbal ~~       verbal  0.000   0.000   0.000    0.000    0.000 #> 61       verbal  ~      low_SES  0.000   0.000   0.000    0.000    0.000"},{"path":"/articles/lecture_sem.html","id":"example-2-diagram-the-model","dir":"Articles","previous_headings":"","what":"Example 2: Diagram the Model","title":"Full SEM","text":"","code":"semPaths(composite.fit,           whatLabels=\"std\",           layout=\"tree\",          edge.label.cex = 1)"},{"path":"/articles/lecture_sem.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Full SEM","text":"lecture ’ve learned: build previous work measurement models build composite variable run models examine invariance building full SEM","code":""},{"path":"/articles/lecture_terms.html","id":"structural-equation-modeling","dir":"Articles","previous_headings":"","what":"Structural Equation Modeling","title":"Terms and Concepts","text":"Regression steroids Model many relationships , rather run single regressions Model variables don’t technically exist!","code":""},{"path":"/articles/lecture_terms.html","id":"structural-equation-modeling-1","dir":"Articles","previous_headings":"","what":"Structural Equation Modeling","title":"Terms and Concepts","text":"Even measure causal way, can specify direction Generally, theory relationship hand Less descriptive/exploratory traditional hypothesis testing can specific error terms, rather just one overall residual","code":""},{"path":"/articles/lecture_terms.html","id":"concepts","dir":"Articles","previous_headings":"","what":"Concepts","title":"Terms and Concepts","text":"Represented circles Abstract phenomena trying model represented number dataset Linked measured variables Represented indirectly variables","code":""},{"path":"/articles/lecture_terms.html","id":"concepts-1","dir":"Articles","previous_headings":"","what":"Concepts","title":"Terms and Concepts","text":"Represented squares Measured participants, business data, sources measured variables continuous, can use categorical ordered measures well","code":""},{"path":"/articles/lecture_terms.html","id":"concepts-2","dir":"Articles","previous_headings":"","what":"Concepts","title":"Terms and Concepts","text":"synonymous independent variables thought cause something. can find model arrow leaving variable Exogenous () variables error term Changes variables represented something else aren’t modeling (like age, gender, etc.)","code":""},{"path":"/articles/lecture_terms.html","id":"concepts-3","dir":"Articles","previous_headings":"","what":"Concepts","title":"Terms and Concepts","text":"synonymous dependent variables caused exogenous variables model diagram, arrow coming variable Endogenous variables error terms (assigned automatically software)","code":""},{"path":"/articles/lecture_terms.html","id":"concepts-4","dir":"Articles","previous_headings":"","what":"Concepts","title":"Terms and Concepts","text":"Remember Y ~ X + ϵ\\epsilon Endogenous ~ Exogenous + Residual Sometimes people call residuals: disturbances","code":""},{"path":"/articles/lecture_terms.html","id":"concepts-5","dir":"Articles","previous_headings":"","what":"Concepts","title":"Terms and Concepts","text":"relationship exogenous latent variable measured variables . Generally used describing confirmatory factor analysis","code":""},{"path":"/articles/lecture_terms.html","id":"concepts-6","dir":"Articles","previous_headings":"","what":"Concepts","title":"Terms and Concepts","text":"measurement model + causal relationships latent variables","code":""},{"path":[]},{"path":"/articles/lecture_terms.html","id":"concepts-8","dir":"Articles","previous_headings":"","what":"Concepts","title":"Terms and Concepts","text":"Recursive models – arrows go one direction","code":""},{"path":"/articles/lecture_terms.html","id":"concepts-9","dir":"Articles","previous_headings":"","what":"Concepts","title":"Terms and Concepts","text":"Nonrecursive models – arrows go backwards original variables","code":""},{"path":"/articles/lecture_terms.html","id":"interpreting-a-sem-diagram","dir":"Articles","previous_headings":"","what":"Interpreting a SEM Diagram","title":"Terms and Concepts","text":"Recap: numbers dataset number dataset Single headed arrows indicate predicted direction relationship (–>) Double headed arrows indicate variance covariance (<–>)","code":""},{"path":"/articles/lecture_terms.html","id":"parameters","dir":"Articles","previous_headings":"","what":"Parameters","title":"Terms and Concepts","text":"Unstandardized estimates two variables aren’t latent –> measured: regressions ~ measured latents: latent variables =~ Indicate coefficient b - relationship two variables, like regression Covariances ~~: amount two variables vary together Remember covariance scaled","code":""},{"path":"/articles/lecture_terms.html","id":"parameters-1","dir":"Articles","previous_headings":"","what":"Parameters","title":"Terms and Concepts","text":"","code":"#> lavaan 0.6-19 ended normally after 35 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        20 #>  #>   Number of observations                           301 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               104.570 #>   Degrees of freedom                                25 #>   P-value (Chi-square)                           0.000 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   visual =~                                            #>     x1                1.000                            #>     x2                0.643    0.114    5.650    0.000 #>     x3                0.899    0.135    6.637    0.000 #>   textual =~                                           #>     x4                1.000                            #>     x5                1.129    0.066   16.992    0.000 #>     x6                0.926    0.056   16.499    0.000 #>   speed =~                                             #>     x7                1.000                            #>     x8                1.178    0.176    6.695    0.000 #>     x9                1.304    0.193    6.774    0.000 #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   visual ~                                             #>     speed             0.831    0.159    5.215    0.000 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   textual ~~                                           #>     speed             0.196    0.048    4.118    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .x1                0.694    0.107    6.470    0.000 #>    .x2                1.107    0.102   10.848    0.000 #>    .x3                0.738    0.096    7.696    0.000 #>    .x4                0.381    0.048    7.867    0.000 #>    .x5                0.423    0.058    7.227    0.000 #>    .x6                0.365    0.044    8.368    0.000 #>    .x7                0.869    0.083   10.494    0.000 #>    .x8                0.585    0.070    8.383    0.000 #>    .x9                0.480    0.072    6.685    0.000 #>    .visual            0.447    0.103    4.324    0.000 #>     textual           0.970    0.112    8.664    0.000 #>     speed             0.315    0.078    4.019    0.000"},{"path":"/articles/lecture_terms.html","id":"parameters-2","dir":"Articles","previous_headings":"","what":"Parameters","title":"Terms and Concepts","text":"Standardized estimates: note several ways “standardize” solution, cover later Regressions ~: β\\beta coefficient, z-scored b Latent variables =~: correlation measured latent variable, usually called loadings like EFA Covariance ~~: correlation two variables R-Squared: SMCs, Squared Multiple Correlation: variance accounted endogenous variable","code":""},{"path":"/articles/lecture_terms.html","id":"parameters-3","dir":"Articles","previous_headings":"","what":"Parameters","title":"Terms and Concepts","text":"","code":"#> lavaan 0.6-19 ended normally after 35 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        20 #>  #>   Number of observations                           301 #>  #> Model Test User Model: #>                                                        #>   Test statistic                               104.570 #>   Degrees of freedom                                25 #>   P-value (Chi-square)                           0.000 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   visual =~                                                              #>     x1                1.000                               0.815    0.700 #>     x2                0.643    0.114    5.650    0.000    0.524    0.446 #>     x3                0.899    0.135    6.637    0.000    0.733    0.649 #>   textual =~                                                             #>     x4                1.000                               0.985    0.847 #>     x5                1.129    0.066   16.992    0.000    1.112    0.863 #>     x6                0.926    0.056   16.499    0.000    0.912    0.834 #>   speed =~                                                               #>     x7                1.000                               0.561    0.516 #>     x8                1.178    0.176    6.695    0.000    0.661    0.654 #>     x9                1.304    0.193    6.774    0.000    0.731    0.726 #>  #> Regressions: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   visual ~                                                               #>     speed             0.831    0.159    5.215    0.000    0.572    0.572 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   textual ~~                                                             #>     speed             0.196    0.048    4.118    0.000    0.354    0.354 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .x1                0.694    0.107    6.470    0.000    0.694    0.511 #>    .x2                1.107    0.102   10.848    0.000    1.107    0.801 #>    .x3                0.738    0.096    7.696    0.000    0.738    0.579 #>    .x4                0.381    0.048    7.867    0.000    0.381    0.282 #>    .x5                0.423    0.058    7.227    0.000    0.423    0.255 #>    .x6                0.365    0.044    8.368    0.000    0.365    0.305 #>    .x7                0.869    0.083   10.494    0.000    0.869    0.734 #>    .x8                0.585    0.070    8.383    0.000    0.585    0.573 #>    .x9                0.480    0.072    6.685    0.000    0.480    0.473 #>    .visual            0.447    0.103    4.324    0.000    0.673    0.673 #>     textual           0.970    0.112    8.664    0.000    1.000    1.000 #>     speed             0.315    0.078    4.019    0.000    1.000    1.000 #>  #> R-Square: #>                    Estimate #>     x1                0.489 #>     x2                0.199 #>     x3                0.421 #>     x4                0.718 #>     x5                0.745 #>     x6                0.695 #>     x7                0.266 #>     x8                0.427 #>     x9                0.527 #>     visual            0.327"},{"path":"/articles/lecture_terms.html","id":"types-of-research-questions","dir":"Articles","previous_headings":"","what":"Types of Research Questions","title":"Terms and Concepts","text":"Model fit, χ2\\chi^2, fit indices errors Heywood cases Low residuals, modification indices Path significance: note large sample sizes, instead path size better competing models? Modification indices","code":""},{"path":"/articles/lecture_terms.html","id":"types-of-research-questions-1","dir":"Articles","previous_headings":"","what":"Types of Research Questions","title":"Terms and Concepts","text":"Amount variance (effect size): SMCS R2R^2 Parameter Estimates: direction strength Multi-group models, multiple indicators models (MIMIC) Longitudinal differences Latent Growth Curves Multilevel modeling repeated measures datasets","code":""},{"path":"/articles/lecture_terms.html","id":"practical-issues","dir":"Articles","previous_headings":"","what":"Practical Issues","title":"Terms and Concepts","text":"Sample size: parameter estimates accurate, large samples many? Hard say, often hundreds necessary http://web.pdx.edu/~newsomj/semclass/ho_sample%20size.pdf https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4334479/","code":""},{"path":"/articles/lecture_terms.html","id":"practical-issues-1","dir":"Articles","previous_headings":"","what":"Practical Issues","title":"Terms and Concepts","text":"Number people, N q number estimated parameters want N:q ratio 20:1 greater perfect world, 10:1 can manage .","code":""},{"path":"/articles/lecture_terms.html","id":"hypothesis-testing","dir":"Articles","previous_headings":"","what":"Hypothesis Testing","title":"Terms and Concepts","text":"Theory + Model Building Get data Build model Run model Examine model fit fit statistics Update, replicate","code":""},{"path":"/articles/lecture_terms.html","id":"hypothesis-testing-1","dir":"Articles","previous_headings":"","what":"Hypothesis Testing","title":"Terms and Concepts","text":"Residuals error terms Y ~ X + ϵ\\epsilon Want residuals small possible residuals estimated model (.e., circles) Smaller error implies model data match - accurate representation relationships trying model","code":""},{"path":"/articles/lecture_terms.html","id":"approaches-to-modeling","dir":"Articles","previous_headings":"","what":"Approaches to Modeling","title":"Terms and Concepts","text":"theorized model accept reject . Comparison many different models construct models common scale development, comparing number expected factors original model doesn’t work, improve testing Sometimes called E-SEM","code":""},{"path":[]},{"path":"/articles/lecture_terms.html","id":"specification","dir":"Articles","previous_headings":"","what":"Specification","title":"Terms and Concepts","text":"Generating model hypothesis Drawing think variables related Defining model code LOVE: left variable error Omitted predictors important left Practically: diagrammed something wrong, typed code incorrectly, etc.","code":""},{"path":"/articles/lecture_terms.html","id":"identification","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":"able understand identification, understand SEM analysis covariances trying explain much variance variables model Often used multigroup analysis","code":""},{"path":"/articles/lecture_terms.html","id":"identification-1","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":"2x = 4 one answer 2x + y = 10 many answers Models identified one probable answer parameters estimating","code":""},{"path":"/articles/lecture_terms.html","id":"identification-2","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":"Parameters estimated Degrees Freedom software programs help always look warnings","code":""},{"path":"/articles/lecture_terms.html","id":"identification-3","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":"Free parameter – estimated data Sometimes set 1 indicator marker variable Sometimes practically set model issues arise Setting value equal another parameter Also known equality constraint Cross group equality constraints – mostly used multigroup models, forces paths equal (estimated) group","code":""},{"path":"/articles/lecture_terms.html","id":"identifying-whats-what","dir":"Articles","previous_headings":"","what":"Identifying What’s What","title":"Terms and Concepts","text":"3 variances latent variables 3 covariances latent variables 6 latent variable loadings 9 error variances","code":""},{"path":"/articles/lecture_terms.html","id":"identifying-whats-what-1","dir":"Articles","previous_headings":"","what":"Identifying What’s What","title":"Terms and Concepts","text":"DF related sample size P number measured variables 9×(9+1)2\\frac{9 \\times (9+1)}{2} = 45 45 - 21 = 24","code":""},{"path":"/articles/lecture_terms.html","id":"identifying-whats-what-2","dir":"Articles","previous_headings":"","what":"Identifying What’s What","title":"Terms and Concepts","text":"get right?","code":"#> lavaan 0.6-19 ended normally after 35 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        21 #>  #>   Number of observations                           301 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                85.306 #>   Degrees of freedom                                24 #>   P-value (Chi-square)                           0.000 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   visual =~                                            #>     x1                1.000                            #>     x2                0.554    0.100    5.554    0.000 #>     x3                0.729    0.109    6.685    0.000 #>   textual =~                                           #>     x4                1.000                            #>     x5                1.113    0.065   17.014    0.000 #>     x6                0.926    0.055   16.703    0.000 #>   speed =~                                             #>     x7                1.000                            #>     x8                1.180    0.165    7.152    0.000 #>     x9                1.082    0.151    7.155    0.000 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   visual ~~                                            #>     textual           0.408    0.074    5.552    0.000 #>     speed             0.262    0.056    4.660    0.000 #>   textual ~~                                           #>     speed             0.173    0.049    3.518    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .x1                0.549    0.114    4.833    0.000 #>    .x2                1.134    0.102   11.146    0.000 #>    .x3                0.844    0.091    9.317    0.000 #>    .x4                0.371    0.048    7.779    0.000 #>    .x5                0.446    0.058    7.642    0.000 #>    .x6                0.356    0.043    8.277    0.000 #>    .x7                0.799    0.081    9.823    0.000 #>    .x8                0.488    0.074    6.573    0.000 #>    .x9                0.566    0.071    8.003    0.000 #>     visual            0.809    0.145    5.564    0.000 #>     textual           0.979    0.112    8.737    0.000 #>     speed             0.384    0.086    4.451    0.000"},{"path":"/articles/lecture_terms.html","id":"identification-4","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":"Generally, good sign Cross panel lagged models set way purpose want ! can’t run !","code":""},{"path":"/articles/lecture_terms.html","id":"identification-5","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":"two observed variables highly correlated, effectively reduces number parameters can estimate Even identified model, can identified sections","code":""},{"path":"/articles/lecture_terms.html","id":"identification-6","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":"Scaling/reference/marker variables: parameter set 1 Helps increase df eliminating free parameter Gives model scale Can done couple ways, generally measurement model Pay attention number variables attached latent variable measurement model","code":""},{"path":"/articles/lecture_terms.html","id":"identification-7","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":", change model change variable set , something likely weird model reference variable estimated unstandardized parameter get standardized parameter, can check variable loading like think need p-value parameter, can run model twice","code":""},{"path":"/articles/lecture_terms.html","id":"identification-8","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":"","code":"#> lavaan 0.6-19 ended normally after 35 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        21 #>  #>   Number of observations                           301 #>  #> Model Test User Model: #>                                                        #>   Test statistic                                85.306 #>   Degrees of freedom                                24 #>   P-value (Chi-square)                           0.000 #>  #> Parameter Estimates: #>  #>   Standard errors                             Standard #>   Information                                 Expected #>   Information saturated (h1) model          Structured #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   visual =~                                                              #>     x1                1.000                               0.900    0.772 #>     x2                0.554    0.100    5.554    0.000    0.498    0.424 #>     x3                0.729    0.109    6.685    0.000    0.656    0.581 #>   textual =~                                                             #>     x4                1.000                               0.990    0.852 #>     x5                1.113    0.065   17.014    0.000    1.102    0.855 #>     x6                0.926    0.055   16.703    0.000    0.917    0.838 #>   speed =~                                                               #>     x7                1.000                               0.619    0.570 #>     x8                1.180    0.165    7.152    0.000    0.731    0.723 #>     x9                1.082    0.151    7.155    0.000    0.670    0.665 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>   visual ~~                                                              #>     textual           0.408    0.074    5.552    0.000    0.459    0.459 #>     speed             0.262    0.056    4.660    0.000    0.471    0.471 #>   textual ~~                                                             #>     speed             0.173    0.049    3.518    0.000    0.283    0.283 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all #>    .x1                0.549    0.114    4.833    0.000    0.549    0.404 #>    .x2                1.134    0.102   11.146    0.000    1.134    0.821 #>    .x3                0.844    0.091    9.317    0.000    0.844    0.662 #>    .x4                0.371    0.048    7.779    0.000    0.371    0.275 #>    .x5                0.446    0.058    7.642    0.000    0.446    0.269 #>    .x6                0.356    0.043    8.277    0.000    0.356    0.298 #>    .x7                0.799    0.081    9.823    0.000    0.799    0.676 #>    .x8                0.488    0.074    6.573    0.000    0.488    0.477 #>    .x9                0.566    0.071    8.003    0.000    0.566    0.558 #>     visual            0.809    0.145    5.564    0.000    1.000    1.000 #>     textual           0.979    0.112    8.737    0.000    1.000    1.000 #>     speed             0.384    0.086    4.451    0.000    1.000    1.000"},{"path":"/articles/lecture_terms.html","id":"identification-9","dir":"Articles","previous_headings":"","what":"Identification","title":"Terms and Concepts","text":"Start small – work measurement model components first, since simple identification rules work adding variables see problem occurs lavaan gives somewhat good warnings Page 130 Kline great set references identification","code":""},{"path":"/articles/lecture_terms.html","id":"positive-definite-matrices","dir":"Articles","previous_headings":"","what":"Positive Definite Matrices","title":"Terms and Concepts","text":"Dreaded: hessian matrix definite Matrix singular Eigenvalues negative Determinants zero negative Correlations bounds","code":""},{"path":"/articles/lecture_terms.html","id":"positive-definite-matrices-1","dir":"Articles","previous_headings":"","what":"Positive Definite Matrices","title":"Terms and Concepts","text":"Therefore, two columns perfectly correlated linear transformations , singular matrix variance positive (’s squared formula!) , negative zero determinant indicates singular matrix bounds – basically means data correlations 1 negative variances (Heywood case)","code":""},{"path":"/articles/lecture_terms.html","id":"summary","dir":"Articles","previous_headings":"","what":"Summary","title":"Terms and Concepts","text":"lecture ’ve learned: Basic terminology Beginning map pictures code definition (~) output Beginning steps specifying creating identified models Degrees freedom Errors may encounter","code":""},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Erin M. Buchanan. Maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Buchanan E (2025). learnSEM: Learning Tutorials Structural Equation Modeling. R package version 0.5.1.","code":"@Manual{,   title = {learnSEM: Learning Tutorials for Structural Equation Modeling},   author = {Erin M. Buchanan},   year = {2025},   note = {R package version 0.5.1}, }"},{"path":"/index.html","id":"learnsem","dir":"","previous_headings":"","what":"learnSEM","title":"Learning Tutorials for Structural Equation Modeling","text":"learnSEM tutorial package learn structural equation modeling written Erin M. Buchanan https://statisticsofdoom.com/. Current Version: 0.5.1","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"learnSEM","what":"Installation","title":"Learning Tutorials for Structural Equation Modeling","text":"can install learnSEM using following code: sure restart R session, helps get Tutorial Window learnr package. see message tutorial found learnSEM, try restarting RStudio.","code":"#install.packages(\"devtools\") #uncomment if you need devtools library(devtools) install_github(\"doomlab/learnSEM\")"},{"path":"/index.html","id":"course-schedule","dir":"","previous_headings":"learnSEM","what":"Course Schedule","title":"Learning Tutorials for Structural Equation Modeling","text":"Introduction R: Lecture: vignette(\"lecture_introR\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"introR\", \"learnSEM\") Data Screening Practice: Lecture: vignette(\"lecture_data_screen\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"datascreen\", \"learnSEM\") Exploratory Factor Analysis: Lecture: vignette(\"lecture_efa\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"efa\", \"learnSEM\") Terminology: Lecture: vignette(\"lecture_terms\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"terms\", \"learnSEM\") Path Models: Lecture: vignette(\"lecture_path\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"path1\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"path2\", \"learnSEM\") CFA Models: Lecture: vignette(\"lecture_cfa\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"cfabasics\", \"learnSEM\") CFA Second Order Models: Lecture: vignette(\"lecture_secondcfa\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"cfasecond\", \"learnSEM\") Full Structural Models: Lecture: vignette(\"lecture_sem\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"fullsem\", \"learnSEM\") Multitrait Multimethod: Lecture: vignette(\"lecture_mtmm\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"mtmm\", \"learnSEM\") Multigroup CFA: Lecture: vignette(\"lecture_mgcfa\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"mgcfa\", \"learnSEM\") Latent Growth Models: Lecture: vignette(\"lecture_lgm\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"lgm\", \"learnSEM\") Item Response Theory: Lecture: vignette(\"lecture_irt\", \"learnSEM\") Tutorial: learnr::run_tutorial(\"irt\", \"learnSEM\")","code":""},{"path":"/reference/caafidata.html","id":null,"dir":"Reference","previous_headings":"","what":"CAAFI Data: Computer Aversion, Attitudes, and Familiarity Inventory — caafidata","title":"CAAFI Data: Computer Aversion, Attitudes, and Familiarity Inventory — caafidata","text":"Study: dataset data collected computer aversion, attitudes, familiarity inventory.","code":""},{"path":"/reference/caafidata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"CAAFI Data: Computer Aversion, Attitudes, and Familiarity Inventory — caafidata","text":"","code":"data(caafidata)"},{"path":"/reference/caafidata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"CAAFI Data: Computer Aversion, Attitudes, and Familiarity Inventory — caafidata","text":"data frame 794 rows 30 variables. q1 enjoy using computers. q2 able use computer important . q3 keep latest computer hardware. q4 Computers beneficial save people time. q5 like using word-processing programs. q6 feel like fool using computer others around. q7 smart enough use computer. q8 avoid using computers whenever possible. q9 understand use computer software (e.g., word-processing programs, spreadsheet programs, etc.). q10 feel understand use computer files, documents, folders. q11 use computer input device every day (e.g., keyboard, touch pad, mouse). q12 can use computer successfully perform tasks. q13 can add new hardware computer. q14 enjoy reading computer magazines. q15 use computer, afraid damage . q16 enjoy connecting new computer accessories. q17 must reference manual help file run computer software. q18 E-mail easy way communicate people. q19 use e-mail every day. q20 comfortable changing (installing/upgrading) computer software. q21 often read computer books. q22 friends often ask computer-related questions. q23 often read computer magazines. q24 Overall, feel know use computer. q25 Computers scientific . q26 using computer, often lose data. q27 enjoy learning use new software programs. q28 like use computer input devices keyboard, touch pad, mouse, etc. q29 Using computer entertaining. q30 keep latest computer software.","code":""},{"path":"/reference/caafidata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"CAAFI Data: Computer Aversion, Attitudes, and Familiarity Inventory — caafidata","text":"instructions : list items describing many thoughts experiences people computers. reading statement, circle number best describes true false statement applies time. opinion item, circle ‘‘0”, please use option absolutely necessary. sure circle one number. Please best respond item. Scale -3 3: absolutely false, neutral, absolutely true Computer Familiarity: Items 3, 13-14, 16, 20-23, 27, 30. Computer Attitudes: Items 1-2, 4-5, 8, 11, 18-19, 28-29. Computer Aversion: Items 6-7, 9-10, 12, 15, 17, 24-26.","code":""},{"path":"/reference/dassdata.html","id":null,"dir":"Reference","previous_headings":"","what":"DASS Data: Depression, Anxiety, and Stress Inventory — dassdata","title":"DASS Data: Depression, Anxiety, and Stress Inventory — dassdata","text":"Study: DASS measurement scale examines depression, anxiety, stress individual.","code":""},{"path":"/reference/dassdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DASS Data: Depression, Anxiety, and Stress Inventory — dassdata","text":"","code":"data(dassdata)"},{"path":"/reference/dassdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"DASS Data: Depression, Anxiety, and Stress Inventory — dassdata","text":"data frame 794 rows 30 variables. Q1 found hard wind . Q2 aware dryness mouth. Q3 seem experience positive feeling . Q4 experienced breathing difficulty. Q5 found difficult work initiative things. Q6 tended -react situations. Q7 experienced trembling (eg, hands). Q8 felt using lot nervous energy. Q9 worried situations might panic make fool . Q10 felt nothing look forward . Q11 found getting agitated. Q12 found difficult relax. Q13 felt -hearted blue. Q14 intolerant anything kept getting . Q15 felt close panic. Q16 unable become enthusiastic anything. Q17 felt worth much person. Q18 felt rather touchy. Q19 aware action heart absence physical exertion. Q20 felt scared without good reason. Q21 felt life meaningless.","code":""},{"path":"/reference/dassdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DASS Data: Depression, Anxiety, and Stress Inventory — dassdata","text":"instructions : Please read statement select number 0, 1, 2 3 indicates much statement applied past week. right wrong answers. spend much time statement, please answer question. Scale 0-3: apply , applied degree, applied considerable degree, applied much Depression: Questions 3, 5, 10, 13, 16, 17, 21 Anxiety: Questions 2, 4, 7, 9, 15, 19, 20 Stress: 1, 6, 8, 11, 12, 14, 18","code":""},{"path":"/reference/datascreen.html","id":null,"dir":"Reference","previous_headings":"","what":"Data Screening Practice Dataset — datascreen","title":"Data Screening Practice Dataset — datascreen","text":"Study: dataset includes male body dissatisfaction scale can used datascreening scale development.","code":""},{"path":"/reference/datascreen.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data Screening Practice Dataset — datascreen","text":"","code":"data(datascreen)"},{"path":"/reference/datascreen.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Data Screening Practice Dataset — datascreen","text":"data frame 797 rows 12 variables. Participant_ID ID number participant q1 think body leaner q2 concerned stomach flabby q3 feel dissatisfied overall body build q4 think much fat body q5 think abs thin enough q6 feel satisfied size shape   body q7 eating sweets, cakes, high   calorie food made feel fat weak? q8 felt excessively large rounded   (.e., fat)? q9 felt ashamed body size   shape? q10 seeing reflection (e.g., mirror   window) made feel badly size shape? q11 worried body size   shape feeling diet?","code":""},{"path":"/reference/dirtdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Dichotomous IRT Practice Data — dirtdata","title":"Dichotomous IRT Practice Data — dirtdata","text":"Study: data represents answers Educational Psychology exam. zero indicates person missed question, one indicates person got question right.","code":""},{"path":"/reference/dirtdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dichotomous IRT Practice Data — dirtdata","text":"","code":"data(dirtdata)"},{"path":"/reference/dirtdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Dichotomous IRT Practice Data — dirtdata","text":"data frame 30 rows 4 variables.","code":""},{"path":"/reference/efa.html","id":null,"dir":"Reference","previous_headings":"","what":"Exploratory Factor Analysis Practice Dataset — efa","title":"Exploratory Factor Analysis Practice Dataset — efa","text":"Study: dataset data Openness Experience scale collected part undergraduate honor's thesis project.","code":""},{"path":"/reference/efa.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Exploratory Factor Analysis Practice Dataset — efa","text":"","code":"data(efa)"},{"path":"/reference/efa.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Exploratory Factor Analysis Practice Dataset — efa","text":"data frame 99 rows 21 variables. o1 Believe importance art. o2 vivid imagination. o3 Tend vote liberal political candidates. o4 Carry conversation higher level. o5 Enjoy hearing new ideas. o6 Enjoy thinking things. o7 Can say things beautifully. o8 Enjoy wild flights fantasy. o9 Get excited new ideas. o10 rich vocabulary. o11 interested abstract ideas. o12 like art. o13 Avoid philosophical discussions. o14 enjoy going art museums. o15 Tend vote conservative political candidates. o16 like poetry. o17 Rarely look deeper meaning things. o18 Believe much tax money goes support artists. o19 interested theoretical discussions. o20 difficulty understanding abstract ideas. condition group condition participant received","code":""},{"path":"/reference/efa.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Exploratory Factor Analysis Practice Dataset — efa","text":"instructions : phrases describing people's behaviors. Please use rating scale describe accurately statement describes . Describe generally now, wish future. Describe honestly see relation people gender roughly age.  Please read statement carefully, check box corresponds response. Scale: inaccurate, moderately inaccurate, neither inaccurate accurate, moderately accurate, accurate","code":""},{"path":"/reference/encoder_logic.html","id":null,"dir":"Reference","previous_headings":"","what":"Encoding Logic for learnr Tutorials — encoder_logic","title":"Encoding Logic for learnr Tutorials — encoder_logic","text":"function grabs student answers learnr tutorial returns HTML output printing tutorial screen.","code":""},{"path":"/reference/encoder_logic.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encoding Logic for learnr Tutorials — encoder_logic","text":"","code":"encoder_logic()"},{"path":"/reference/encoder_logic.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encoding Logic for learnr Tutorials — encoder_logic","text":"HTML output student tutorial","code":""},{"path":"/reference/encoder_logic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encoding Logic for learnr Tutorials — encoder_logic","text":"","code":"# Be sure to put this into a server-context chunk. #```{r context=\"server\"} #encoder_logic() #```"},{"path":"/reference/encoder_ui.html","id":null,"dir":"Reference","previous_headings":"","what":"Encoding User Interface for learnr Tutorials — encoder_ui","title":"Encoding User Interface for learnr Tutorials — encoder_ui","text":"function shiny user interface creating submission output. can define instructions go submission window!","code":""},{"path":"/reference/encoder_ui.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Encoding User Interface for learnr Tutorials — encoder_ui","text":"","code":"encoder_ui(ui_before = NULL, ui_after = NULL)"},{"path":"/reference/encoder_ui.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Encoding User Interface for learnr Tutorials — encoder_ui","text":"ui_before Shiny code go submission box. ui_after Shiny code go submission box.","code":""},{"path":"/reference/encoder_ui.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Encoding User Interface for learnr Tutorials — encoder_ui","text":"Shiny interface creating submissions learnr tutorials.","code":""},{"path":"/reference/encoder_ui.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Encoding User Interface for learnr Tutorials — encoder_ui","text":"","code":"#```{r encode, echo=FALSE} #encoder_ui() #```"},{"path":"/reference/introR.html","id":null,"dir":"Reference","previous_headings":"","what":"Introduction to R Dataset — introR","title":"Introduction to R Dataset — introR","text":"dataset containing research results experiment examined pleasant people felt words, information word typed. dataset examines QWERTY effect.","code":""},{"path":"/reference/introR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Introduction to R Dataset — introR","text":"","code":"data(introR)"},{"path":"/reference/introR.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Introduction to R Dataset — introR","text":"data frame 33949 rows 14 variables: expno experiment number assigned   group participants rating pleasantness rating word originalcode word particpiant saw id participant ID number speed typing speed participant error number typing errors participant whichhand participant indicated   dominate hand LR_switch number times typing word   switch left right hands finger_switch number times switch   fingers typing word rha right hand advantage: Right - Left handed letters word_length number characters word letter_freq average frequency   letters word real_fake word real English word speed_c z-scored speed values","code":""},{"path":"/reference/is_server_context.html","id":null,"dir":"Reference","previous_headings":"","what":"Server Functions for learnr Tutorials — is_server_context","title":"Server Functions for learnr Tutorials — is_server_context","text":"functions help check put together tutorial correctly student answers print end tutorial","code":""},{"path":"/reference/is_server_context.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Server Functions for learnr Tutorials — is_server_context","text":"","code":"is_server_context(.envir)"},{"path":"/reference/is_server_context.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Server Functions for learnr Tutorials — is_server_context","text":".envir Automatically grabs environment variable shiny session.","code":""},{"path":"/reference/is_server_context.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Server Functions for learnr Tutorials — is_server_context","text":"Error messages incorrectly use functions.","code":""},{"path":"/reference/meaningdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Meaning and Purpose in Life Data — meaningdata","title":"Meaning and Purpose in Life Data — meaningdata","text":"Study: data includes three measures meaning purpose life exploring latent variables multi-trait multi-methods analysis.","code":""},{"path":"/reference/meaningdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Meaning and Purpose in Life Data — meaningdata","text":"","code":"data(meaningdata)"},{"path":"/reference/meaningdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Meaning and Purpose in Life Data — meaningdata","text":"data frame 567 rows 50 variables. p1 usually (completely bored exuberant,   enthusiastic) p2 Life seems (completely routine   always exciting) p3 life (goals aims   clear goals aims) p4 personal existence (utterlying   meaningless without purpose purposeful   meaningful) p5 Every day (exactly constantly   new) p6 choose, (prefer never   born like nine lives just like   one) p7 retiring, (loaf completely   rest life exciting things   always wanted ) p8 achieving life goals (made   progress whatever progressed complete fulfillment) p9 life (empty, filled despair   running exciting good things) p10 die today, feel   life (completely worthless   worthwhile) p11 thinking life, (often wonder   exist always see reason ) p12 view world relation life,   world (completely confuses fits meaningfully   life) p13 (irresponsible person   responsible person) p14 Concerning man's freedom make   choices, believe man (completely bound   limitations heridity environment absolutely   free make life choices) p15 regard death, (unprepared   afraid prepared unafraid) p16 regard suicide, (thought   seriously way never given second   thought) p17 regard ability find meaning,   purpose, mission life (practically none   great) p18 life (hands controlled   external factors hands   control ) p19 Facing daily tasks (painful   boring experience source pleasure   satisfaction) p20 discovered (mission purpose   life clear-cut goals satisfying life purpose) m1 understand life's meaning. m2 lookin gof rsomething ath makes life   feel meaningful. m3 always looking find life's ppurpose. m4 life clear sense purpose. m5 good sense makes life   meaningful. m6 discovered satifying life purpose. m7 lalways searching something makes   life feel significant. m8 seeking purpose mission life. m9 life clear purpose. m10 searching meaning life. s1 think ultimate meaning life. s2 experienced feeling   destined accomplish something important,   quite put myfinger just . s3 try new activities areas interest,   soon lose attractiveness. s4 feel element   quite define missing fro mmy life. s5 restless. s6 feel greatest fulfillment   life lies wyet future. s7 hope something exciting future. s8 daydream finding new place life   new identity. s9 feel lack – need find –   real meaning purpose life. s10 think achieving something new    different. s11 seem change main objective life. s12 mystery life puzzles disturbs . s13 feel need 'new lease life'. s14 achieve one goal, start toward   different one. s15 feel need adventure 'new worlds   conquer'. s16 lifetime felt strong urge   find . s17 occasion thought found   looking life, vanish later. s18 aware -powerful consuming   purpose toward life directed. s19 sensed lack worthwhile job   life. s20 felt determination achieve something f   ar beyond ordinary.","code":""},{"path":"/reference/meaningdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Meaning and Purpose in Life Data — meaningdata","text":"Meaning Life Questionnaire scored 1 absolutely untrue 7 absolutely true. items marked M dataset. Purpose Life Questionnaire scaled 1 7 varying question marked P dataset. Last, Seeking Neotic Goals scale ranges 1 never 7 constantly marked S dataset.","code":""},{"path":"/reference/mirtdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Polytomous IRT Practice Data — mirtdata","title":"Polytomous IRT Practice Data — mirtdata","text":"Study: dataset includes 15 questions scored 1 7 use polytomous IRT examples. One indicate low score latent trait, seven indicate higher score latent trait (scale works!).","code":""},{"path":"/reference/mirtdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Polytomous IRT Practice Data — mirtdata","text":"","code":"data(mirtdata)"},{"path":"/reference/mirtdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Polytomous IRT Practice Data — mirtdata","text":"data frame 171 rows 15 variables.","code":""},{"path":"/reference/resdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Multigroup CFA Practice Data — resdata","title":"Multigroup CFA Practice Data — resdata","text":"Study: dataset data gender, ethnicity, resiliency scale practicing factor analysis structural equation modeling topics like multigroup CFA.","code":""},{"path":"/reference/resdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multigroup CFA Practice Data — resdata","text":"","code":"data(resdata)"},{"path":"/reference/resdata.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Multigroup CFA Practice Data — resdata","text":"data frame 516 rows 16 variables. Sex variable gender 1   male, 2 female, 3 /na. Ethnicity variable ethnicity   coded 1 Black, 2 White, 0   . RS1 usually manage one way   another. RS2 feel proud accomplished   things life. RS3 usually take things stride. RS4 friends . RS5 feel can handle many   things time. RS6 determined. RS7 can get difficult times   ’ve experienced difficulty . RS8 self-discipline. RS9 keep interested things. RS10 can usually find something   laugh . RS11 belief gets   hard times. RS12 emergency, ’m someone people   can generally rely . RS13 life meaning. RS14 ’m difficult situation,   can usually find way .","code":""},{"path":"/reference/resdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multigroup CFA Practice Data — resdata","text":"instructions : Please read following statements. right find seven numbers, ranging \"1\" (Strongly Disagree) left \"7\" (Strongly Agree) right. Circle number best indicates feelings statement. example, strongly disagree statement, circle \"1\". neutral, circle \"4\", strongly agree, circle \"7\", etc. Scale: strongly disagree, moderately disagree, somewhat disagree, neutral, somewhat agree, moderately agree, strongly agree","code":""},{"path":"/news/index.html","id":"learnsem-051","dir":"Changelog","previous_headings":"","what":"learnSEM 0.5.1","title":"learnSEM 0.5.1","text":"Fixed typos MTMM tutorial Fixed typos resdata readme Added note MGCFA results depend number decimals","code":""},{"path":"/news/index.html","id":"learnsem-050","dir":"Changelog","previous_headings":"","what":"learnSEM 0.5.0","title":"learnSEM 0.5.0","text":"Initial build classroom.","code":""}]
